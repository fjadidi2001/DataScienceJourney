{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM8WSmAwte93S7rsR1Yw8Zf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/DataScienceJourney/blob/master/Insurance_TabNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    matthews_corrcoef,  # Added for Matthews Correlation\n",
        "    roc_auc_score,      # Added for AUC score\n",
        "    accuracy_score      # Added for test accuracy\n",
        ")"
      ],
      "metadata": {
        "id": "VHalzFpPozlK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the dataset\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify file path\n",
        "file_path = '/content/drive/My Drive/telematics_syn.csv'\n",
        "\n",
        "# Import pandas (assuming you want to use it to read the CSV)\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Step 2: Explore the data\n",
        "print(data.head())\n",
        "print(data.info())\n",
        "\n",
        "# Check for missing values\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Display basic statistics\n",
        "print(data.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T2USPsHo23n",
        "outputId": "575b2320-57f5-4848-d730-83edd36f867e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "   Duration  Insured.age Insured.sex  Car.age  Marital  Car.use  Credit.score  \\\n",
            "0       366           45        Male       -1  Married  Commute         609.0   \n",
            "1       182           44      Female        3  Married  Commute         575.0   \n",
            "2       184           48      Female        6  Married  Commute         847.0   \n",
            "3       183           71        Male        6  Married  Private         842.0   \n",
            "4       183           84        Male       10  Married  Private         856.0   \n",
            "\n",
            "  Region  Annual.miles.drive  Years.noclaims  ...  Left.turn.intensity10  \\\n",
            "0  Urban             6213.71              25  ...                    1.0   \n",
            "1  Urban            12427.42              20  ...                   58.0   \n",
            "2  Urban            12427.42              14  ...                    0.0   \n",
            "3  Urban             6213.71              43  ...                    0.0   \n",
            "4  Urban             6213.71              65  ...                    2.0   \n",
            "\n",
            "   Left.turn.intensity11  Left.turn.intensity12  Right.turn.intensity08  \\\n",
            "0                    0.0                    0.0                     3.0   \n",
            "1                   24.0                   11.0                  1099.0   \n",
            "2                    0.0                    0.0                     0.0   \n",
            "3                    0.0                    0.0                     0.0   \n",
            "4                    0.0                    0.0                   325.0   \n",
            "\n",
            "   Right.turn.intensity09  Right.turn.intensity10  Right.turn.intensity11  \\\n",
            "0                     1.0                     0.0                     0.0   \n",
            "1                   615.0                   219.0                   101.0   \n",
            "2                     0.0                     0.0                     0.0   \n",
            "3                     0.0                     0.0                     0.0   \n",
            "4                   111.0                    18.0                     4.0   \n",
            "\n",
            "   Right.turn.intensity12  NB_Claim    AMT_Claim  \n",
            "0                     0.0         1  5100.171753  \n",
            "1                    40.0         1   883.554840  \n",
            "2                     0.0         0     0.000000  \n",
            "3                     0.0         0     0.000000  \n",
            "4                     2.0         0     0.000000  \n",
            "\n",
            "[5 rows x 52 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100000 entries, 0 to 99999\n",
            "Data columns (total 52 columns):\n",
            " #   Column                  Non-Null Count   Dtype  \n",
            "---  ------                  --------------   -----  \n",
            " 0   Duration                100000 non-null  int64  \n",
            " 1   Insured.age             100000 non-null  int64  \n",
            " 2   Insured.sex             100000 non-null  object \n",
            " 3   Car.age                 100000 non-null  int64  \n",
            " 4   Marital                 100000 non-null  object \n",
            " 5   Car.use                 100000 non-null  object \n",
            " 6   Credit.score            100000 non-null  float64\n",
            " 7   Region                  100000 non-null  object \n",
            " 8   Annual.miles.drive      100000 non-null  float64\n",
            " 9   Years.noclaims          100000 non-null  int64  \n",
            " 10  Territory               100000 non-null  int64  \n",
            " 11  Annual.pct.driven       100000 non-null  float64\n",
            " 12  Total.miles.driven      100000 non-null  float64\n",
            " 13  Pct.drive.mon           100000 non-null  float64\n",
            " 14  Pct.drive.tue           100000 non-null  float64\n",
            " 15  Pct.drive.wed           100000 non-null  float64\n",
            " 16  Pct.drive.thr           100000 non-null  float64\n",
            " 17  Pct.drive.fri           100000 non-null  float64\n",
            " 18  Pct.drive.sat           100000 non-null  float64\n",
            " 19  Pct.drive.sun           100000 non-null  float64\n",
            " 20  Pct.drive.2hrs          100000 non-null  float64\n",
            " 21  Pct.drive.3hrs          100000 non-null  float64\n",
            " 22  Pct.drive.4hrs          100000 non-null  float64\n",
            " 23  Pct.drive.wkday         100000 non-null  float64\n",
            " 24  Pct.drive.wkend         100000 non-null  float64\n",
            " 25  Pct.drive.rush am       100000 non-null  float64\n",
            " 26  Pct.drive.rush pm       100000 non-null  float64\n",
            " 27  Avgdays.week            100000 non-null  float64\n",
            " 28  Accel.06miles           100000 non-null  float64\n",
            " 29  Accel.08miles           100000 non-null  float64\n",
            " 30  Accel.09miles           100000 non-null  float64\n",
            " 31  Accel.11miles           100000 non-null  float64\n",
            " 32  Accel.12miles           100000 non-null  float64\n",
            " 33  Accel.14miles           100000 non-null  float64\n",
            " 34  Brake.06miles           100000 non-null  float64\n",
            " 35  Brake.08miles           100000 non-null  float64\n",
            " 36  Brake.09miles           100000 non-null  float64\n",
            " 37  Brake.11miles           100000 non-null  float64\n",
            " 38  Brake.12miles           100000 non-null  float64\n",
            " 39  Brake.14miles           100000 non-null  float64\n",
            " 40  Left.turn.intensity08   100000 non-null  float64\n",
            " 41  Left.turn.intensity09   100000 non-null  float64\n",
            " 42  Left.turn.intensity10   100000 non-null  float64\n",
            " 43  Left.turn.intensity11   100000 non-null  float64\n",
            " 44  Left.turn.intensity12   100000 non-null  float64\n",
            " 45  Right.turn.intensity08  100000 non-null  float64\n",
            " 46  Right.turn.intensity09  100000 non-null  float64\n",
            " 47  Right.turn.intensity10  100000 non-null  float64\n",
            " 48  Right.turn.intensity11  100000 non-null  float64\n",
            " 49  Right.turn.intensity12  100000 non-null  float64\n",
            " 50  NB_Claim                100000 non-null  int64  \n",
            " 51  AMT_Claim               100000 non-null  float64\n",
            "dtypes: float64(42), int64(6), object(4)\n",
            "memory usage: 39.7+ MB\n",
            "None\n",
            "Duration                  0\n",
            "Insured.age               0\n",
            "Insured.sex               0\n",
            "Car.age                   0\n",
            "Marital                   0\n",
            "Car.use                   0\n",
            "Credit.score              0\n",
            "Region                    0\n",
            "Annual.miles.drive        0\n",
            "Years.noclaims            0\n",
            "Territory                 0\n",
            "Annual.pct.driven         0\n",
            "Total.miles.driven        0\n",
            "Pct.drive.mon             0\n",
            "Pct.drive.tue             0\n",
            "Pct.drive.wed             0\n",
            "Pct.drive.thr             0\n",
            "Pct.drive.fri             0\n",
            "Pct.drive.sat             0\n",
            "Pct.drive.sun             0\n",
            "Pct.drive.2hrs            0\n",
            "Pct.drive.3hrs            0\n",
            "Pct.drive.4hrs            0\n",
            "Pct.drive.wkday           0\n",
            "Pct.drive.wkend           0\n",
            "Pct.drive.rush am         0\n",
            "Pct.drive.rush pm         0\n",
            "Avgdays.week              0\n",
            "Accel.06miles             0\n",
            "Accel.08miles             0\n",
            "Accel.09miles             0\n",
            "Accel.11miles             0\n",
            "Accel.12miles             0\n",
            "Accel.14miles             0\n",
            "Brake.06miles             0\n",
            "Brake.08miles             0\n",
            "Brake.09miles             0\n",
            "Brake.11miles             0\n",
            "Brake.12miles             0\n",
            "Brake.14miles             0\n",
            "Left.turn.intensity08     0\n",
            "Left.turn.intensity09     0\n",
            "Left.turn.intensity10     0\n",
            "Left.turn.intensity11     0\n",
            "Left.turn.intensity12     0\n",
            "Right.turn.intensity08    0\n",
            "Right.turn.intensity09    0\n",
            "Right.turn.intensity10    0\n",
            "Right.turn.intensity11    0\n",
            "Right.turn.intensity12    0\n",
            "NB_Claim                  0\n",
            "AMT_Claim                 0\n",
            "dtype: int64\n",
            "            Duration    Insured.age        Car.age   Credit.score  \\\n",
            "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
            "mean      314.204060      51.378950       5.639720     800.888870   \n",
            "std        79.746222      15.467075       4.062135      83.382316   \n",
            "min        27.000000      16.000000      -2.000000     422.000000   \n",
            "25%       200.000000      39.000000       2.000000     766.000000   \n",
            "50%       365.000000      51.000000       5.000000     825.000000   \n",
            "75%       366.000000      63.000000       8.000000     856.000000   \n",
            "max       366.000000     103.000000      20.000000     900.000000   \n",
            "\n",
            "       Annual.miles.drive  Years.noclaims      Territory  Annual.pct.driven  \\\n",
            "count       100000.000000   100000.000000  100000.000000      100000.000000   \n",
            "mean          9124.122908       28.839960      56.531390           0.502294   \n",
            "std           3826.144730       16.123717      24.036518           0.299189   \n",
            "min              0.000000        0.000000      11.000000           0.002740   \n",
            "25%           6213.710000       15.000000      35.000000           0.249315   \n",
            "50%           7456.452000       29.000000      62.000000           0.490411   \n",
            "75%          12427.420000       41.000000      78.000000           0.753425   \n",
            "max          56731.172300       79.000000      91.000000           1.000000   \n",
            "\n",
            "       Total.miles.driven  Pct.drive.mon  ...  Left.turn.intensity10  \\\n",
            "count       100000.000000  100000.000000  ...          100000.000000   \n",
            "mean          4833.575303       0.139365  ...             551.574010   \n",
            "std           4545.943016       0.042807  ...           14687.929802   \n",
            "min              0.095298       0.000000  ...               0.000000   \n",
            "25%           1529.897500       0.120894  ...               0.000000   \n",
            "50%           3468.287765       0.137909  ...               3.000000   \n",
            "75%           6779.876842       0.155203  ...              30.000000   \n",
            "max          47282.603936       0.998172  ...          794380.000000   \n",
            "\n",
            "       Left.turn.intensity11  Left.turn.intensity12  Right.turn.intensity08  \\\n",
            "count          100000.000000          100000.000000           100000.000000   \n",
            "mean              487.340690             447.758420              843.461830   \n",
            "std             14198.331308           13719.790281            11630.185503   \n",
            "min                 0.000000               0.000000                0.000000   \n",
            "25%                 0.000000               0.000000               11.000000   \n",
            "50%                 1.000000               0.000000              122.000000   \n",
            "75%                 9.000000               2.000000              680.000000   \n",
            "max            793926.000000          793170.000000           841210.000000   \n",
            "\n",
            "       Right.turn.intensity09  Right.turn.intensity10  Right.turn.intensity11  \\\n",
            "count           100000.000000           100000.000000           100000.000000   \n",
            "mean               565.056100              326.654840              246.713120   \n",
            "std              10657.402935             9460.244357             8977.569994   \n",
            "min                  0.000000                0.000000                0.000000   \n",
            "25%                  3.000000                0.000000                0.000000   \n",
            "50%                 43.000000                7.000000                2.000000   \n",
            "75%                321.000000               81.000000               27.000000   \n",
            "max             841207.000000           841200.000000           841176.000000   \n",
            "\n",
            "       Right.turn.intensity12      NB_Claim      AMT_Claim  \n",
            "count           100000.000000  100000.00000  100000.000000  \n",
            "mean               198.753690       0.04494     137.602253  \n",
            "std               8585.177049       0.21813    1264.320056  \n",
            "min                  0.000000       0.00000       0.000000  \n",
            "25%                  0.000000       0.00000       0.000000  \n",
            "50%                  0.000000       0.00000       0.000000  \n",
            "75%                  9.000000       0.00000       0.000000  \n",
            "max             841144.000000       3.00000  104074.886700  \n",
            "\n",
            "[8 rows x 48 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load Data and Explore\n",
        "import pandas as pd\n",
        "\n",
        "# Load your data (replace with actual file path)\n",
        "data = pd.read_csv('/mnt/data/your_dataset.csv')\n",
        "\n",
        "# Display the first few rows\n",
        "print(data.head())\n",
        "\n",
        "# Step 2: Preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Identify categorical columns (assuming 'Region' is categorical, adapt for your data)\n",
        "categorical_columns = ['Region', 'Territory']\n",
        "\n",
        "# One-Hot Encoding for categorical variables\n",
        "data_encoded = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "# Feature scaling (StandardScaler and MinMaxScaler)\n",
        "scaler = StandardScaler()\n",
        "min_max_scaler = MinMaxScaler()\n",
        "\n",
        "# Example features to scale (replace with relevant columns)\n",
        "columns_to_scale = ['Annual.miles.drive', 'Duration', 'Credit.score']\n",
        "data_encoded[columns_to_scale] = scaler.fit_transform(data_encoded[columns_to_scale])\n",
        "\n",
        "# Show processed data\n",
        "print(data_encoded.head())\n",
        "\n",
        "# Step 3: Feature Engineering\n",
        "# Example: Combining harsh driving events\n",
        "data_encoded['Harsh_Driving_Score'] = data_encoded[['Accel.12miles', 'Brake.14miles',\n",
        "                                                   'Left.turn.intensity12', 'Right.turn.intensity12']].mean(axis=1)\n",
        "\n",
        "# Drop the individual columns after aggregation if needed\n",
        "data_encoded.drop(['Accel.12miles', 'Brake.14miles', 'Left.turn.intensity12', 'Right.turn.intensity12'], axis=1, inplace=True)\n",
        "\n",
        "# Show processed data with new features\n",
        "print(data_encoded[['Harsh_Driving_Score']].head())\n",
        "\n",
        "# Step 4: Data Splitting (Train/Test Split)\n",
        "X = data_encoded.drop(columns=['AMT_Claim'])  # Drop target\n",
        "y = data_encoded['AMT_Claim']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Data Balancing (if necessary)\n",
        "# If 'AMT_Claim' is highly imbalanced, you can apply SMOTE or other techniques\n",
        "\n",
        "# Step 6: Model Building (Combining DNN and TabNet)\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import tensorflow as tf\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "\n",
        "# Define a simple DNN model for regression\n",
        "def create_dnn_model(input_dim):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(1)  # Regression output\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae', 'mse'])\n",
        "    return model\n",
        "\n",
        "# TabNet model\n",
        "tabnet = TabNetRegressor()\n",
        "\n",
        "# Tune hyperparameters for DNN using RandomizedSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Example hyperparameters to tune (adjust as needed)\n",
        "param_dist = {\n",
        "    'max_epochs': [50, 100],\n",
        "    'patience': [5, 10],\n",
        "    'learning_rate': [0.001, 0.01],\n",
        "}\n",
        "\n",
        "# Use RandomizedSearchCV for TabNet\n",
        "search = RandomizedSearchCV(tabnet, param_distributions=param_dist, n_iter=5, cv=3)\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "# Train DNN\n",
        "dnn_model = create_dnn_model(X_train.shape[1])\n",
        "dnn_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
        "\n",
        "# Step 7: Model Evaluation\n",
        "# Evaluate both models (TabNet and DNN) using regression metrics\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Predictions\n",
        "y_pred_tabnet = search.predict(X_test)\n",
        "y_pred_dnn = dnn_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "print(\"TabNet Results:\")\n",
        "print(f\"MAE: {mean_absolute_error(y_test, y_pred_tabnet)}\")\n",
        "print(f\"RMSE: {mean_squared_error(y_test, y_pred_tabnet, squared=False)}\")\n",
        "print(f\"R²: {r2_score(y_test, y_pred_tabnet)}\")\n",
        "\n",
        "print(\"DNN Results:\")\n",
        "print(f\"MAE: {mean_absolute_error(y_test, y_pred_dnn)}\")\n",
        "print(f\"RMSE: {mean_squared_error(y_test, y_pred_dnn, squared=False)}\")\n",
        "print(f\"R²: {r2_score(y_test, y_pred_dnn)}\")\n"
      ],
      "metadata": {
        "id": "WSHdgz0IZXXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('/path_to_data.csv')  # Modify to your file path\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing Values:\\n\", data.isnull().sum())\n",
        "\n",
        "# Impute missing values (if any) - using median for numerical, mode for categorical\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == 'object':\n",
        "        data[col].fillna(data[col].mode()[0], inplace=True)\n",
        "    else:\n",
        "        data[col].fillna(data[col].median(), inplace=True)\n",
        "\n",
        "# Label encoding for categorical features\n",
        "categorical_cols = ['Region', 'Territory', 'Car.usage', 'Marital.status']\n",
        "le = LabelEncoder()\n",
        "for col in categorical_cols:\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "\n",
        "# Scale and normalize numerical data\n",
        "scaler = StandardScaler()\n",
        "minmax_scaler = MinMaxScaler()\n",
        "\n",
        "numerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Exclude the target 'AMT_Claim' from scaling\n",
        "numerical_cols.remove('AMT_Claim')\n",
        "\n",
        "# Apply both Standard and MinMax scaling\n",
        "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
        "data[numerical_cols] = minmax_scaler.fit_transform(data[numerical_cols])\n",
        "\n",
        "# Display first few rows to confirm\n",
        "print(\"Preprocessed Data:\\n\", data.head())\n"
      ],
      "metadata": {
        "id": "Ne6oQw55ZyNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering - Aggregating harsh driving events\n",
        "data['HarshDrivingScore'] = data.filter(regex='Accel|Brake|Left.turn|Right.turn').sum(axis=1)\n",
        "\n",
        "# Drop original harsh driving event columns\n",
        "data.drop(data.filter(regex='Accel|Brake|Left.turn|Right.turn').columns, axis=1, inplace=True)\n",
        "\n",
        "# Display to confirm feature engineering\n",
        "print(\"Feature Engineered Data:\\n\", data[['HarshDrivingScore', 'AMT_Claim']].head())\n"
      ],
      "metadata": {
        "id": "j2fUV_VWZys4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}