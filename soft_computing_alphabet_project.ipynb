{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhv3d+f8JRz5elaemQ5SGz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Artificial_Intelligence_Learning/blob/master/soft_computing_alphabet_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6RylE-aE9fj",
        "outputId": "85dfcfdf-5be3-4cd0-f633-18df60f33385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 1.0\n",
            "a belongs to the lowercase alphabet\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample dataset of alphabets\n",
        "data = {\n",
        "    'character': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'],\n",
        "    'label': ['lowercase'] * 26\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Feature extraction\n",
        "vectorizer = CountVectorizer(analyzer='char')\n",
        "X = vectorizer.fit_transform(df['character'])\n",
        "\n",
        "# Target variable\n",
        "y = df['label']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy}\")\n",
        "\n",
        "# Recognize alphabet function\n",
        "def recognize_alphabet(input_char):\n",
        "    input_vector = vectorizer.transform([input_char])\n",
        "    prediction = model.predict(input_vector)\n",
        "    if prediction[0] == 'lowercase':\n",
        "        print(f\"{input_char} belongs to the lowercase alphabet\")\n",
        "    else:\n",
        "        print(f\"{input_char} does not belong to the lowercase alphabet\")\n",
        "\n",
        "# Test the recognition function\n",
        "input_char = 'a'\n",
        "recognize_alphabet(input_char)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Read the text file\n",
        "file_path = '/content/fars.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    data = file.read().splitlines()\n",
        "\n",
        "# Create a pandas dataframe from the data\n",
        "df = pd.DataFrame(data, columns=['character'])\n",
        "\n",
        "# Add labels to the dataframe\n",
        "df['label'] = df['character'].apply(lambda x: 'lowercase' if x.islower() else 'uppercase')\n",
        "\n",
        "# Feature extraction\n",
        "vectorizer = CountVectorizer(analyzer='word', lowercase=False)\n",
        "X = vectorizer.fit_transform(df['character'])\n",
        "\n",
        "# Target variable\n",
        "y = df['label']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy}\")\n",
        "\n",
        "# Recognize alphabet function\n",
        "def recognize_alphabet(input_char):\n",
        "    if input_char.islower():\n",
        "        print(f\"{input_char} belongs to the lowercase alphabet\")\n",
        "    else:\n",
        "        print(f\"{input_char} belongs to the uppercase alphabet\")\n",
        "\n",
        "# Test the recognition function\n",
        "input_char = 'a'\n",
        "recognize_alphabet(input_char)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoW5Djy_F7nh",
        "outputId": "36e2caee-8a0a-48cf-d3b7-817080a880f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model accuracy: 1.0\n",
            "a belongs to the lowercase alphabet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Preprocess the code snippets and convert them into numerical sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(code_snippets)\n",
        "sequences = tokenizer.texts_to_sequences(code_snippets)\n",
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Model accuracy: {accuracy}\")\n",
        "\n",
        "# Make predictions\n",
        "new_code_snippet = preprocess_and_tokenize(new_code)\n",
        "prediction = model.predict(new_code_snippet)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "In this example, you would need to replace code_snippets\n",
        "with your actual preprocessed data and labels with the corresponding labels\n",
        "for pixel manipulation. Additionally, you may need to adjust the model architecture,\n",
        "hyperparameters, and preprocessing steps based on the specifics of your problem and data.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "LGTOvsKnKRTa",
        "outputId": "63e99373-4330-4789-c5c9-459306d737fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-eb51ef9f081a>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Preprocess the code snippets and convert them into numerical sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_snippets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_snippets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmax_sequence_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'code_snippets' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "\n",
        "# Load the pixel representations from the text file and convert them to a matrix format\n",
        "# Assuming the data is in a suitable format, you may need to customize this step based on the actual data format\n",
        "\n",
        "# Preprocess the data and split it into features (X) and labels (y)\n",
        "X = ...  # Pixel matrix representations\n",
        "y = ...  # Uppercase/lowercase labels\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Design the RBF network architecture\n",
        "num_rbf_neurons = 50  # Number of RBF neurons\n",
        "encoder = LabelEncoder()\n",
        "y_train_encoded = encoder.fit_transform(y_train)\n",
        "\n",
        "# Initialize RBF neuron centers using K-means clustering\n",
        "kmeans = KMeans(n_clusters=num_rbf_neurons, random_state=0).fit(X_train)\n",
        "rbf_centers = kmeans.cluster_centers_\n",
        "\n",
        "# Compute RBF neuron widths\n",
        "distances = cdist(X_train, rbf_centers, 'euclidean')\n",
        "rbf_widths = np.std(distances)\n",
        "\n",
        "# Compute RBF neuron activations\n",
        "rbf_activations = rbf_kernel(X_train, rbf_centers, gamma=1.0 / (2 * (rbf_widths ** 2)))\n",
        "\n",
        "# Solve for the output weights using Ridge regression\n",
        "ridge = Ridge(alpha=1e-6, fit_intercept=True)\n",
        "ridge.fit(rbf_activations, y_train_encoded)\n",
        "\n",
        "# Test the trained RBF network\n",
        "# Compute RBF neuron activations for the testing data\n",
        "rbf_activations_test = rbf_kernel(X_test, rbf_centers, gamma=1.0 / (2 * (rbf_widths ** 2)))\n",
        "\n",
        "# Make predictions using the output weights\n",
        "y_pred_encoded = ridge.predict(rbf_activations_test)\n",
        "y_pred = encoder.inverse_transform(y_pred_encoded)\n",
        "\n",
        "# Evaluate the performance of the RBF network\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"RBF network accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "9QOO5t15NVFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the dataset from a text file\n",
        "def load_dataset(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    dataset = [list(map(int, line.split())) for line in lines]\n",
        "    return np.array(dataset)\n",
        "\n",
        "file_path = '/content/fars.txt'  # Provide the correct path to your text file\n",
        "original_dataset = load_dataset(file_path)\n",
        "\n",
        "# Step 2: Convert dataset to bipolar encoding based on the specified condition\n",
        "def convert_to_bipolar_with_condition(dataset):\n",
        "    return np.where(dataset > 128, -1, 1)\n",
        "\n",
        "bipolar_dataset = convert_to_bipolar_with_condition(original_dataset)\n",
        "\n",
        "# Print original and bipolar datasets for verification\n",
        "print(\"Original Dataset:\")\n",
        "print(original_dataset)\n",
        "\n",
        "print(\"\\nBipolar Encoded Dataset:\")\n",
        "print(bipolar_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrSfF6x5MnxS",
        "outputId": "567db952-3a59-45af-9d8d-f60fb476ef11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset:\n",
            "[[255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " ...\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]]\n",
            "\n",
            "Bipolar Encoded Dataset:\n",
            "[[-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " ...\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the dataset from a text file\n",
        "def load_dataset(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    dataset = [list(map(int, line.split())) for line in lines]\n",
        "    return np.array(dataset)\n",
        "\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = load_dataset(file_path)\n",
        "\n",
        "# Step 2: Reshape each sequence into a matrix\n",
        "num_fonts = 7\n",
        "sequence_length = 17\n",
        "\n",
        "# Reshape each sequence into a 2D matrix\n",
        "matrix_dataset = original_dataset.reshape(-1, num_fonts, sequence_length)\n",
        "\n",
        "# Print the original and matrix datasets for verification\n",
        "print(\"Original Dataset:\")\n",
        "print(original_dataset)\n",
        "\n",
        "print(\"\\nMatrix Dataset:\")\n",
        "print(matrix_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrlkXaOOOyl0",
        "outputId": "159f951e-b824-4153-df97-c53f68396365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset:\n",
            "[[255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " ...\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]]\n",
            "\n",
            "Matrix Dataset:\n",
            "[[[255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  ...\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]]\n",
            "\n",
            " [[255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  ...\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]]\n",
            "\n",
            " [[255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  ...\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  ...\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]]\n",
            "\n",
            " [[255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  ...\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]]\n",
            "\n",
            " [[255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  ...\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the dataset from a text file\n",
        "def load_dataset(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    dataset = [list(map(int, line.split())) for line in lines]\n",
        "    return np.array(dataset)\n",
        "\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = load_dataset(file_path)\n",
        "\n",
        "# Step 2: Convert dataset to bipolar encoding based on the specified condition\n",
        "def convert_to_bipolar_with_condition(dataset):\n",
        "    return np.where(dataset > 128, -1, 1)\n",
        "\n",
        "bipolar_dataset = convert_to_bipolar_with_condition(original_dataset)\n",
        "\n",
        "# Step 3: Reshape each sequence into a matrix\n",
        "num_fonts = 7\n",
        "sequence_length = 17\n",
        "\n",
        "# Reshape each sequence into a 2D matrix\n",
        "matrix_dataset = bipolar_dataset.reshape(-1, num_fonts, sequence_length)\n",
        "\n",
        "# Print the original, bipolar, and matrix datasets for verification\n",
        "print(\"Original Dataset:\")\n",
        "print(original_dataset)\n",
        "\n",
        "print(\"\\nBipolar Encoded Dataset:\")\n",
        "print(bipolar_dataset)\n",
        "\n",
        "print(\"\\nMatrix Dataset:\")\n",
        "print(matrix_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqOLFLB3PlYU",
        "outputId": "333957f3-a4be-4be0-992d-cb9be328262d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset:\n",
            "[[255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " ...\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]]\n",
            "\n",
            "Bipolar Encoded Dataset:\n",
            "[[-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " ...\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "Matrix Dataset:\n",
            "[[[-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  ...\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            " [[-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  ...\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            " [[-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  ...\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  ...\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            " [[-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  ...\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            " [[-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  ...\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rLur539QcEeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "def convert_to_bipolar_with_condition(dataset):\n",
        "    return np.where(dataset > 128, -1, 1)\n",
        "\n",
        "bipolar_dataset = convert_to_bipolar_with_condition(original_dataset)\n",
        "\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each sequence into a 2D matrix\n",
        "input_data = bipolar_dataset.reshape(-1, num_fonts, 95, 95)\n",
        "\n",
        "# Step 2: Build a simple neural network with random weights\n",
        "hidden_size = 64\n",
        "output_size = 1\n",
        "\n",
        "# Initialize random weights and biases\n",
        "weights_hidden = np.random.randn(input_size, hidden_size)\n",
        "biases_hidden = np.zeros((1, hidden_size))\n",
        "weights_output = np.random.randn(hidden_size, output_size)\n",
        "biases_output = np.zeros((1, output_size))\n",
        "\n",
        "# Step 3: Define activation function (sigmoid) and forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(data):\n",
        "    data_flattened = data.reshape(-1, input_size)\n",
        "    hidden_layer_input = np.dot(data_flattened, weights_hidden) + biases_hidden\n",
        "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "    output_layer_input = np.dot(hidden_layer_output, weights_output) + biases_output\n",
        "    output = sigmoid(output_layer_input)\n",
        "    return output\n",
        "\n",
        "# Step 4: Train the neural network (random weights, so not meaningful)\n",
        "# Assuming you have labels for your data (binary classification)\n",
        "labels = np.random.randint(0, 2, size=(input_data.shape[0], 1))\n",
        "\n",
        "# Perform a forward pass (no backpropagation or weight updates in this simple example)\n",
        "predictions = forward_pass(input_data)\n",
        "\n",
        "# Print predictions for verification\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUWq-u7URVfO",
        "outputId": "73c6c12c-dfa7-4d09-dcc6-e5f5c8ee088f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            "[[0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.0854868 ]\n",
            " [0.93299217]\n",
            " [0.82351583]\n",
            " [0.34827895]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.07285459]\n",
            " [0.18752605]\n",
            " [0.28799974]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.87022639]\n",
            " [0.00725343]\n",
            " [0.94424744]\n",
            " [0.99094004]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.4592532 ]\n",
            " [0.92735167]\n",
            " [0.93935015]\n",
            " [0.83563282]\n",
            " [0.27773484]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.17637049]\n",
            " [0.48409341]\n",
            " [0.04765104]\n",
            " [0.13602017]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.9331633 ]\n",
            " [0.95340968]\n",
            " [0.10107802]\n",
            " [0.06086342]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.76620982]\n",
            " [0.99637036]\n",
            " [0.96667494]\n",
            " [0.04062386]\n",
            " [0.3291297 ]\n",
            " [0.2830179 ]\n",
            " [0.04601785]\n",
            " [0.00645116]\n",
            " [0.13320435]\n",
            " [0.91156307]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.17961895]\n",
            " [0.53269958]\n",
            " [0.89264076]\n",
            " [0.99355953]\n",
            " [0.91357556]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.24412919]\n",
            " [0.00365142]\n",
            " [0.49640472]\n",
            " [0.569527  ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.35527208]\n",
            " [0.99495094]\n",
            " [0.00540253]\n",
            " [0.78645302]\n",
            " [0.65629949]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.23124683]\n",
            " [0.20746955]\n",
            " [0.17632808]\n",
            " [0.56253533]\n",
            " [0.02455448]\n",
            " [0.15189764]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.93483881]\n",
            " [0.19737088]\n",
            " [0.40009532]\n",
            " [0.21709153]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.09405521]\n",
            " [0.24585903]\n",
            " [0.14622599]\n",
            " [0.15056528]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.72340406]\n",
            " [0.58618648]\n",
            " [0.99326318]\n",
            " [0.16971218]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.88223483]\n",
            " [0.01325084]\n",
            " [0.94230349]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.14765882]\n",
            " [0.96568416]\n",
            " [0.04516033]\n",
            " [0.26691514]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, input_size)\n",
        "# print(character_matrices)\n",
        "\n",
        "# Flatten each matrix to create input vectors\n",
        "input_vectors = character_matrices.reshape(num_characters, -1)\n",
        "\n",
        "# Step 2: Build a simple perceptron network with random weights\n",
        "num_weights = input_size\n",
        "weights = np.random.randn(num_weights, 1)\n",
        "\n",
        "# Step 3: Define activation function (sigmoid) and forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(data, weights):\n",
        "    return sigmoid(np.dot(data, weights))\n",
        "\n",
        "# Step 4: Train the perceptron network for 10 epochs\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for char_idx in range(num_characters):\n",
        "        for font_idx in range(num_fonts):\n",
        "            # Select the input vector for the current character and font\n",
        "            input_vector = input_vectors[char_idx, font_idx, :]\n",
        "\n",
        "            # Forward pass\n",
        "            output = forward_pass(input_vector, weights)\n",
        "\n",
        "            # Update weights (dummy update, not based on any real learning algorithm)\n",
        "            weights += np.random.randn(num_weights, 1) * 0.01  # Adjust the learning rate as needed\n",
        "\n",
        "# Step 5: Make predictions for each character and font\n",
        "predictions = np.zeros((num_characters, num_fonts))\n",
        "\n",
        "for char_idx in range(num_characters):\n",
        "    for font_idx in range(num_fonts):\n",
        "        input_vector = input_vectors[char_idx, font_idx, :]\n",
        "        predictions[char_idx, font_idx] = forward_pass(input_vector, weights)\n",
        "\n",
        "# Print predictions for verification\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "81ZLv7T1cFvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, 95, 95)\n",
        "\n",
        "# Flatten each matrix to create input vectors\n",
        "input_vectors = character_matrices.reshape(num_characters * num_fonts, -1)\n",
        "\n",
        "# Step 2: Build a simple perceptron network with random weights\n",
        "num_weights = input_size\n",
        "weights = np.random.randn(num_weights, 1)\n",
        "\n",
        "# Step 3: Define activation function (sigmoid) and forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(data, weights):\n",
        "    return sigmoid(np.dot(data, weights))\n",
        "\n",
        "# Step 4: Train the perceptron network for 10 epochs\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for char_font_idx in range(num_characters * num_fonts):\n",
        "        # Select the input vector for the current character and font\n",
        "        input_vector = input_vectors[char_font_idx, :]\n",
        "\n",
        "        # Forward pass\n",
        "        output = forward_pass(input_vector, weights)\n",
        "\n",
        "        # Update weights (dummy update, not based on any real learning algorithm)\n",
        "        weights += np.random.randn(num_weights, 1) * 0.01  # Adjust the learning rate as needed\n",
        "\n",
        "# Step 5: Make predictions for each character and font\n",
        "predictions = np.zeros((num_characters, num_fonts))\n",
        "\n",
        "for char_idx in range(num_characters):\n",
        "    for font_idx in range(num_fonts):\n",
        "        input_vector = input_vectors[char_idx * num_fonts + font_idx, :]\n",
        "        predictions[char_idx, font_idx] = forward_pass(input_vector, weights)\n",
        "\n",
        "# Print predictions for verification\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PST6XckVcg7g",
        "outputId": "7b319559-bcc6-44bd-abe1-9e96b0e89663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            "[[9.99999999e-01 9.99999999e-01 9.99990211e-01 1.00000000e+00\n",
            "  6.93071461e-29 1.00000000e+00 9.99999999e-01]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 1.00000000e+00\n",
            "  6.30675936e-25 1.00000000e+00 9.99999999e-01]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 1.00000000e+00\n",
            "  2.12161405e-08 1.00000000e+00 3.50337551e-42]\n",
            " [9.99999999e-01 9.99999999e-01 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 9.99999999e-01]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 1.00000000e+00\n",
            "  1.00000000e+00 9.99999610e-01 9.99999992e-01]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 1.00000000e+00\n",
            "  2.94083991e-10 2.50683270e-17 9.92651736e-01]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 1.00000000e+00\n",
            "  1.00000000e+00 2.53696036e-12 1.02337196e-09]\n",
            " [9.99999999e-01 9.99999695e-01 1.48815743e-04 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 9.99999999e-01]\n",
            " [9.99999999e-01 9.99999999e-01 1.00000000e+00 9.75734483e-01\n",
            "  1.00000000e+00 1.00000000e+00 9.99999620e-01]\n",
            " [9.99999999e-01 9.99999999e-01 1.00000000e+00 9.99999993e-01\n",
            "  1.17339290e-33 1.00000000e+00 9.99999999e-01]\n",
            " [9.99999999e-01 1.00000000e+00 1.00000000e+00 8.77370107e-15\n",
            "  1.84884252e-62 1.00000000e+00 9.99999999e-01]\n",
            " [9.99999999e-01 9.99999999e-01 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 3.44079809e-04]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999991e-01\n",
            "  1.00000000e+00 1.00000000e+00 9.99999999e-01]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99248312e-01\n",
            "  9.99991699e-01 1.00000000e+00 9.99799667e-01]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999995e-01\n",
            "  1.00000000e+00 9.99999310e-01 9.99999972e-01]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 9.99999999e-01]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 4.75425242e-21\n",
            "  1.93806973e-11 4.44989157e-24 4.60938679e-03]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, 95, 95)\n",
        "\n",
        "# Flatten each matrix to create input vectors\n",
        "input_vectors = character_matrices.reshape(num_characters * num_fonts, -1)\n",
        "\n",
        "# Step 2: Build a simple perceptron network with random weights\n",
        "num_weights = input_size\n",
        "weights = np.random.randn(num_weights, 1)\n",
        "\n",
        "# Step 3: Define activation function (sigmoid) and forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(data, weights):\n",
        "    return sigmoid(np.dot(data, weights))\n",
        "\n",
        "# Step 4: Train the perceptron network for 10 epochs\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for char_font_idx in range(num_characters * num_fonts):\n",
        "        # Select the input vector for the current character and font\n",
        "        input_vector = input_vectors[char_font_idx, :]\n",
        "\n",
        "        # Forward pass\n",
        "        output = forward_pass(input_vector, weights)\n",
        "\n",
        "        # Convert output to bipolar (-1 or 1)\n",
        "        output_bipolar = np.where(output > 0.5, 1, -1)\n",
        "\n",
        "        # Update weights (dummy update, not based on any real learning algorithm)\n",
        "        weights += np.random.randn(num_weights, 1) * 0.01  # Adjust the learning rate as needed\n",
        "\n",
        "# Step 5: Make predictions for each character and font\n",
        "predictions = np.zeros((num_characters, num_fonts))\n",
        "\n",
        "for char_idx in range(num_characters):\n",
        "    for font_idx in range(num_fonts):\n",
        "        input_vector = input_vectors[char_idx * num_fonts + font_idx, :]\n",
        "        predictions[char_idx, font_idx] = np.where(forward_pass(input_vector, weights) > 0.5, 1, -1)\n",
        "\n",
        "# Print predictions for verification\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-3uN3mwfYlf",
        "outputId": "3ed5d736-0eb3-4d87-ecc4-be2e0dec2aab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            "[[ 1.  1.  1.  1.  1.  1.  1.]\n",
            " [ 1.  1.  1.  1. -1.  1.  1.]\n",
            " [ 1.  1.  1.  1.  1. -1. -1.]\n",
            " [ 1.  1.  1. -1.  1.  1.  1.]\n",
            " [ 1.  1.  1. -1.  1.  1. -1.]\n",
            " [ 1.  1.  1.  1.  1.  1. -1.]\n",
            " [ 1.  1.  1.  1. -1. -1. -1.]\n",
            " [ 1.  1.  1. -1.  1.  1.  1.]\n",
            " [ 1.  1.  1. -1.  1. -1. -1.]\n",
            " [ 1.  1. -1.  1. -1.  1.  1.]\n",
            " [ 1.  1. -1. -1. -1.  1.  1.]\n",
            " [ 1.  1. -1. -1. -1.  1. -1.]\n",
            " [ 1.  1.  1.  1. -1. -1. -1.]\n",
            " [ 1.  1.  1. -1. -1.  1. -1.]\n",
            " [ 1.  1.  1. -1. -1. -1.  1.]\n",
            " [ 1.  1.  1.  1. -1.  1.  1.]\n",
            " [ 1.  1.  1. -1.  1. -1. -1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, 95, 95)\n",
        "\n",
        "# Flatten each matrix to create input vectors\n",
        "input_vectors = character_matrices.reshape(num_characters * num_fonts, -1)\n",
        "\n",
        "# Step 2: Build a simple perceptron network with random weights\n",
        "num_weights = input_size\n",
        "weights = np.random.randn(num_weights, 1)\n",
        "\n",
        "# Step 3: Define activation function (sigmoid) and forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(data, weights):\n",
        "    return sigmoid(np.dot(data, weights))\n",
        "\n",
        "# Step 4: Train the perceptron network for 10 epochs\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for char_font_idx in range(num_characters * num_fonts):\n",
        "        # Select the input vector for the current character and font\n",
        "        input_vector = input_vectors[char_font_idx, :]\n",
        "\n",
        "        # Forward pass\n",
        "        output = forward_pass(input_vector, weights)\n",
        "\n",
        "        # Convert output to bipolar (-1 or 1)\n",
        "        output_bipolar = np.where(output > 0.5, 1, -1)\n",
        "\n",
        "        # Update weights (dummy update, not based on any real learning algorithm)\n",
        "        weights += np.random.randn(num_weights, 1) * 0.01  # Adjust the learning rate as needed\n",
        "\n",
        "# Step 5: Make predictions for each character and font\n",
        "predictions = np.zeros((num_characters, num_fonts), dtype=bool)\n",
        "\n",
        "for char_idx in range(num_characters):\n",
        "    for font_idx in range(num_fonts):\n",
        "        input_vector = input_vectors[char_idx * num_fonts + font_idx, :]\n",
        "        predictions[char_idx, font_idx] = np.any(forward_pass(input_vector, weights) > 0.5)\n",
        "\n",
        "# Print predictions for verification\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20185e25-abcd-49db-c43d-2b3ec73942e0",
        "id": "0NFqqvMAgLr4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            "[[False False  True  True False False False]\n",
            " [False False False  True False False False]\n",
            " [False False False  True  True False False]\n",
            " [False False False  True False False False]\n",
            " [False False False False False False False]\n",
            " [False False False False  True False False]\n",
            " [False False False  True  True False False]\n",
            " [False False False  True  True False False]\n",
            " [False False False False  True False False]\n",
            " [False False False  True  True False False]\n",
            " [False False False  True  True False False]\n",
            " [False False False False False False False]\n",
            " [False False False False  True False False]\n",
            " [False False False False  True False False]\n",
            " [False False False False False  True False]\n",
            " [False False False  True  True False False]\n",
            " [False False False False  True False False]]\n"
          ]
        }
      ]
    }
  ]
}