{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/DataScienceJourney/blob/master/FaceTrackNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# G sol"
      ],
      "metadata": {
        "id": "9FuOAC6yMVsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install roboflow  # For dataset management\n",
        "!pip install ultralytics  # For YOLO implementation\n",
        "!pip install opencv-python  # For image processing\n",
        "!pip install numpy  # For numerical operations\n",
        "\n",
        "# Import required libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from roboflow import Roboflow\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Avoid locale issues by setting encoding explicitly (no locale change needed)\n",
        "import sys\n",
        "sys.stdout.reconfigure(encoding='utf-8')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3cVsas2IxiEt",
        "outputId": "77bea1e8-366c-49e7-e883-29580bc1bdee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.11/dist-packages (1.1.54)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.1.31)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.1.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.3.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.56.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.1)\n",
            "Collecting ultralytics\n",
            "  Using cached ultralytics-8.3.81-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Using cached ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.81-py3-none-any.whl (922 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m922.1/922.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.81 ultralytics-thop-2.0.14\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'OutStream' object has no attribute 'reconfigure'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-235108ed35e4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Avoid locale issues by setting encoding explicitly (no locale change needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'OutStream' object has no attribute 'reconfigure'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download your specific face detection dataset from Roboflow\n",
        "rf = Roboflow(api_key=\"2IGtFaicFMGaMwb2mX8A\")\n",
        "project = rf.workspace(\"mohamed-traore-2ekkp\").project(\"face-detection-mik1i\")\n",
        "version = project.version(25)\n",
        "dataset = version.download(\"yolov11\")\n",
        "\n",
        "# Get the path to the downloaded dataset's YAML file\n",
        "dataset_yaml = f\"{dataset.location}/data.yaml\"\n",
        "print(f\"Dataset YAML file located at: {dataset_yaml}\")\n",
        "\n",
        "# Load the YOLOv11 model using your approach\n",
        "# Option 1: Build a new model from YAML (untrained)\n",
        "model = YOLO(\"yolo11n.yaml\")  # This creates a fresh model structure\n",
        "\n",
        "# Option 2: Load a pretrained model (recommended if you want to fine-tune)\n",
        "model = YOLO(\"yolo11n.pt\")  # Loads pre-trained weights (automatically downloaded)\n",
        "\n",
        "# Option 3: Build from YAML and transfer pretrained weights (combines both)\n",
        "model = YOLO(\"yolo11n.yaml\").load(\"yolo11n.pt\")  # Recommended for your case\n",
        "\n",
        "# For this project, we'll use Option 3 as it matches your code and allows training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzPnYcNLykiC",
        "outputId": "5903fd60-68f1-4aa7-9f2d-a4fa8e5dddc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Face-Detection-25 to yolov11:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 158791/158791 [00:02<00:00, 56718.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Face-Detection-25 in yolov11:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8552/8552 [00:01<00:00, 7144.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset YAML file located at: /content/Face-Detection-25/data.yaml\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 106MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transferred 499/499 items from pretrained weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on your Roboflow dataset\n",
        "results = model.train(\n",
        "    data=dataset_yaml,  # Use your downloaded dataset's YAML file\n",
        "    epochs=10,         # Number of training epochs\n",
        "    imgsz=640,          # Image size (adjust based on your needs/performance)\n",
        "    batch=16,           # Batch size (adjust based on Colab GPU memory)\n",
        "    name='face_detection_yolo11n'  # Name for the training run\n",
        ")\n",
        "\n",
        "# After training, the model will be saved automatically\n",
        "print(\"Training complete. Model weights saved in runs/train/face_detection_yolo11n/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw-0knq7yst_",
        "outputId": "1354e286-a156-4f86-e6d0-504a46719091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.81 ðŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.yaml, data=/content/Face-Detection-25/data.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=face_detection_yolo11n2, exist_ok=False, pretrained=yolo11n.pt, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/face_detection_yolo11n2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
            "\n",
            "Transferred 499/499 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/face_detection_yolo11n2', view at http://localhost:6006/\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/Face-Detection-25/train/labels.cache... 4068 images, 708 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4068/4068 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Face-Detection-25/valid/labels.cache... 38 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/face_detection_yolo11n2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/face_detection_yolo11n2\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/10      4.73G      1.328       1.17      1.378          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [01:12<00:00,  3.51it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         38         49      0.931      0.796      0.942      0.524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/10      4.85G      1.336       1.14      1.392          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [01:08<00:00,  3.70it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         38         49      0.895      0.872      0.942       0.63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/10      4.93G      1.334      1.089      1.389          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [01:09<00:00,  3.69it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         38         49      0.747      0.837      0.854      0.421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/10      4.69G       1.31      1.079      1.369          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [01:07<00:00,  3.75it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         38         49      0.812      0.918      0.938      0.635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/10      4.67G      1.252     0.9864      1.332          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [01:08<00:00,  3.73it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         38         49      0.878      0.918      0.937      0.625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/10       4.7G        1.2     0.9132      1.295         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [01:08<00:00,  3.73it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         38         49      0.883      0.924      0.961      0.621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/10      4.72G       1.15     0.8477      1.259         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [01:07<00:00,  3.75it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         38         49      0.978      0.916      0.945      0.641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/10      4.76G      1.098     0.7891      1.218          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [01:08<00:00,  3.71it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         38         49      0.959      0.958      0.979      0.759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/10      4.58G      1.043     0.7322      1.178          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [01:08<00:00,  3.74it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         38         49      0.928      0.918      0.971      0.753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/10      4.41G     0.9907     0.6801      1.154          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [01:08<00:00,  3.74it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         38         49      0.957      0.959      0.983      0.744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10 epochs completed in 0.196 hours.\n",
            "Optimizer stripped from runs/detect/face_detection_yolo11n2/weights/last.pt, 5.5MB\n",
            "Optimizer stripped from runs/detect/face_detection_yolo11n2/weights/best.pt, 5.5MB\n",
            "\n",
            "Validating runs/detect/face_detection_yolo11n2/weights/best.pt...\n",
            "Ultralytics 8.3.81 ðŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         38         49      0.959      0.958      0.979      0.759\n",
            "Speed: 0.3ms preprocess, 3.1ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/face_detection_yolo11n2\u001b[0m\n",
            "Training complete. Model weights saved in runs/train/face_detection_yolo11n/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from roboflow import Roboflow\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_age_gender_models():\n",
        "    # Set locale to UTF-8 before downloading models\n",
        "    locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
        "\n",
        "    # Download pre-trained models\n",
        "    !wget https://github.com/GilLevi/AgeGenderDeepLearning/raw/master/models/age_net.caffemodel\n",
        "    !wget https://raw.githubusercontent.com/GilLevi/AgeGenderDeepLearning/master/models/age_deploy.prototxt\n",
        "    !wget https://github.com/GilLevi/AgeGenderDeepLearning/raw/master/models/gender_net.caffemodel\n",
        "    !wget https://raw.githubusercontent.com/GilLevi/AgeGenderDeepLearning/master/models/gender_deploy.prototxt\n",
        "\n",
        "    # Load the models\n",
        "    age_net = cv2.dnn.readNet(\"age_net.caffemodel\", \"age_deploy.prototxt\")\n",
        "    gender_net = cv2.dnn.readNet(\"gender_net.caffemodel\", \"gender_deploy.prototxt\")\n",
        "\n",
        "    return age_net, gender_net\n",
        "\n",
        "def predict_age_gender(face, age_net, gender_net):\n",
        "    # ... (Your existing predict_age_gender function code) ...\n",
        "    # Placeholder for your age and gender prediction logic\n",
        "    # This function should return the predicted gender and age\n",
        "    MODEL_MEAN_VALUES = (78.426337771, 87.7689143744, 100.290166006)\n",
        "    age_list = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
        "    gender_list = ['Male', 'Female']\n",
        "\n",
        "    blob = cv2.dnn.blobFromImage(face, 1, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
        "\n",
        "    # Predict Gender\n",
        "    gender_net.setInput(blob)\n",
        "    gender_preds = gender_net.forward()\n",
        "    gender = gender_list[np.argmax(gender_preds)]\n",
        "\n",
        "    # Predict Age\n",
        "    age_net.setInput(blob)\n",
        "    age_preds = age_net.forward()\n",
        "    age = age_list[np.argmax(age_preds)]\n",
        "\n",
        "    return gender, age\n",
        "\n",
        "def process_image(image_path, age_net, gender_net): # Pass age_net and gender_net as arguments\n",
        "    # Read the image\n",
        "    img = cv2.imread(image_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Perform face detection with the trained YOLO model\n",
        "    results = model(img_rgb)\n",
        "\n",
        "    # Process detections\n",
        "    for r in results:\n",
        "        boxes = r.boxes\n",
        "        for box in boxes:\n",
        "            # Get coordinates\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "\n",
        "            # Extract face region\n",
        "            face = img_rgb[y1:y2, x1:x2]\n",
        "\n",
        "            # Predict age and gender\n",
        "            gender, age = predict_age_gender(face, age_net, gender_net) # Call predict_age_gender\n",
        "\n",
        "            # Draw bounding box and label\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            label = f\"{gender}, {age}\"\n",
        "            cv2.putText(img, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                       0.9, (0, 255, 0), 2)\n",
        "\n",
        "    # Display result\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage: Upload an image to Colab and test\n",
        "# 1. Upload an image via Colab's file upload (left sidebar)\n",
        "# 2. Replace 'your_image.jpg' with the actual filename\n",
        "age_net, gender_net = load_age_gender_models() # Load models before calling process_image\n",
        "process_image('SingleFace.jpg', age_net, gender_net) # Pass age_net and gender_net to process_image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "5JFsmDrl2kWM",
        "outputId": "ccd6881c-dad8-4015-84dc-06fd7607dd57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "A UTF-8 locale is required. Got ANSI_X3.4-1968",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d85bcdbe958c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# 1. Upload an image via Colab's file upload (left sidebar)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# 2. Replace 'your_image.jpg' with the actual filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mage_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_age_gender_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Load models before calling process_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SingleFace.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender_net\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Pass age_net and gender_net to process_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-d85bcdbe958c>\u001b[0m in \u001b[0;36mload_age_gender_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Download pre-trained models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget https://github.com/GilLevi/AgeGenderDeepLearning/raw/master/models/age_net.caffemodel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget https://raw.githubusercontent.com/GilLevi/AgeGenderDeepLearning/master/models/age_deploy.prototxt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget https://github.com/GilLevi/AgeGenderDeepLearning/raw/master/models/gender_net.caffemodel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    169\u001b[0m           \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image(image_path):\n",
        "    # Read the image\n",
        "    img = cv2.imread(image_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Perform face detection with the trained YOLO model\n",
        "    results = model(img_rgb)\n",
        "\n",
        "    # Process detections\n",
        "    for r in results:\n",
        "        boxes = r.boxes\n",
        "        for box in boxes:\n",
        "            # Get coordinates\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "\n",
        "            # Extract face region\n",
        "            face = img_rgb[y1:y2, x1:x2]\n",
        "\n",
        "            # Predict age and gender\n",
        "            gender, age = predict_age_gender(face, age_net, gender_net)\n",
        "\n",
        "            # Draw bounding box and label\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            label = f\"{gender}, {age}\"\n",
        "            cv2.putText(img, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                       0.9, (0, 255, 0), 2)\n",
        "\n",
        "    # Display result\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage: Upload an image to Colab and test\n",
        "# 1. Upload an image via Colab's file upload (left sidebar)\n",
        "# 2. Replace 'your_image.jpg' with the actual filename\n",
        "process_image('SingleFace.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "3249gn4v21lo",
        "outputId": "207af87e-1dcf-448a-aa96-f341a72fd515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 416x640 2 faces, 47.8ms\n",
            "Speed: 3.1ms preprocess, 47.8ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'age_net' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-260f2a65b05b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# 1. Upload an image via Colab's file upload (left sidebar)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# 2. Replace 'your_image.jpg' with the actual filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SingleFace.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-260f2a65b05b>\u001b[0m in \u001b[0;36mprocess_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Predict age and gender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mgender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_age_gender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Draw bounding box and label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'age_net' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error `NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968` persists because Colab's default environment doesn't support changing the locale using `locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')` directly. This is a limitation of Colab's runtime, which uses a minimal POSIX environment. Instead, we can use a simpler workaround by overriding the encoding check or avoiding locale changes altogether.\n",
        "\n",
        "Hereâ€™s the corrected code, rewritten step-by-step, avoiding the locale issue while keeping your structure intact:\n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: Import Libraries and Set Up Environment\n",
        "\n",
        "```python\n",
        "# Import required libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from roboflow import Roboflow\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Avoid locale issues by setting encoding explicitly (no locale change needed)\n",
        "import sys\n",
        "sys.stdout.reconfigure(encoding='utf-8')\n",
        "```\n",
        "\n",
        "**Explanation**:\n",
        "- Removed `locale` manipulation since `locale.setlocale` isnâ€™t supported in Colab.\n",
        "- Used `sys.stdout.reconfigure` to ensure UTF-8 encoding for output, which avoids the error.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Load YOLO Model and Dataset\n",
        "\n",
        "```python\n",
        "# Download your specific face detection dataset from Roboflow\n",
        "rf = Roboflow(api_key=\"2IGtFaicFMGaMwb2mX8A\")\n",
        "project = rf.workspace(\"mohamed-traore-2ekkp\").project(\"face-detection-mik1i\")\n",
        "version = project.version(25)\n",
        "dataset = version.download(\"yolov11\")\n",
        "\n",
        "# Get the path to the downloaded dataset's YAML file\n",
        "dataset_yaml = f\"{dataset.location}/data.yaml\"\n",
        "print(f\"Dataset YAML file located at: {dataset_yaml}\")\n",
        "\n",
        "# Load the YOLOv11 model (using your preferred approach)\n",
        "model = YOLO(\"yolo11n.yaml\").load(\"yolo11n.pt\")  # Build from YAML and transfer weights\n",
        "```\n",
        "\n",
        "**Notes**:\n",
        "- This step remains largely unchanged but ensures the model is available for inference.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Define and Load Age/Gender Models\n",
        "\n",
        "```python\n",
        "def load_age_gender_models():\n",
        "    # Download pre-trained models (quietly to reduce output)\n",
        "    !wget -q https://github.com/GilLevi/AgeGenderDeepLearning/raw/master/models/age_net.caffemodel\n",
        "    !wget -q https://raw.githubusercontent.com/GilLevi/AgeGenderDeepLearning/master/models/age_deploy.prototxt\n",
        "    !wget -q https://github.com/GilLevi/AgeGenderDeepLearning/raw/master/models/gender_net.caffemodel\n",
        "    !wget -q https://raw.githubusercontent.com/GilLevi/AgeGenderDeepLearning/master/models/gender_deploy.prototxt\n",
        "\n",
        "    # Load the models\n",
        "    age_net = cv2.dnn.readNet(\"age_net.caffemodel\", \"age_deploy.prototxt\")\n",
        "    gender_net = cv2.dnn.readNet(\"gender_net.caffemodel\", \"gender_deploy.prototxt\")\n",
        "    \n",
        "    return age_net, gender_net\n",
        "```\n",
        "\n",
        "**Explanation**:\n",
        "- Removed `locale.setlocale` since itâ€™s unnecessary and causes the error.\n",
        "- The downloads and model loading should work fine without locale changes in Colab.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 4: Define Age/Gender Prediction Function\n",
        "\n",
        "```python\n",
        "def predict_age_gender(face, age_net, gender_net):\n",
        "    # Mean values for preprocessing\n",
        "    MODEL_MEAN_VALUES = (78.426337771, 87.7689143744, 100.290166006)\n",
        "    age_list = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
        "    gender_list = ['Male', 'Female']\n",
        "\n",
        "    # Preprocess the face image\n",
        "    blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
        "\n",
        "    # Predict Gender\n",
        "    gender_net.setInput(blob)\n",
        "    gender_preds = gender_net.forward()\n",
        "    gender = gender_list[np.argmax(gender_preds)]\n",
        "\n",
        "    # Predict Age\n",
        "    age_net.setInput(blob)\n",
        "    age_preds = age_net.forward()\n",
        "    age = age_list[np.argmax(age_preds)]\n",
        "\n",
        "    return gender, age\n",
        "```\n",
        "\n",
        "**Notes**:\n",
        "- This is your original function, unchanged, and should work fine once the models are loaded.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 5: Process Image with Face Detection and Age/Gender Prediction\n",
        "\n",
        "```python\n",
        "def process_image(image_path, age_net, gender_net):\n",
        "    # Read the image\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Error: Could not load image at {image_path}\")\n",
        "        return\n",
        "    \n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Perform face detection with the YOLO model\n",
        "    results = model(img_rgb)\n",
        "    \n",
        "    # Process detections\n",
        "    for r in results:\n",
        "        boxes = r.boxes\n",
        "        for box in boxes:\n",
        "            # Get coordinates\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "            \n",
        "            # Ensure coordinates are within image bounds\n",
        "            h, w = img_rgb.shape[:2]\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(w, x2), min(h, y2)\n",
        "            \n",
        "            # Extract face region (check if region is valid)\n",
        "            if x2 > x1 and y2 > y1:\n",
        "                face = img_rgb[y1:y2, x1:x2]\n",
        "                # Predict age and gender\n",
        "                gender, age = predict_age_gender(face, age_net, gender_net)\n",
        "                \n",
        "                # Draw bounding box and label\n",
        "                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                label = f\"{gender}, {age}\"\n",
        "                cv2.putText(img, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                           0.9, (0, 255, 0), 2)\n",
        "            else:\n",
        "                print(\"Invalid face region detected\")\n",
        "    \n",
        "    # Display result\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Load age and gender models\n",
        "age_net, gender_net = load_age_gender_models()\n",
        "\n",
        "# Process the image (ensure the file exists in Colab)\n",
        "process_image('SingleFace.jpg', age_net, gender_net)\n",
        "```\n",
        "\n",
        "**Changes**:\n",
        "- Added error checking for image loading and face region extraction to prevent crashes.\n",
        "- Kept your structure but made it more robust.\n",
        "\n",
        "---\n",
        "\n",
        "### Running in Colab\n",
        "1. **Start a New Notebook**: Open Google Colab and create a new notebook.\n",
        "2. **Copy Each Step**: Paste each block into a separate cell.\n",
        "3. **Upload Image**: Upload `SingleFace.jpg` to Colab using the file upload button in the left sidebar.\n",
        "4. **Run Cells**: Execute each cell sequentially.\n",
        "\n",
        "---\n",
        "\n",
        "### Troubleshooting\n",
        "If you still get errors:\n",
        "1. **Locale Error Persists**:\n",
        "   - Replace the `sys.stdout.reconfigure` fix with:\n",
        "     ```python\n",
        "     import os\n",
        "     os.environ[\"PYTHONIOENCODING\"] = \"utf-8\"\n",
        "     ```\n",
        "   - Restart the runtime (`Runtime > Restart Runtime`) and rerun all cells.\n",
        "2. **Image Not Found**:\n",
        "   - Verify `SingleFace.jpg` is uploaded and spelled correctly. Use `!ls` to list files in Colab.\n",
        "3. **Other Errors**:\n",
        "   - Let me know the exact error message and which step it occurs in.\n",
        "\n",
        "This should resolve the locale issue and run your code as intended. Let me know how it works or if you need further tweaks!"
      ],
      "metadata": {
        "id": "xNuJ6ig93Zjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# added C"
      ],
      "metadata": {
        "id": "gRZ0_oPdMbcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import Libraries and Set Up Environment\n",
        "\n",
        "```python\n",
        "# Import required libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "from roboflow import Roboflow\n",
        "\n",
        "# Handle encoding without using reconfigure\n",
        "import os\n",
        "os.environ[\"PYTHONIOENCODING\"] = \"utf-8\"\n",
        "```\n",
        "\n",
        "### Step 2: Load YOLO Model and Dataset\n",
        "\n",
        "```python\n",
        "# Download your specific face detection dataset from Roboflow\n",
        "rf = Roboflow(api_key=\"2IGtFaicFMGaMwb2mX8A\")\n",
        "project = rf.workspace(\"mohamed-traore-2ekkp\").project(\"face-detection-mik1i\")\n",
        "version = project.version(25)\n",
        "dataset = version.download(\"yolov11\")\n",
        "\n",
        "# Get the path to the downloaded dataset's YAML file\n",
        "dataset_yaml = f\"{dataset.location}/data.yaml\"\n",
        "print(f\"Dataset YAML file located at: {dataset_yaml}\")\n",
        "\n",
        "# Load the YOLOv11 model\n",
        "model = YOLO(\"yolo11n.yaml\").load(\"yolo11n.pt\")  # Build from YAML and transfer weights\n",
        "```\n",
        "\n",
        "### Step 3: Define and Load Age/Gender Models\n",
        "\n",
        "```python\n",
        "def load_age_gender_models():\n",
        "    \"\"\"Download and load the pre-trained age and gender models\"\"\"\n",
        "    \n",
        "    # Download pre-trained models (use curl for better compatibility)\n",
        "    !curl -s -L https://github.com/GilLevi/AgeGenderDeepLearning/raw/master/models/age_net.caffemodel -o age_net.caffemodel\n",
        "    !curl -s -L https://raw.githubusercontent.com/GilLevi/AgeGenderDeepLearning/master/models/age_deploy.prototxt -o age_deploy.prototxt\n",
        "    !curl -s -L https://github.com/GilLevi/AgeGenderDeepLearning/raw/master/models/gender_net.caffemodel -o gender_net.caffemodel\n",
        "    !curl -s -L https://raw.githubusercontent.com/GilLevi/AgeGenderDeepLearning/master/models/gender_deploy.prototxt -o gender_deploy.prototxt\n",
        "    \n",
        "    # Check if files were downloaded successfully\n",
        "    for file in [\"age_net.caffemodel\", \"age_deploy.prototxt\", \"gender_net.caffemodel\", \"gender_deploy.prototxt\"]:\n",
        "        if not os.path.exists(file):\n",
        "            raise FileNotFoundError(f\"Failed to download: {file}\")\n",
        "    \n",
        "    # Load the models\n",
        "    age_net = cv2.dnn.readNet(\"age_net.caffemodel\", \"age_deploy.prototxt\")\n",
        "    gender_net = cv2.dnn.readNet(\"gender_net.caffemodel\", \"gender_deploy.prototxt\")\n",
        "    \n",
        "    print(\"Age and gender models loaded successfully\")\n",
        "    return age_net, gender_net\n",
        "```\n",
        "\n",
        "### Step 4: Define Age/Gender Prediction Function\n",
        "\n",
        "```python\n",
        "def predict_age_gender(face, age_net, gender_net):\n",
        "    \"\"\"Predict age and gender from a face image\"\"\"\n",
        "    \n",
        "    # Mean values for preprocessing\n",
        "    MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
        "    age_list = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
        "    gender_list = ['Male', 'Female']\n",
        "\n",
        "    # Make a copy of the face to avoid modifying the original\n",
        "    face_copy = face.copy()\n",
        "    \n",
        "    # Resize to the required input size\n",
        "    face_blob = cv2.resize(face_copy, (227, 227))\n",
        "    \n",
        "    # Create blob from image\n",
        "    blob = cv2.dnn.blobFromImage(face_blob, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
        "\n",
        "    # Predict Gender\n",
        "    gender_net.setInput(blob)\n",
        "    gender_preds = gender_net.forward()\n",
        "    gender = gender_list[np.argmax(gender_preds[0])]\n",
        "\n",
        "    # Predict Age\n",
        "    age_net.setInput(blob)\n",
        "    age_preds = age_net.forward()\n",
        "    age = age_list[np.argmax(age_preds[0])]\n",
        "\n",
        "    return gender, age\n",
        "```\n",
        "\n",
        "### Step 5: Process Image with Face Detection and Age/Gender Prediction\n",
        "\n",
        "```python\n",
        "def process_image(image_path, model, age_net, gender_net):\n",
        "    \"\"\"Process an image to detect faces and predict age/gender\"\"\"\n",
        "    \n",
        "    # Read the image\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Error: Could not load image at {image_path}\")\n",
        "        return None\n",
        "    \n",
        "    # Convert to RGB for display with matplotlib\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Create a copy for drawing\n",
        "    result_img = img_rgb.copy()\n",
        "    \n",
        "    # Perform face detection with the YOLO model\n",
        "    results = model(img_rgb)\n",
        "    \n",
        "    face_count = 0\n",
        "    \n",
        "    # Process detections\n",
        "    for r in results:\n",
        "        boxes = r.boxes\n",
        "        for box in boxes:\n",
        "            # Get coordinates\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "            \n",
        "            # Get confidence\n",
        "            conf = float(box.conf[0])\n",
        "            \n",
        "            # Only process high-confidence detections\n",
        "            if conf < 0.5:\n",
        "                continue\n",
        "                \n",
        "            # Ensure coordinates are within image bounds\n",
        "            h, w = img_rgb.shape[:2]\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(w, x2), min(h, y2)\n",
        "            \n",
        "            # Extract face region (check if region is valid)\n",
        "            if x2 > x1 and y2 > y1 and (x2-x1)*(y2-y1) > 100:  # Minimum size check\n",
        "                face = img_rgb[y1:y2, x1:x2]\n",
        "                \n",
        "                # Predict age and gender\n",
        "                try:\n",
        "                    gender, age = predict_age_gender(face, age_net, gender_net)\n",
        "                    \n",
        "                    # Draw bounding box and label\n",
        "                    cv2.rectangle(result_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                    label = f\"{gender}, {age}\"\n",
        "                    \n",
        "                    # Draw label with better background for visibility\n",
        "                    label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
        "                    cv2.rectangle(result_img, (x1, y1-label_size[1]-10), (x1+label_size[0], y1), (0, 255, 0), -1)\n",
        "                    cv2.putText(result_img, label, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
        "                    \n",
        "                    face_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing face: {e}\")\n",
        "    \n",
        "    print(f\"Detected {face_count} faces\")\n",
        "    \n",
        "    # Display result\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.imshow(result_img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Detected {face_count} faces\")\n",
        "    plt.show()\n",
        "    \n",
        "    return result_img\n",
        "```\n",
        "\n",
        "### Step 6: Main Execution Code\n",
        "\n",
        "```python\n",
        "def main():\n",
        "    \"\"\"Main function to run the face detection and age/gender prediction\"\"\"\n",
        "    try:\n",
        "        # Load age and gender models\n",
        "        print(\"Loading age and gender models...\")\n",
        "        age_net, gender_net = load_age_gender_models()\n",
        "        \n",
        "        # Check if an image exists, if not, use a sample image\n",
        "        image_path = 'SingleFace.jpg'\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"Warning: {image_path} not found. Please upload an image or specify the correct path.\")\n",
        "            return\n",
        "        \n",
        "        # Process the image\n",
        "        print(f\"Processing image: {image_path}\")\n",
        "        result = process_image(image_path, model, age_net, gender_net)\n",
        "        \n",
        "        if result is not None:\n",
        "            print(\"Processing completed successfully\")\n",
        "            \n",
        "            # Optionally save the result\n",
        "            cv2.imwrite(\"result.jpg\", cv2.cvtColor(result, cv2.COLOR_RGB2BGR))\n",
        "            print(\"Result saved as 'result.jpg'\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "```\n",
        "\n",
        "### Step 7: Complete Running Instructions for Colab\n",
        "\n",
        "```python\n",
        "# To run this in Colab, execute this cell to install dependencies\n",
        "!pip install ultralytics roboflow\n",
        "```"
      ],
      "metadata": {
        "id": "xsyGRQLhMl4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "from roboflow import Roboflow\n",
        "\n",
        "# Handle encoding without using reconfigure\n",
        "import os\n",
        "os.environ[\"PYTHONIOENCODING\"] = \"utf-8\""
      ],
      "metadata": {
        "id": "P5V-DZ_3Mn4Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download your specific face detection dataset from Roboflow\n",
        "rf = Roboflow(api_key=\"2IGtFaicFMGaMwb2mX8A\")\n",
        "project = rf.workspace(\"mohamed-traore-2ekkp\").project(\"face-detection-mik1i\")\n",
        "version = project.version(25)\n",
        "dataset = version.download(\"yolov11\")\n",
        "\n",
        "# Get the path to the downloaded dataset's YAML file\n",
        "dataset_yaml = f\"{dataset.location}/data.yaml\"\n",
        "print(f\"Dataset YAML file located at: {dataset_yaml}\")\n",
        "\n",
        "# Load the YOLOv11 model\n",
        "model = YOLO(\"yolo11n.yaml\").load(\"yolo11n.pt\")  # Build from YAML and transfer weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSALS_jdMvWl",
        "outputId": "24306ea4-2274-4cbd-bfca-470af2a93d78"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Face-Detection-25 to yolov11:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 158791/158791 [00:03<00:00, 49178.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Face-Detection-25 in yolov11:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8552/8552 [00:01<00:00, 6007.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset YAML file located at: /content/Face-Detection-25/data.yaml\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 108MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transferred 499/499 items from pretrained weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_age_gender_models():\n",
        "    \"\"\"Download and load the pre-trained age and gender models\"\"\"\n",
        "\n",
        "    # Download pre-trained models (use curl for better compatibility)\n",
        "    !curl -s -L https://github.com/GilLevi/AgeGenderDeepLearning/raw/master/models/age_net.caffemodel -o age_net.caffemodel\n",
        "    !curl -s -L https://raw.githubusercontent.com/GilLevi/AgeGenderDeepLearning/master/models/age_deploy.prototxt -o age_deploy.prototxt\n",
        "    !curl -s -L https://github.com/GilLevi/AgeGenderDeepLearning/raw/master/models/gender_net.caffemodel -o gender_net.caffemodel\n",
        "    !curl -s -L https://raw.githubusercontent.com/GilLevi/AgeGenderDeepLearning/master/models/gender_deploy.prototxt -o gender_deploy.prototxt\n",
        "\n",
        "    # Check if files were downloaded successfully\n",
        "    for file in [\"age_net.caffemodel\", \"age_deploy.prototxt\", \"gender_net.caffemodel\", \"gender_deploy.prototxt\"]:\n",
        "        if not os.path.exists(file):\n",
        "            raise FileNotFoundError(f\"Failed to download: {file}\")\n",
        "\n",
        "    # Load the models\n",
        "    age_net = cv2.dnn.readNet(\"age_net.caffemodel\", \"age_deploy.prototxt\")\n",
        "    gender_net = cv2.dnn.readNet(\"gender_net.caffemodel\", \"gender_deploy.prototxt\")\n",
        "\n",
        "    print(\"Age and gender models loaded successfully\")\n",
        "    return age_net, gender_net"
      ],
      "metadata": {
        "id": "a1DkiYkQM5rF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_age_gender(face, age_net, gender_net):\n",
        "    \"\"\"Predict age and gender from a face image\"\"\"\n",
        "\n",
        "    # Mean values for preprocessing\n",
        "    MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
        "    age_list = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
        "    gender_list = ['Male', 'Female']\n",
        "\n",
        "    # Make a copy of the face to avoid modifying the original\n",
        "    face_copy = face.copy()\n",
        "\n",
        "    # Resize to the required input size\n",
        "    face_blob = cv2.resize(face_copy, (227, 227))\n",
        "\n",
        "    # Create blob from image\n",
        "    blob = cv2.dnn.blobFromImage(face_blob, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
        "\n",
        "    # Predict Gender\n",
        "    gender_net.setInput(blob)\n",
        "    gender_preds = gender_net.forward()\n",
        "    gender = gender_list[np.argmax(gender_preds[0])]\n",
        "\n",
        "    # Predict Age\n",
        "    age_net.setInput(blob)\n",
        "    age_preds = age_net.forward()\n",
        "    age = age_list[np.argmax(age_preds[0])]\n",
        "\n",
        "    return gender, age"
      ],
      "metadata": {
        "id": "rsUBIYY9NAon"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}