{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/DataScienceJourney/blob/master/Homework_3_Student_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEYjJmbFQ8TF"
      },
      "source": [
        "# **Rayan International AI Contest**\n",
        "\n",
        "<font color='black' style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'><strong>Machine Learning & Deep Learning Course </strong></font>\n",
        "\n",
        "<font color='black' style='font-family: \"Times New Roman\", Times, serif; font-size: 20px;'><em>**Homework 3** || **Title**: Variational Autoencoder</em></font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwLYWx0jbIg"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "os9f7omcnCVZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rgOeF51jbIn"
      },
      "source": [
        "## Parameter Settings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5fpNu7MvHph"
      },
      "source": [
        "Here we define some key hyperparameters:\n",
        "\n",
        "- **Latent Dimensions (`latent_dims = 2`)**: Allows for easy 2D visualization of the results.\n",
        "- **Epochs (`num_epochs = 30`)**: The model will train for 30 iterations.\n",
        "- **Batch Size (`batch_size = 128`)**: 128 samples are processed at a time.\n",
        "- **Capacity (`capacity = 64`)**: Determines the model's initial complexity.\n",
        "- **Learning Rate (`learning_rate = 1e-3`)**: Controls the step size during optimization.\n",
        "- **Variational Beta (`variational_beta = 1`)**: Balances reconstruction and regularization.\n",
        "- **Use GPU (`use_gpu = True`)**: Enables faster training with GPU support.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qg0qkm8pjbIn"
      },
      "outputs": [],
      "source": [
        "latent_dims = 2\n",
        "num_epochs = 30\n",
        "batch_size = 128\n",
        "capacity = 64\n",
        "learning_rate = 1e-3\n",
        "variational_beta = 1\n",
        "use_gpu = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpQ_YjnSjbIo"
      },
      "source": [
        "We will use MNIST as always. Fill the required lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjoLLbqxjbIo",
        "outputId": "8f6461f1-fa0a-4151-d84c-2facb0eb89d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:02<00:00, 4589526.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 134332.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1115440.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4119033.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load the training and test datasets\n",
        "train_dataset = MNIST(root='./data', train=True, transform=img_transform, download=True)\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = MNIST(root='./data', train=False, transform=img_transform, download=True)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JLPpBOm1cuB"
      },
      "source": [
        "## Variational Autoencoder (VAE) Implementation\n",
        "\n",
        "This code defines a Variational Autoencoder (VAE) using PyTorch. A VAE is a type of generative model that learns to encode input data into a latent space, then decode it back to the original data distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q36hKn9U1mYf"
      },
      "source": [
        "### **Encoder Class**\n",
        "The `Encoder` class maps input images into a latent space:\n",
        "- **Convolutional Layers (`conv1`, `conv2`)**: Reduce the spatial dimensions of the input while increasing depth (channels) to extract features.\n",
        "- **Fully Connected Layers (`fc_mu`, `fc_logvar`)**: Map the output features to the mean (`mu`) and log variance (`logvar`) of the latent space distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbbgX_3-2AQA"
      },
      "source": [
        "The output of convolutional layer should be passed to 2 Fully Connected Layers. One will predict the mu and the other will predict the log variance of the given image.\n",
        "\n",
        "These 2 values are used to sample from the space of images that will allow the decoder to produce a corresponding image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fj36YPwa1a03"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        c = capacity\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=c, kernel_size=4, stride=2, padding=1)  # out: c x 14 x 14\n",
        "        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1)  # out: c*2 x 7 x 7\n",
        "\n",
        "        # Flattened size is c*2 * 7 * 7\n",
        "        self.fc_mu = nn.Linear(in_features=c*2 * 7 * 7, out_features=latent_dims)  # for mu (mean)\n",
        "        self.fc_logvar = nn.Linear(in_features=c*2 * 7 * 7, out_features=latent_dims)  # for logvar (log of variance)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply two convolutional layers with ReLU activations\n",
        "        x = F.relu(self.conv1(x))  # out: c x 14 x 14\n",
        "        x = F.relu(self.conv2(x))  # out: c*2 x 7 x 7\n",
        "\n",
        "        # Flatten the output from the convolutional layers to a vector\n",
        "        x = x.view(x.size(0), -1)  # batch_size x (c*2 * 7 * 7)\n",
        "\n",
        "        # Pass the flattened vector through the fully connected layers for mean and log variance\n",
        "        x_mu = self.fc_mu(x)       # out: batch_size x latent_dims (mean)\n",
        "        x_logvar = self.fc_logvar(x)  # out: batch_size x latent_dims (log variance)\n",
        "\n",
        "        return x_mu, x_logvar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm7mu07b2nR3"
      },
      "source": [
        "### **Decoder Class**\n",
        "The `Decoder` class reconstructs the input data from the latent space:\n",
        "- **Fully Connected Layer (`fc`)**: Expands the latent vector back into a feature map.\n",
        "- **Transpose Convolutional Layers (`conv2`, `conv1`)**: Upsample the feature maps back to the original image size.\n",
        "- **Sigmoid Activation**: Applied in the last layer to produce outputs suitable for binary cross-entropy loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMQscoB223hO"
      },
      "source": [
        "Decoder is like the opposite of the encoder. Takes in two numbers and outputs an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V0vl8P92swH"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        c = capacity\n",
        "        self.fc = nn.Linear(in_features=latent_dims, out_features=c*2*7*7)\n",
        "        # For architecture of the cnn use the reverse of the one used in Encoder\n",
        "        # Remeber to use Transpose Convolution instead of the regular one\n",
        "        self.conv2 = # TODO\n",
        "        self.conv1 = # TODO\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(x.size(0), capacity*2, 7, 7) # unflatten batch of feature vectors to a batch of multi-channel feature maps\n",
        "        x = # TODO\n",
        "        x = F.relu(x)\n",
        "        x = # TODO\n",
        "        x = torch.sigmoid(x) # last layer before output is sigmoid, since we are using BCE as reconstruction loss\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyFTbVAN37To"
      },
      "source": [
        "### **VariationalAutoencoder Class**\n",
        "Now it is time to define the VAE.\n",
        "The `VariationalAutoencoder` class ties the encoder and decoder together:\n",
        "- **Forward Pass**: Encodes the input to get `mu` and `logvar`, samples from the latent space, and decodes the sample back into an image.\n",
        "- **Latent Sampling (`latent_sample`)**: Implements the reparameterization trick, allowing gradients to pass through the stochastic sampling process during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cj1uYMjn35h3"
      },
      "outputs": [],
      "source": [
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "\n",
        "        # TODO: Defin an Encoder and a Decoder instans\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent_mu, latent_logvar = self.encoder(x)\n",
        "        latent = self.latent_sample(latent_mu, latent_logvar)\n",
        "        x_recon = self.decoder(latent)\n",
        "        return x_recon, latent_mu, latent_logvar\n",
        "\n",
        "    def latent_sample(self, mu, logvar):\n",
        "        if self.training:\n",
        "            # the reparameterization trick\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            eps = torch.empty_like(std).normal_()\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLOQzMRv47pw"
      },
      "source": [
        "### **Loss Function (`vae_loss`)**\n",
        "This part is basicly the heart of VAE. A good loss will teach the model our target which is to predict mean and variance of distribution for an input image. It should also teach the decoder to produce the result for a sample.\n",
        "The VAE loss consists of two parts:\n",
        "- **Reconstruction Loss**: Binary cross-entropy between the input and reconstructed images, encouraging the decoder to produce outputs close to the original inputs.\n",
        "- **KL Divergence**: A regularization term that ensures the learned latent distribution is close to a standard normal distribution, aiding in generating new, diverse samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8Uh-mVK445W"
      },
      "outputs": [],
      "source": [
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "\n",
        "    recon_loss = F.binary_cross_entropy(recon_x.view(-1, 784), x.view(-1, 784), reduction='sum')\n",
        "    kldivergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    return recon_loss + variational_beta * kldivergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H_ZSx0JjbIp"
      },
      "source": [
        "Now as usual we can define the VAE and import it to GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEiJarCRjbIp"
      },
      "outputs": [],
      "source": [
        "vae = VariationalAutoencoder()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "vae = vae.to(device)\n",
        "\n",
        "num_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)\n",
        "print('Number of parameters: %d' % num_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGz92IvhjbIr"
      },
      "source": [
        "Train VAE\n",
        "--------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdKRJWkT6D0r"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=vae.parameters(), lr=learning_rate, weight_decay=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGPc-7C55zmn"
      },
      "source": [
        "Now we have to define the training loop. Use what you have learned up to this point to write the loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyGAVR6fjbIr",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# set to training mode\n",
        "vae.train()\n",
        "\n",
        "train_loss_avg = []\n",
        "\n",
        "print('Training ...')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss_avg.append(0)\n",
        "    num_batches = 0\n",
        "\n",
        "    for image_batch, _ in train_dataloader:\n",
        "\n",
        "        # TODO: Train the model for an image_batch\n",
        "\n",
        "        train_loss_avg[-1] += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    train_loss_avg[-1] /= num_batches\n",
        "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-S2tRM9jbIs"
      },
      "source": [
        "Plot Training Curve\n",
        "--------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4Qp_z_DjbIs"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(train_loss_avg)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYO6BqE4jbIs"
      },
      "source": [
        "Alternatively: Load Pre-Trained VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg5O2JYOjbIt"
      },
      "source": [
        "Evaluate on the Test Set\n",
        "-------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Je0hqc6VjbIt"
      },
      "outputs": [],
      "source": [
        "# set to evaluation mode\n",
        "vae.eval()\n",
        "\n",
        "test_loss_avg, num_batches = 0, 0\n",
        "for image_batch, _ in test_dataloader:\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        image_batch = image_batch.to(device)\n",
        "\n",
        "        # vae reconstruction\n",
        "        image_batch_recon, latent_mu, latent_logvar = vae(image_batch)\n",
        "\n",
        "        # reconstruction error\n",
        "        loss = vae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar)\n",
        "\n",
        "        test_loss_avg += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "test_loss_avg /= num_batches\n",
        "print('average reconstruction error: %f' % (test_loss_avg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRNWUfLTjbIt"
      },
      "source": [
        "Visualize Reconstructions\n",
        "--------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE5fvnB46LaV"
      },
      "source": [
        "Now let's test and see how similar is the output to the input image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J78cU7ixjbIt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "\n",
        "import torchvision.utils\n",
        "\n",
        "vae.eval()\n",
        "\n",
        "# This function takes as an input the images to reconstruct\n",
        "# and the name of the model with which the reconstructions\n",
        "# are performed\n",
        "def to_img(x):\n",
        "    x = x.clamp(0, 1)\n",
        "    return x\n",
        "\n",
        "def show_image(img):\n",
        "    img = to_img(img)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "def visualise_output(images, model):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        images = images.to(device)\n",
        "        images, _, _ = model(images)\n",
        "        images = images.cpu()\n",
        "        images = to_img(images)\n",
        "        np_imagegrid = torchvision.utils.make_grid(images[1:50], 10, 5).numpy()\n",
        "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "images, labels = next(iter(test_dataloader))\n",
        "\n",
        "# First visualise the original images\n",
        "print('Original images')\n",
        "show_image(torchvision.utils.make_grid(images[1:50],10,5))\n",
        "plt.show()\n",
        "\n",
        "# Reconstruct and visualise the images using the vae\n",
        "print('VAE reconstruction:')\n",
        "visualise_output(images, vae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW_7rdkAjbIt"
      },
      "source": [
        "Interpolate in Latent Space\n",
        "----------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1lXd9bz61R4"
      },
      "source": [
        "Since the Decoder only needs a sample to create an image, it is fairly easy to produce interpolation of two image.\n",
        "1. First we will provide each of the images to the Encoder.\n",
        "2. After getting the `mu` for both of them we can create a sample from each image distribution without using the variances (`logvar`). Why is that?\n",
        "3. We will use `lambda1` to get a point between the two samples using weighted average."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hlx0J4kU60z0"
      },
      "outputs": [],
      "source": [
        "def interpolation(lambda1, model, img1, img2):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # TODO get both mu values for each image and use the mu itself as a sample of latent space\n",
        "\n",
        "        # interpolation of the two latent vectors\n",
        "        # TODO: write a weighted average based on lambda in (0,1) for our two latents. Name the output \"inter_latent\"\n",
        "\n",
        "        # reconstruct interpolated image\n",
        "        inter_image = model.decoder(inter_latent)\n",
        "        inter_image = inter_image.cpu()\n",
        "\n",
        "        return inter_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh0FIdgO78Se"
      },
      "source": [
        "Having the `interpolation` function, Now we can perform our interpolation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMZjsx4xjbIu"
      },
      "outputs": [],
      "source": [
        "vae.eval()\n",
        "\n",
        "# sort part of test set by digit\n",
        "digits = [[] for _ in range(10)]\n",
        "for img_batch, label_batch in test_dataloader:\n",
        "    for i in range(img_batch.size(0)):\n",
        "        digits[label_batch[i]].append(img_batch[i:i+1])\n",
        "    if sum(len(d) for d in digits) >= 1000:\n",
        "        break;\n",
        "\n",
        "# interpolation lambdas\n",
        "lambda_range=np.linspace(0,1,10)\n",
        "\n",
        "fig, axs = plt.subplots(2,5, figsize=(15, 6))\n",
        "fig.subplots_adjust(hspace = .5, wspace=.001)\n",
        "axs = axs.ravel()\n",
        "\n",
        "for ind,l in enumerate(lambda_range):\n",
        "    inter_image=interpolation(float(l), vae, digits[7][0], digits[1][0])\n",
        "\n",
        "    inter_image = to_img(inter_image)\n",
        "\n",
        "    image = inter_image.numpy()\n",
        "\n",
        "    axs[ind].imshow(image[0,0,:,:], cmap='gray')\n",
        "    axs[ind].set_title('lambda_val='+str(round(l,1)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8m1MgmSjbIv"
      },
      "source": [
        "Show 2D Latent Space\n",
        "---------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAdK6wHj8J4V"
      },
      "source": [
        "Now it is time a for a really cool experiment to see how the Decoder will react to sample from any point in the latent space (The space the sampled points come from an then are feeded to the Encoder and input of Decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jAIJU5JjbIv"
      },
      "outputs": [],
      "source": [
        "# load a network that was trained with a 2d latent space\n",
        "if latent_dims != 2:\n",
        "    print('Please change the parameters to two latent dimensions.')\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # create a sample grid in 2d latent space\n",
        "    latent_x = np.linspace(-1.5,1.5,20)\n",
        "    latent_y = np.linspace(-1.5,1.5,20)\n",
        "    latents = torch.FloatTensor(len(latent_y), len(latent_x), 2)\n",
        "    for i, lx in enumerate(latent_x):\n",
        "        for j, ly in enumerate(latent_y):\n",
        "            latents[j, i, 0] = lx\n",
        "            latents[j, i, 1] = ly\n",
        "    latents = latents.view(-1, 2) # flatten grid into a batch\n",
        "\n",
        "    # reconstruct images from the latent vectors\n",
        "    latents = latents.to(device)\n",
        "    image_recon = vae.decoder(latents)\n",
        "    image_recon = image_recon.cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    show_image(torchvision.utils.make_grid(image_recon.data[:400],20,5))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrgLKuhN82J-"
      },
      "source": [
        "The results are really good. But there is only one problem.\n",
        "\n",
        "**Why are most of the outputs blurry?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5B8_6g9U2lu"
      },
      "source": [
        "# Stable Diffusion\n",
        "\n",
        "Stable Diffusion is a technique used for generating high-quality images from text descriptions. It works by gradually adding noise to an image and then reversing the process to refine the image, guided by the text prompt. The model starts with random noise and iteratively denoises it, aligning the output with the input text. This approach allows for creating detailed and coherent images, making it popular for various creative applications.\n",
        "\n",
        "You can read more about diffusion models <a href=\"https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb\">here</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pPA2_ay8PEU"
      },
      "source": [
        "## Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4r8h9BTnr2wI"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mzVTn9VsA68"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers==4.40.0 diffusers==0.27.2 ftfy==6.2.0 accelerate datasets bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwSV-J2Qybtv"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "import math\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch import autocast\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler, LMSDiscreteScheduler, DDPMScheduler\n",
        "from accelerate import Accelerator\n",
        "from accelerate.utils import set_seed\n",
        "import bitsandbytes as bnb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1YnTBycKy2G"
      },
      "outputs": [],
      "source": [
        "height = 512                        # default height of Stable Diffusion\n",
        "width = 512                         # default width of Stable Diffusion\n",
        "num_inference_steps = 50           # Number of denoising steps\n",
        "guidance_scale = 8                # Scale for classifier-free guidance\n",
        "generator = torch.manual_seed(64)   # Seed generator to create the inital latent noise\n",
        "batch_size = 1\n",
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch_device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtf0GZmmBzHR"
      },
      "source": [
        "## Importing Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAsP3LXYsK3u"
      },
      "source": [
        "Stable Diffusion utilizes multiple models to operate. As it is demonstrated in the image below the process consists of multiple steps:\n",
        "- The process starts with a tensor of random numbers, chosen as the initial embedding of the image.\n",
        "- Simultaneously, a text prompt provided by the user is tokenized and converted into an embedding.\n",
        "- Both embeddings are combined and fed into a U-Net model.\n",
        "- The U-Net model goes through multiple iterations, gradually reducing the noise in the image.\n",
        "- The scheduler defines the algorithm for noise reduction.\n",
        "- Finally, the refined embedding is passed through the decoder of a VAE (Variational Autoencoder) to produce the output image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ0-AEvzajHr"
      },
      "source": [
        "<p align=\"left\">\n",
        "<img src=\"https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png\" alt=\"sd-pipeline\" width=\"500\"/>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg3BBZCsbRoS"
      },
      "source": [
        "To start, first we have to import all the required models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlrD3RK7zLj1"
      },
      "outputs": [],
      "source": [
        "# Load the autoencoder model which will be used to decode the latents into image space.\n",
        "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "# The UNet model for generating the latents\n",
        "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n",
        "scheduler = PNDMScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n",
        "vae = vae.to(torch_device)\n",
        "text_encoder = text_encoder.to(torch_device)\n",
        "unet = unet.to(torch_device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjbXwUrZbeuB"
      },
      "source": [
        "We will define a function to convert the latent outputs to images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRHjivXoG00w"
      },
      "outputs": [],
      "source": [
        "def latents_to_pil(latents):\n",
        "\n",
        "    # bath of latents -> list of images\n",
        "    latents = (1 / 0.18215) * latents\n",
        "    with torch.no_grad():\n",
        "        image = vae.decode(latents).sample\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "    images = (image * 255).round().astype(\"uint8\")\n",
        "    pil_images = [Image.fromarray(image) for image in images]\n",
        "    return pil_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMustQaPyWUY"
      },
      "source": [
        "## Prompt to Embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shRJQE5kJKkK"
      },
      "source": [
        "To provide the U-net model with text guidance, we need to convert text inputs to embeddings.\n",
        "\n",
        "It's important to note that the U-net model requires a positive embedding and also a negative embedding. The code for positive embedding is provided to you. As part of the homework try writing the code for getting the negative embedding. Since we are not going to provide any negative prompt, You can use empty strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai-TdfJG1Rp3"
      },
      "outputs": [],
      "source": [
        "def prompt_to_emb(prompt):\n",
        "    # Tokenize the prompts\n",
        "    batch_size = len(prompt)\n",
        "\n",
        "    # Convert the text prompts into tokenized inputs with fixed max length\n",
        "    text_inputs = tokenizer(\n",
        "        prompt,\n",
        "        padding=\"max_length\",\n",
        "        max_length=77,  # Maximum token length\n",
        "        truncation=True,  # Truncate if the prompt exceeds max_length\n",
        "        return_tensors=\"pt\",  # Return as PyTorch tensors\n",
        "    )\n",
        "    text_input_ids = text_inputs.input_ids\n",
        "\n",
        "    # Encode the tokenized inputs into embeddings using the text encoder\n",
        "    prompt_embeds = text_encoder(text_input_ids.cuda())\n",
        "    prompt_embeds = prompt_embeds[0]  # Extract the embeddings\n",
        "\n",
        "    # Get the data type of the embeddings\n",
        "    prompt_embeds_dtype = text_encoder.dtype\n",
        "\n",
        "    # Ensure embeddings are in the correct dtype and on the correct device\n",
        "    prompt_embeds = prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)\n",
        "\n",
        "    # Get the sequence length from the embeddings\n",
        "    _, seq_len, _ = prompt_embeds.shape\n",
        "\n",
        "    # Repeat and reshape the embeddings for further processing\n",
        "    prompt_embeds = prompt_embeds.repeat(1, 1, 1)\n",
        "    prompt_embeds = prompt_embeds.view(batch_size * 1, seq_len, -1)\n",
        "    max_length = prompt_embeds.shape[1]\n",
        "    ##############TODO################\n",
        "    # TODO: Write the code for negetive prompt embedding by analysing the first half of the function\n",
        "    # where the embedding of the of positive prompt was produced.\n",
        "    ##################################\n",
        "\n",
        "    # Finally Concatenate the unconditional and prompt embeddings\n",
        "    concatenated_embeddings = torch.cat([negative_prompt_embeds, prompt_embeds])\n",
        "\n",
        "    # Return the concatenated embeddings\n",
        "    return concatenated_embeddings\n",
        "prompt_embeddings = prompt_to_emb([\"A photo of a cat\"])\n",
        "print(prompt_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCihR2ADgdSb"
      },
      "source": [
        "## Embeding to Latents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKVZzyvdgoz5"
      },
      "source": [
        "This is the part where the input Embeddings go through multiple iterations to produce the image embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkziZGc3BzHU"
      },
      "outputs": [],
      "source": [
        "def emb_to_latents(text_embeddings):\n",
        "\n",
        "    scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "    # Generate random latent noise for the input to the model\n",
        "    latents = torch.randn((1, 4, 64, 64),dtype=torch.float32).to(torch_device)\n",
        "\n",
        "    for t in tqdm(scheduler.timesteps):\n",
        "        # Duplicate the random latent noise\n",
        "        latent_model_input = torch.cat([latents] * 2)\n",
        "\n",
        "        # Scale the model input\n",
        "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
        "        with torch.no_grad():\n",
        "            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings,return_dict=False,added_cond_kwargs={'text_embeds':text_embeddings})[0]\n",
        "\n",
        "        # Apply classifier-free guidance scale\n",
        "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "        latents = scheduler.step(noise_pred, t, latents,return_dict=False)[0]\n",
        "\n",
        "    return latents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYzH1wxgg_yM"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeigybPPhCuK"
      },
      "source": [
        "Let's try the pipeline with a few examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cto3Nhagb3BD"
      },
      "outputs": [],
      "source": [
        "prompt = [\"a photograph of an astronaut riding a horse\"]\n",
        "text_embeddings = prompt_to_emb(prompt)\n",
        "latents = emb_to_latents(text_embeddings)\n",
        "image = latents_to_pil(latents)\n",
        "image[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBsevhsF9epI"
      },
      "outputs": [],
      "source": [
        "prompt = [\"a campfire (oil on canvas)\"]\n",
        "text_embeddings = prompt_to_emb(prompt)\n",
        "latents = emb_to_latents(text_embeddings)\n",
        "image = latents_to_pil(latents)\n",
        "image[0]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}