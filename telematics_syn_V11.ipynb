{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOYsbArkQqA734yXoNLKgMq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/DataScienceJourney/blob/master/telematics_syn_V11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlKvn7dGihG5",
        "outputId": "36e9212e-b34b-4e4e-a2ff-c1a66b7c068b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Step 1: Load the dataset\n",
        "dataset = pd.read_csv('/content/drive/My Drive/telematics_syn.csv')"
      ],
      "metadata": {
        "id": "WjZiBMQ_iooq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "4HYlQKONmiuw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Create the adjusted ClaimYN label\n",
        "dataset['ClaimYN'] = ((dataset['NB_Claim'] >= 1) & (dataset['AMT_Claim'] > 1000)).astype(int)\n",
        "\n",
        "# Preprocess the dataset\n",
        "# 1. Handle missing values\n",
        "dataset.fillna(method='ffill', inplace=True)  # Simple forward fill for missing values\n",
        "\n",
        "# 2. Encode categorical variables (if any exist)\n",
        "categorical_columns = dataset.select_dtypes(include=['object']).columns\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    dataset[col] = le.fit_transform(dataset[col])\n",
        "\n",
        "# 3. Standardize numerical columns\n",
        "numerical_columns = dataset.drop(columns=['ClaimYN', 'NB_Claim', 'AMT_Claim']).columns\n",
        "scaler = StandardScaler()\n",
        "dataset[numerical_columns] = scaler.fit_transform(dataset[numerical_columns])\n",
        "\n",
        "# Handle the imbalance in the dataset using SMOTE\n",
        "X = dataset.drop(columns=['ClaimYN'])\n",
        "y = dataset['ClaimYN']\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Merging resampled data back into a single DataFrame\n",
        "dataset_resampled = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "\n",
        "# Drop NB_Claim and AMT_Claim columns\n",
        "dataset_resampled = dataset_resampled.drop(columns=['NB_Claim', 'AMT_Claim'])\n",
        "\n",
        "# Split the dataset into train, test, and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = dataset_resampled.drop(columns=['ClaimYN'])\n",
        "y = dataset_resampled['ClaimYN']\n",
        "\n",
        "# Split the data (70% train, 15% test, 15% validation)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Testing set size: {len(X_test)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d87ORJ_liX6x",
        "outputId": "70f3a70e-c667-49ec-a1a0-f496d1c9dc01"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 136222\n",
            "Testing set size: 29191\n",
            "Validation set size: 29191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=200)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "print(\"Logistic Regression Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVjhONWfjrkw",
        "outputId": "4d2b731d-e8e9-4166-870a-740a816ddf89"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.72      0.75     14593\n",
            "           1       0.74      0.80      0.77     14598\n",
            "\n",
            "    accuracy                           0.76     29191\n",
            "   macro avg       0.76      0.76      0.76     29191\n",
            "weighted avg       0.76      0.76      0.76     29191\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize and train the Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = rf.predict(X_test)\n",
        "print(\"Random Forest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6TpqdIflVwd",
        "outputId": "5c587947-d4de-46d6-fe65-17e0408d4cdd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99     14593\n",
            "           1       1.00      0.98      0.99     14598\n",
            "\n",
            "    accuracy                           0.99     29191\n",
            "   macro avg       0.99      0.99      0.99     29191\n",
            "weighted avg       0.99      0.99      0.99     29191\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Initialize and train the XGBoost model\n",
        "xgb = XGBClassifier(random_state=42)\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = xgb.predict(X_test)\n",
        "print(\"XGBoost Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YIHzFpone0q",
        "outputId": "0dae7cd2-a71d-4b4d-b7f7-4606d9ec32ab"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99     14593\n",
            "           1       1.00      0.98      0.99     14598\n",
            "\n",
            "    accuracy                           0.99     29191\n",
            "   macro avg       0.99      0.99      0.99     29191\n",
            "weighted avg       0.99      0.99      0.99     29191\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_tabnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAHHElrJoA58",
        "outputId": "1d3741a5-81c5-4b4a-8ebc-969be7f592f3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_tabnet\n",
            "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.26.4)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.3.2)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (2.3.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (4.66.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch_tabnet) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->pytorch_tabnet) (1.3.0)\n",
            "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch_tabnet\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 pytorch_tabnet-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Convert data to numpy arrays for TabNet\n",
        "X_train_np, y_train_np = X_train.values, y_train.values\n",
        "X_test_np, y_test_np = X_test.values, y_test.values\n",
        "\n",
        "# Initialize and train the TabNet model\n",
        "tabnet = TabNetClassifier(seed=42)\n",
        "tabnet.fit(X_train_np, y_train_np, eval_set=[(X_test_np, y_test_np)], patience=10)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = np.argmax(tabnet.predict_proba(X_test_np), axis=1)\n",
        "print(\"TabNet Classification Report:\")\n",
        "print(classification_report(y_test_np, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6leXUTFoDoA",
        "outputId": "2880a17a-dad6-4752-b7ae-425e8a7562c5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.52451 | val_0_auc: 0.87515 |  0:00:07s\n",
            "epoch 1  | loss: 0.43317 | val_0_auc: 0.89764 |  0:00:14s\n",
            "epoch 2  | loss: 0.39164 | val_0_auc: 0.92828 |  0:00:20s\n",
            "epoch 3  | loss: 0.34    | val_0_auc: 0.94508 |  0:00:26s\n",
            "epoch 4  | loss: 0.31046 | val_0_auc: 0.95073 |  0:00:32s\n",
            "epoch 5  | loss: 0.28787 | val_0_auc: 0.96103 |  0:00:38s\n",
            "epoch 6  | loss: 0.27042 | val_0_auc: 0.962   |  0:00:45s\n",
            "epoch 7  | loss: 0.26094 | val_0_auc: 0.96252 |  0:00:50s\n",
            "epoch 8  | loss: 0.25302 | val_0_auc: 0.96613 |  0:00:57s\n",
            "epoch 9  | loss: 0.24544 | val_0_auc: 0.96825 |  0:01:03s\n",
            "epoch 10 | loss: 0.23994 | val_0_auc: 0.97131 |  0:01:10s\n",
            "epoch 11 | loss: 0.23557 | val_0_auc: 0.96242 |  0:01:15s\n",
            "epoch 12 | loss: 0.22619 | val_0_auc: 0.96915 |  0:01:22s\n",
            "epoch 13 | loss: 0.22559 | val_0_auc: 0.96316 |  0:01:28s\n",
            "epoch 14 | loss: 0.21921 | val_0_auc: 0.97498 |  0:01:35s\n",
            "epoch 15 | loss: 0.21526 | val_0_auc: 0.9745  |  0:01:40s\n",
            "epoch 16 | loss: 0.21137 | val_0_auc: 0.97374 |  0:01:47s\n",
            "epoch 17 | loss: 0.21189 | val_0_auc: 0.97562 |  0:01:53s\n",
            "epoch 18 | loss: 0.21145 | val_0_auc: 0.97257 |  0:01:59s\n",
            "epoch 19 | loss: 0.20618 | val_0_auc: 0.96458 |  0:02:05s\n",
            "epoch 20 | loss: 0.20369 | val_0_auc: 0.97169 |  0:02:11s\n",
            "epoch 21 | loss: 0.20056 | val_0_auc: 0.97665 |  0:02:18s\n",
            "epoch 22 | loss: 0.20625 | val_0_auc: 0.96594 |  0:02:23s\n",
            "epoch 23 | loss: 0.19931 | val_0_auc: 0.96531 |  0:02:30s\n",
            "epoch 24 | loss: 0.20434 | val_0_auc: 0.96378 |  0:02:36s\n",
            "epoch 25 | loss: 0.21298 | val_0_auc: 0.96297 |  0:02:44s\n",
            "epoch 26 | loss: 0.21783 | val_0_auc: 0.96301 |  0:02:59s\n",
            "epoch 27 | loss: 0.21867 | val_0_auc: 0.97427 |  0:03:04s\n",
            "epoch 28 | loss: 0.20036 | val_0_auc: 0.9572  |  0:03:11s\n",
            "epoch 29 | loss: 0.19617 | val_0_auc: 0.97778 |  0:03:17s\n",
            "epoch 30 | loss: 0.20589 | val_0_auc: 0.97139 |  0:03:23s\n",
            "epoch 31 | loss: 0.19946 | val_0_auc: 0.93818 |  0:03:29s\n",
            "epoch 32 | loss: 0.19635 | val_0_auc: 0.96305 |  0:03:35s\n",
            "epoch 33 | loss: 0.18932 | val_0_auc: 0.97071 |  0:03:42s\n",
            "epoch 34 | loss: 0.20191 | val_0_auc: 0.92974 |  0:03:47s\n",
            "epoch 35 | loss: 0.18758 | val_0_auc: 0.94984 |  0:03:54s\n",
            "epoch 36 | loss: 0.18522 | val_0_auc: 0.95398 |  0:04:00s\n",
            "epoch 37 | loss: 0.18305 | val_0_auc: 0.9475  |  0:04:07s\n",
            "epoch 38 | loss: 0.19342 | val_0_auc: 0.96845 |  0:04:13s\n",
            "epoch 39 | loss: 0.1973  | val_0_auc: 0.955   |  0:04:19s\n",
            "\n",
            "Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_0_auc = 0.97778\n",
            "TabNet Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.80      0.87     14593\n",
            "           1       0.83      0.97      0.89     14598\n",
            "\n",
            "    accuracy                           0.88     29191\n",
            "   macro avg       0.90      0.88      0.88     29191\n",
            "weighted avg       0.90      0.88      0.88     29191\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN\n",
        "\n",
        "# Reshape data for RNN\n",
        "X_train_rnn = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test_rnn = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Initialize and train the RNN model\n",
        "rnn_model = Sequential()\n",
        "rnn_model.add(SimpleRNN(32, input_shape=(X_train_rnn.shape[1], 1), activation='relu'))\n",
        "rnn_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "rnn_model.fit(X_train_rnn, y_train, epochs=10, batch_size=32, validation_data=(X_test_rnn, y_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = (rnn_model.predict(X_test_rnn) > 0.5).astype(\"int32\")\n",
        "print(\"RNN Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU2SkBz0sr3N",
        "outputId": "b82fa315-e250-46f1-bdb5-d5408d04b75c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 8ms/step - accuracy: 0.7323 - loss: 0.5387 - val_accuracy: 0.8166 - val_loss: 0.4124\n",
            "Epoch 2/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 8ms/step - accuracy: 0.8428 - loss: 0.3618 - val_accuracy: 0.8580 - val_loss: 0.3183\n",
            "Epoch 3/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 9ms/step - accuracy: 0.8597 - loss: 0.3208 - val_accuracy: 0.8251 - val_loss: 0.3738\n",
            "Epoch 4/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 8ms/step - accuracy: 0.8703 - loss: 0.2980 - val_accuracy: 0.8669 - val_loss: 0.3065\n",
            "Epoch 5/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 8ms/step - accuracy: 0.8774 - loss: 0.2814 - val_accuracy: 0.8688 - val_loss: 0.3067\n",
            "Epoch 6/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 8ms/step - accuracy: 0.8824 - loss: 0.2697 - val_accuracy: 0.8578 - val_loss: 0.3130\n",
            "Epoch 7/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 8ms/step - accuracy: 0.8854 - loss: 0.2621 - val_accuracy: 0.8863 - val_loss: 0.2514\n",
            "Epoch 8/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 8ms/step - accuracy: 0.8840 - loss: 0.2611 - val_accuracy: 0.8819 - val_loss: 0.2754\n",
            "Epoch 9/10\n",
            "\u001b[1m1344/4257\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 8ms/step - accuracy: 0.8935 - loss: 0.2503"
          ]
        }
      ]
    }
  ]
}