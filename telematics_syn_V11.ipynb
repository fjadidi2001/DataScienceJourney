{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM+qVi/NwMyThlSpV/4l6q5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/DataScienceJourney/blob/master/telematics_syn_V11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlKvn7dGihG5",
        "outputId": "3f27c6c0-a028-49eb-bdf9-c574fe2d8316"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Step 1: Load the dataset\n",
        "dataset = pd.read_csv('/content/drive/My Drive/telematics_syn.csv')"
      ],
      "metadata": {
        "id": "WjZiBMQ_iooq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "4HYlQKONmiuw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Create the adjusted ClaimYN label\n",
        "dataset['ClaimYN'] = ((dataset['NB_Claim'] >= 1) & (dataset['AMT_Claim'] > 1000)).astype(int)\n",
        "\n",
        "# Preprocess the dataset\n",
        "# 1. Handle missing values\n",
        "dataset.fillna(method='ffill', inplace=True)  # Simple forward fill for missing values\n",
        "\n",
        "# 2. Encode categorical variables (if any exist)\n",
        "categorical_columns = dataset.select_dtypes(include=['object']).columns\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    dataset[col] = le.fit_transform(dataset[col])\n",
        "\n",
        "# 3. Standardize numerical columns\n",
        "numerical_columns = dataset.drop(columns=['ClaimYN', 'NB_Claim', 'AMT_Claim']).columns\n",
        "scaler = StandardScaler()\n",
        "dataset[numerical_columns] = scaler.fit_transform(dataset[numerical_columns])\n",
        "\n",
        "# Handle the imbalance in the dataset using SMOTE\n",
        "X = dataset.drop(columns=['ClaimYN'])\n",
        "y = dataset['ClaimYN']\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Merging resampled data back into a single DataFrame\n",
        "dataset_resampled = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "\n",
        "# Drop NB_Claim and AMT_Claim columns\n",
        "dataset_resampled = dataset_resampled.drop(columns=['NB_Claim', 'AMT_Claim'])\n",
        "\n",
        "# Split the dataset into train, test, and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = dataset_resampled.drop(columns=['ClaimYN'])\n",
        "y = dataset_resampled['ClaimYN']\n",
        "\n",
        "# Split the data (70% train, 15% test, 15% validation)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Testing set size: {len(X_test)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d87ORJ_liX6x",
        "outputId": "4dbe439e-9d42-4d66-cac6-0f86772862d5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 136222\n",
            "Testing set size: 29191\n",
            "Validation set size: 29191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=200)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "print(\"Logistic Regression Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVjhONWfjrkw",
        "outputId": "c19ee5e6-4763-4431-e99d-3af5c51e3a0d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.72      0.75     14593\n",
            "           1       0.74      0.80      0.77     14598\n",
            "\n",
            "    accuracy                           0.76     29191\n",
            "   macro avg       0.76      0.76      0.76     29191\n",
            "weighted avg       0.76      0.76      0.76     29191\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize and train the Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = rf.predict(X_test)\n",
        "print(\"Random Forest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6TpqdIflVwd",
        "outputId": "d980d98b-9ba8-4299-cb2c-3ea0b309f82e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99     14593\n",
            "           1       1.00      0.98      0.99     14598\n",
            "\n",
            "    accuracy                           0.99     29191\n",
            "   macro avg       0.99      0.99      0.99     29191\n",
            "weighted avg       0.99      0.99      0.99     29191\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Initialize and train the XGBoost model\n",
        "xgb = XGBClassifier(random_state=42)\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = xgb.predict(X_test)\n",
        "print(\"XGBoost Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YIHzFpone0q",
        "outputId": "4085ad57-b77d-4d89-a9fa-69253ad2c3e3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99     14593\n",
            "           1       1.00      0.98      0.99     14598\n",
            "\n",
            "    accuracy                           0.99     29191\n",
            "   macro avg       0.99      0.99      0.99     29191\n",
            "weighted avg       0.99      0.99      0.99     29191\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_tabnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAHHElrJoA58",
        "outputId": "1f874b9d-5355-48d9-dcdb-6897edc4d73e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_tabnet\n",
            "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.26.4)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.3.2)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (2.3.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (4.66.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3->pytorch_tabnet)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch_tabnet) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->pytorch_tabnet) (1.3.0)\n",
            "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch_tabnet\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 pytorch_tabnet-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Convert data to numpy arrays for TabNet\n",
        "X_train_np, y_train_np = X_train.values, y_train.values\n",
        "X_test_np, y_test_np = X_test.values, y_test.values\n",
        "\n",
        "# Initialize and train the TabNet model\n",
        "tabnet = TabNetClassifier(seed=42)\n",
        "tabnet.fit(X_train_np, y_train_np, eval_set=[(X_test_np, y_test_np)], patience=10)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = np.argmax(tabnet.predict_proba(X_test_np), axis=1)\n",
        "print(\"TabNet Classification Report:\")\n",
        "print(classification_report(y_test_np, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6leXUTFoDoA",
        "outputId": "3bf40533-308c-4e42-d557-e557e1325dba"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.52504 | val_0_auc: 0.88173 |  0:00:26s\n",
            "epoch 1  | loss: 0.40063 | val_0_auc: 0.91582 |  0:00:44s\n",
            "epoch 2  | loss: 0.36553 | val_0_auc: 0.92501 |  0:00:58s\n",
            "epoch 3  | loss: 0.33912 | val_0_auc: 0.84918 |  0:01:07s\n",
            "epoch 4  | loss: 0.32147 | val_0_auc: 0.93409 |  0:01:17s\n",
            "epoch 5  | loss: 0.30906 | val_0_auc: 0.87753 |  0:01:26s\n",
            "epoch 6  | loss: 0.29658 | val_0_auc: 0.82212 |  0:01:36s\n",
            "epoch 7  | loss: 0.28286 | val_0_auc: 0.85643 |  0:01:46s\n",
            "epoch 8  | loss: 0.28663 | val_0_auc: 0.9294  |  0:01:57s\n",
            "epoch 9  | loss: 0.28628 | val_0_auc: 0.92491 |  0:02:06s\n",
            "epoch 10 | loss: 0.27638 | val_0_auc: 0.93338 |  0:02:17s\n",
            "epoch 11 | loss: 0.26571 | val_0_auc: 0.95694 |  0:02:28s\n",
            "epoch 12 | loss: 0.25292 | val_0_auc: 0.87831 |  0:02:36s\n",
            "epoch 13 | loss: 0.2587  | val_0_auc: 0.95842 |  0:02:47s\n",
            "epoch 14 | loss: 0.24295 | val_0_auc: 0.96465 |  0:02:56s\n",
            "epoch 15 | loss: 0.2294  | val_0_auc: 0.97015 |  0:03:05s\n",
            "epoch 16 | loss: 0.22512 | val_0_auc: 0.91345 |  0:03:16s\n",
            "epoch 17 | loss: 0.22399 | val_0_auc: 0.90909 |  0:03:25s\n",
            "epoch 18 | loss: 0.22023 | val_0_auc: 0.96004 |  0:03:35s\n",
            "epoch 19 | loss: 0.22448 | val_0_auc: 0.89027 |  0:03:45s\n",
            "epoch 20 | loss: 0.21761 | val_0_auc: 0.90456 |  0:03:54s\n",
            "epoch 21 | loss: 0.21288 | val_0_auc: 0.93026 |  0:04:04s\n",
            "epoch 22 | loss: 0.20567 | val_0_auc: 0.93939 |  0:04:13s\n",
            "epoch 23 | loss: 0.20692 | val_0_auc: 0.94654 |  0:04:22s\n",
            "epoch 24 | loss: 0.21801 | val_0_auc: 0.97154 |  0:04:32s\n",
            "epoch 25 | loss: 0.2093  | val_0_auc: 0.95141 |  0:04:42s\n",
            "epoch 26 | loss: 0.20255 | val_0_auc: 0.97528 |  0:04:52s\n",
            "epoch 27 | loss: 0.19143 | val_0_auc: 0.958   |  0:05:02s\n",
            "epoch 28 | loss: 0.18806 | val_0_auc: 0.96425 |  0:05:11s\n",
            "epoch 29 | loss: 0.18392 | val_0_auc: 0.87724 |  0:05:21s\n",
            "epoch 30 | loss: 0.18544 | val_0_auc: 0.94733 |  0:05:31s\n",
            "epoch 31 | loss: 0.18438 | val_0_auc: 0.92586 |  0:05:40s\n",
            "epoch 32 | loss: 0.18295 | val_0_auc: 0.96226 |  0:05:51s\n",
            "epoch 33 | loss: 0.18742 | val_0_auc: 0.9528  |  0:06:02s\n",
            "epoch 34 | loss: 0.18931 | val_0_auc: 0.95816 |  0:06:11s\n",
            "epoch 35 | loss: 0.24861 | val_0_auc: 0.91273 |  0:06:22s\n",
            "epoch 36 | loss: 0.24947 | val_0_auc: 0.93342 |  0:06:33s\n",
            "\n",
            "Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_0_auc = 0.97528\n",
            "TabNet Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.92      0.91     14593\n",
            "           1       0.92      0.91      0.91     14598\n",
            "\n",
            "    accuracy                           0.91     29191\n",
            "   macro avg       0.91      0.91      0.91     29191\n",
            "weighted avg       0.91      0.91      0.91     29191\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Reshape data for RNN (3D input: samples, timesteps, features)\n",
        "X_train_rnn = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_val_rnn = X_val.values.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
        "X_test_rnn = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Initialize the RNN model\n",
        "rnn_model = Sequential()\n",
        "rnn_model.add(SimpleRNN(32, input_shape=(X_train_rnn.shape[1], 1), activation='relu'))\n",
        "rnn_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "rnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "rnn_history = rnn_model.fit(X_train_rnn, y_train, epochs=10, batch_size=32,\n",
        "                            validation_data=(X_val_rnn, y_val), verbose=1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = rnn_model.evaluate(X_test_rnn, y_test)\n",
        "print(f\"RNN Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU2SkBz0sr3N",
        "outputId": "94935f16-1c1c-4bd3-e0a4-89ef9295855f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 12ms/step - accuracy: 0.6979 - loss: 0.5763 - val_accuracy: 0.7704 - val_loss: 0.4868\n",
            "Epoch 2/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 11ms/step - accuracy: 0.7944 - loss: 0.4473 - val_accuracy: 0.8012 - val_loss: 0.4516\n",
            "Epoch 3/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 11ms/step - accuracy: 0.8144 - loss: 0.4220 - val_accuracy: 0.8432 - val_loss: 0.3656\n",
            "Epoch 4/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 11ms/step - accuracy: 0.8323 - loss: 0.3848 - val_accuracy: 0.8571 - val_loss: 0.3268\n",
            "Epoch 5/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 11ms/step - accuracy: 0.8512 - loss: 0.3437 - val_accuracy: 0.8757 - val_loss: 0.2834\n",
            "Epoch 6/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 11ms/step - accuracy: 0.7880 - loss: 0.4017 - val_accuracy: 0.7543 - val_loss: 0.5500\n",
            "Epoch 7/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 11ms/step - accuracy: 0.8249 - loss: 0.3867 - val_accuracy: 0.8662 - val_loss: 0.2930\n",
            "Epoch 8/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 11ms/step - accuracy: 0.8624 - loss: 0.3014 - val_accuracy: 0.8673 - val_loss: 0.2982\n",
            "Epoch 9/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 11ms/step - accuracy: 0.8705 - loss: 0.2839 - val_accuracy: 0.8865 - val_loss: 0.2533\n",
            "Epoch 10/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 11ms/step - accuracy: 0.8711 - loss: 0.2845 - val_accuracy: 0.8763 - val_loss: 0.2635\n",
            "\u001b[1m913/913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8722 - loss: 0.2686\n",
            "RNN Test Accuracy: 0.8728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
        "\n",
        "# Initialize the CNN model\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_rnn.shape[1], 1)))\n",
        "cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "cnn_history = cnn_model.fit(X_train_rnn, y_train, epochs=10, batch_size=32,\n",
        "                            validation_data=(X_val_rnn, y_val), verbose=1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = cnn_model.evaluate(X_test_rnn, y_test)\n",
        "print(f\"CNN Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM9j_CCNwLMX",
        "outputId": "426e5497-adc0-4180-85c7-584686be5fa7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - accuracy: 0.8130 - loss: 0.4153 - val_accuracy: 0.8544 - val_loss: 0.3278\n",
            "Epoch 2/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.8602 - loss: 0.3178 - val_accuracy: 0.8651 - val_loss: 0.3048\n",
            "Epoch 3/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.8696 - loss: 0.2975 - val_accuracy: 0.8744 - val_loss: 0.2873\n",
            "Epoch 4/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.8768 - loss: 0.2827 - val_accuracy: 0.8785 - val_loss: 0.2738\n",
            "Epoch 5/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.8804 - loss: 0.2727 - val_accuracy: 0.8816 - val_loss: 0.2712\n",
            "Epoch 6/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.8849 - loss: 0.2638 - val_accuracy: 0.8876 - val_loss: 0.2649\n",
            "Epoch 7/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.8884 - loss: 0.2563 - val_accuracy: 0.8899 - val_loss: 0.2534\n",
            "Epoch 8/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 4ms/step - accuracy: 0.8917 - loss: 0.2498 - val_accuracy: 0.8917 - val_loss: 0.2523\n",
            "Epoch 9/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - accuracy: 0.8944 - loss: 0.2448 - val_accuracy: 0.8933 - val_loss: 0.2421\n",
            "Epoch 10/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - accuracy: 0.8956 - loss: 0.2411 - val_accuracy: 0.8975 - val_loss: 0.2352\n",
            "\u001b[1m913/913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9006 - loss: 0.2335\n",
            "CNN Test Accuracy: 0.8967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM\n",
        "\n",
        "# Initialize the LSTM model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(32, input_shape=(X_train_rnn.shape[1], 1), activation='relu'))\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "lstm_history = lstm_model.fit(X_train_rnn, y_train, epochs=10, batch_size=32,\n",
        "                              validation_data=(X_val_rnn, y_val), verbose=1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = lstm_model.evaluate(X_test_rnn, y_test)\n",
        "print(f\"LSTM Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btCePnE0xr8d",
        "outputId": "335ae2ea-2abf-4527-ac4e-6ae75ad9d3df"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 22ms/step - accuracy: 0.6749 - loss: 0.7676 - val_accuracy: 0.7551 - val_loss: 0.5074\n",
            "Epoch 2/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 21ms/step - accuracy: 0.7585 - loss: 0.5019 - val_accuracy: 0.7838 - val_loss: 0.4617\n",
            "Epoch 3/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 23ms/step - accuracy: 0.7644 - loss: 0.4958 - val_accuracy: 0.7904 - val_loss: 0.4571\n",
            "Epoch 4/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 21ms/step - accuracy: 0.7938 - loss: 0.4468 - val_accuracy: 0.8223 - val_loss: 0.3889\n",
            "Epoch 5/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 23ms/step - accuracy: 0.7871 - loss: 0.4680 - val_accuracy: 0.8207 - val_loss: 0.3889\n",
            "Epoch 6/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 22ms/step - accuracy: 0.8198 - loss: 0.3887 - val_accuracy: 0.8312 - val_loss: 0.3665\n",
            "Epoch 7/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 22ms/step - accuracy: 0.8237 - loss: 0.3798 - val_accuracy: 0.8453 - val_loss: 0.3371\n",
            "Epoch 8/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 22ms/step - accuracy: 0.8439 - loss: 0.3404 - val_accuracy: 0.8460 - val_loss: 0.3315\n",
            "Epoch 9/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 22ms/step - accuracy: 0.8401 - loss: 0.3517 - val_accuracy: 0.8067 - val_loss: 0.4353\n",
            "Epoch 10/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 21ms/step - accuracy: 0.7841 - loss: 0.4593 - val_accuracy: 0.7141 - val_loss: 0.5277\n",
            "\u001b[1m913/913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.7093 - loss: 0.5296\n",
            "LSTM Test Accuracy: 0.7101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import GRU\n",
        "\n",
        "# Initialize the GRU model\n",
        "gru_model = Sequential()\n",
        "gru_model.add(GRU(32, input_shape=(X_train_rnn.shape[1], 1), activation='relu'))\n",
        "gru_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "gru_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "gru_history = gru_model.fit(X_train_rnn, y_train, epochs=10, batch_size=32,\n",
        "                            validation_data=(X_val_rnn, y_val), verbose=1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = gru_model.evaluate(X_test_rnn, y_test)\n",
        "print(f\"GRU Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4bAZxp3CVM8",
        "outputId": "27bdcf16-c92e-4b6a-ca6d-a1dc29f2a9cf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 28ms/step - accuracy: 0.7182 - loss: 0.5350 - val_accuracy: 0.8042 - val_loss: 0.4264\n",
            "Epoch 2/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 28ms/step - accuracy: 0.8101 - loss: 0.4099 - val_accuracy: 0.8147 - val_loss: 0.3937\n",
            "Epoch 3/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 27ms/step - accuracy: 0.8390 - loss: 0.3489 - val_accuracy: 0.8725 - val_loss: 0.2816\n",
            "Epoch 4/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 28ms/step - accuracy: 0.8679 - loss: 0.2862 - val_accuracy: 0.8710 - val_loss: 0.2758\n",
            "Epoch 5/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 27ms/step - accuracy: 0.8840 - loss: 0.2579 - val_accuracy: 0.8850 - val_loss: 0.2583\n",
            "Epoch 6/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 28ms/step - accuracy: 0.8902 - loss: 0.2436 - val_accuracy: 0.8994 - val_loss: 0.2255\n",
            "Epoch 7/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 27ms/step - accuracy: 0.8960 - loss: 0.2320 - val_accuracy: 0.9020 - val_loss: 0.2204\n",
            "Epoch 8/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 28ms/step - accuracy: 0.8980 - loss: 0.2264 - val_accuracy: 0.9118 - val_loss: 0.2028\n",
            "Epoch 9/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 29ms/step - accuracy: 0.9056 - loss: 0.2144 - val_accuracy: 0.9125 - val_loss: 0.2029\n",
            "Epoch 10/10\n",
            "\u001b[1m4257/4257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 28ms/step - accuracy: 0.9073 - loss: 0.2069 - val_accuracy: 0.9082 - val_loss: 0.2060\n",
            "\u001b[1m913/913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9114 - loss: 0.2061\n",
            "GRU Test Accuracy: 0.9091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Necessary Libraries for Evaluation"
      ],
      "metadata": {
        "id": "nLHsydXCJia6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure you have seaborn for nicer plots\n",
        "sns.set(style=\"whitegrid\")"
      ],
      "metadata": {
        "id": "jyz0mK1PCV5d"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(y_true, y_pred, y_pred_proba, model_name):\n",
        "    \"\"\"Evaluate the model using MCC, AUC, and plot the confusion matrix.\"\"\"\n",
        "\n",
        "    # Calculate MCC\n",
        "    mcc = matthews_corrcoef(y_true, y_pred)\n",
        "\n",
        "    # Calculate AUC\n",
        "    auc = roc_auc_score(y_true, y_pred_proba)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"{model_name} Evaluation:\")\n",
        "    print(f\"  MCC: {mcc:.4f}\")\n",
        "    print(f\"  AUC: {auc:.4f}\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(f'{model_name} Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "def plot_training_history(history, model_name):\n",
        "    \"\"\"Plot training & validation accuracy and loss from the model's history.\"\"\"\n",
        "    # Accuracy\n",
        "    plt.figure(figsize=(14, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'{model_name} Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'{model_name} Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "x5p6WJI0Jro-"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}