{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjl2v8a7yGumussfdmh0Fb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/DataScienceJourney/blob/master/fake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TCOI_22nzs8V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c4774977-9cf6-4a00-f735-495f13ad2607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Fake News Detection System...\n",
            "Step 1: Loading Dataset...\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/clmentbisaillon/fake-and-real-news-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41.0M/41.0M [00:00<00:00, 121MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/clmentbisaillon/fake-and-real-news-dataset/versions/1\n",
            "Dataset loaded successfully with 44898 entries.\n",
            "True news: 21417 entries\n",
            "Fake news: 23481 entries\n",
            "Columns in the dataset: ['title', 'text', 'subject', 'date', 'label']\n",
            "Step 2: Cleaning Data...\n",
            "Null values before cleaning: 0\n",
            "Null values after cleaning: 0\n",
            "Dataset shape after cleaning: (44898, 3)\n",
            "Step 3: Preprocessing Text...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-bad50d2ebef0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;31m# Run the main pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;31m# Example news article for prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-bad50d2ebef0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;31m# Step 3: Preprocess text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;31m# Split data into training and testing sets (80:20 ratio)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-bad50d2ebef0>\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Apply text cleaning to the 'text' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;31m# If title column exists, process it too and combine with text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-bad50d2ebef0>\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# Remove stopwords and apply stemming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "# Fake News Detection System\n",
        "# A step-by-step implementation of the research methodology\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Step 1: Dataset Loading\n",
        "def load_dataset():\n",
        "    \"\"\"\n",
        "    Load the Kaggle fake and real news dataset using kagglehub.\n",
        "    \"\"\"\n",
        "    print(\"Step 1: Loading Dataset...\")\n",
        "\n",
        "    # Download the dataset\n",
        "    path = kagglehub.dataset_download(\"clmentbisaillon/fake-and-real-news-dataset\")\n",
        "    print(f\"Path to dataset files: {path}\")\n",
        "\n",
        "    # Load the data\n",
        "    # This dataset has separate files for true and fake news\n",
        "    true_news_path = os.path.join(path, \"True.csv\")\n",
        "    fake_news_path = os.path.join(path, \"Fake.csv\")\n",
        "\n",
        "    true_news = pd.read_csv(true_news_path)\n",
        "    fake_news = pd.read_csv(fake_news_path)\n",
        "\n",
        "    # Add labels to the datasets\n",
        "    true_news['label'] = 1  # 1 for true news\n",
        "    fake_news['label'] = 0  # 0 for fake news\n",
        "\n",
        "    # Combine the datasets\n",
        "    df = pd.concat([true_news, fake_news], ignore_index=True)\n",
        "\n",
        "    # Shuffle the dataset\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # Display dataset information\n",
        "    print(f\"Dataset loaded successfully with {len(df)} entries.\")\n",
        "    print(f\"True news: {len(true_news)} entries\")\n",
        "    print(f\"Fake news: {len(fake_news)} entries\")\n",
        "    print(f\"Columns in the dataset: {df.columns.tolist()}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Step 2: Data Cleaning\n",
        "def clean_data(df):\n",
        "    \"\"\"\n",
        "    Clean the dataset by removing null values and unnecessary columns.\n",
        "    \"\"\"\n",
        "    print(\"Step 2: Cleaning Data...\")\n",
        "\n",
        "    # Check for null values\n",
        "    print(f\"Null values before cleaning: {df.isnull().sum().sum()}\")\n",
        "\n",
        "    # Drop rows with null values\n",
        "    df = df.dropna()\n",
        "\n",
        "    # For this specific dataset, we'll focus on the text, title, and label columns\n",
        "    if 'title' in df.columns and 'text' in df.columns:\n",
        "        df = df[['title', 'text', 'label']]\n",
        "    elif 'title' in df.columns and 'text' not in df.columns:\n",
        "        # If 'text' is not present but 'content' is (common alternative name)\n",
        "        if 'content' in df.columns:\n",
        "            df = df.rename(columns={'content': 'text'})\n",
        "            df = df[['title', 'text', 'label']]\n",
        "\n",
        "    print(f\"Null values after cleaning: {df.isnull().sum().sum()}\")\n",
        "    print(f\"Dataset shape after cleaning: {df.shape}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Step 3: Text Processing (NLP)\n",
        "def preprocess_text(df):\n",
        "    \"\"\"\n",
        "    Perform text preprocessing using NLP techniques:\n",
        "    - Tokenization\n",
        "    - Stop words removal\n",
        "    - Punctuation removal\n",
        "    - Stemming\n",
        "    \"\"\"\n",
        "    print(\"Step 3: Preprocessing Text...\")\n",
        "\n",
        "    # Initialize stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Get English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def clean_text(text):\n",
        "        # Convert to lowercase\n",
        "        text = str(text).lower()\n",
        "\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "\n",
        "        # Remove HTML tags\n",
        "        text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "        # Remove punctuation and special characters\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "        # Remove numbers\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "\n",
        "        # Remove stopwords and apply stemming\n",
        "        cleaned_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
        "\n",
        "        return ' '.join(cleaned_tokens)\n",
        "\n",
        "    # Apply text cleaning to the 'text' column\n",
        "    df['processed_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "    # If title column exists, process it too and combine with text\n",
        "    if 'title' in df.columns:\n",
        "        df['processed_title'] = df['title'].apply(clean_text)\n",
        "        df['processed_content'] = df['processed_title'] + ' ' + df['processed_text']\n",
        "    else:\n",
        "        df['processed_content'] = df['processed_text']\n",
        "\n",
        "    print(\"Text preprocessing completed.\")\n",
        "    return df\n",
        "\n",
        "# Step 4: Feature Extraction (Vectorization)\n",
        "def vectorize_text(df_train, df_test, method='tfidf'):\n",
        "    \"\"\"\n",
        "    Convert text to numerical vectors using either TF-IDF or Bag of Words\n",
        "    \"\"\"\n",
        "    print(f\"Step 4: Vectorizing Text using {method}...\")\n",
        "\n",
        "    if method == 'tfidf':\n",
        "        # TF-IDF Vectorization\n",
        "        vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "    else:\n",
        "        # Bag of Words Vectorization\n",
        "        vectorizer = CountVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "\n",
        "    # Fit and transform the training data\n",
        "    X_train = vectorizer.fit_transform(df_train['processed_content'])\n",
        "\n",
        "    # Transform the test data\n",
        "    X_test = vectorizer.transform(df_test['processed_content'])\n",
        "\n",
        "    print(f\"Vectorization completed. Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
        "    return X_train, X_test, vectorizer\n",
        "\n",
        "# Step 5: Machine Learning Models\n",
        "def train_and_evaluate_models(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Train and evaluate multiple machine learning models:\n",
        "    - Random Forest\n",
        "    - Decision Tree\n",
        "    - Support Vector Machine\n",
        "    \"\"\"\n",
        "    print(\"Step 5: Training and Evaluating Machine Learning Models...\")\n",
        "\n",
        "    # Define models\n",
        "    models = {\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "        'SVM': SVC(kernel='linear', random_state=42, probability=True)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        # Generate classification report\n",
        "        report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "        # Generate confusion matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'accuracy': accuracy,\n",
        "            'report': report,\n",
        "            'confusion_matrix': cm\n",
        "        }\n",
        "\n",
        "        print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"{name} Classification Report:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return results\n",
        "\n",
        "# Step 6: Visualize Results\n",
        "def visualize_results(results):\n",
        "    \"\"\"\n",
        "    Visualize the performance of different models\n",
        "    \"\"\"\n",
        "    print(\"Step 6: Visualizing Results...\")\n",
        "\n",
        "    # Plot accuracy comparison\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    accuracies = [results[model]['accuracy'] for model in results.keys()]\n",
        "    plt.bar(results.keys(), accuracies, color=['blue', 'green', 'red'])\n",
        "    plt.title('Model Accuracy Comparison')\n",
        "    plt.xlabel('Models')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim(0, 1)\n",
        "    for i, v in enumerate(accuracies):\n",
        "        plt.text(i, v + 0.01, f\"{v:.4f}\", ha='center')\n",
        "    plt.savefig('model_accuracy_comparison.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot confusion matrices\n",
        "    for name, result in results.items():\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        cm = result['confusion_matrix']\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=['Fake', 'True'],\n",
        "                   yticklabels=['Fake', 'True'])\n",
        "        plt.title(f'Confusion Matrix - {name}')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.savefig(f'confusion_matrix_{name.replace(\" \", \"_\").lower()}.png')\n",
        "        plt.close()\n",
        "\n",
        "    # Plot precision, recall, and F1-score\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    model_names = list(results.keys())\n",
        "    precision = [results[model]['report']['1']['precision'] for model in model_names]\n",
        "    recall = [results[model]['report']['1']['recall'] for model in model_names]\n",
        "    f1 = [results[model]['report']['1']['f1-score'] for model in model_names]\n",
        "\n",
        "    x = np.arange(len(model_names))\n",
        "    width = 0.25\n",
        "\n",
        "    plt.bar(x - width, precision, width, label='Precision')\n",
        "    plt.bar(x, recall, width, label='Recall')\n",
        "    plt.bar(x + width, f1, width, label='F1-score')\n",
        "\n",
        "    plt.xlabel('Models')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Precision, Recall, and F1-score for True News Classification')\n",
        "    plt.xticks(x, model_names)\n",
        "    plt.legend()\n",
        "    plt.savefig('precision_recall_f1_comparison.png')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Visualization completed. Results saved as images.\")\n",
        "\n",
        "# Function to extract important features\n",
        "def extract_important_features(vectorizer, model, top_n=20):\n",
        "    \"\"\"\n",
        "    Extract the most important features for classification\n",
        "    \"\"\"\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        # For tree-based models\n",
        "        feature_importances = model.feature_importances_\n",
        "    elif hasattr(model, 'coef_'):\n",
        "        # For linear models like SVM\n",
        "        feature_importances = np.abs(model.coef_[0])\n",
        "    else:\n",
        "        print(\"Model does not support feature importance extraction\")\n",
        "        return None\n",
        "\n",
        "    # Get feature names\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Create a DataFrame of features and their importance\n",
        "    features_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': feature_importances\n",
        "    })\n",
        "\n",
        "    # Sort by importance\n",
        "    features_df = features_df.sort_values('importance', ascending=False)\n",
        "\n",
        "    # Get top N features\n",
        "    top_features = features_df.head(top_n)\n",
        "\n",
        "    return top_features\n",
        "\n",
        "# Main function to run the entire pipeline\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute the complete fake news detection pipeline\n",
        "    \"\"\"\n",
        "    print(\"Starting Fake News Detection System...\")\n",
        "\n",
        "    # Step 1: Load dataset\n",
        "    df = load_dataset()\n",
        "\n",
        "    # Step 2: Clean data\n",
        "    df = clean_data(df)\n",
        "\n",
        "    # Step 3: Preprocess text\n",
        "    df = preprocess_text(df)\n",
        "\n",
        "    # Split data into training and testing sets (80:20 ratio)\n",
        "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
        "\n",
        "    # Extract labels\n",
        "    y_train = df_train['label']\n",
        "    y_test = df_test['label']\n",
        "\n",
        "    # Step 4: Vectorize text\n",
        "    X_train, X_test, vectorizer = vectorize_text(df_train, df_test, method='tfidf')\n",
        "\n",
        "    # Step 5: Train and evaluate models\n",
        "    results = train_and_evaluate_models(X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # Step 6: Visualize results\n",
        "    visualize_results(results)\n",
        "\n",
        "    # Find the best model\n",
        "    best_model_name = max(results, key=lambda x: results[x]['accuracy'])\n",
        "    best_model = results[best_model_name]['model']\n",
        "    best_accuracy = results[best_model_name]['accuracy']\n",
        "\n",
        "    print(f\"\\nBest performing model: {best_model_name} with accuracy {best_accuracy:.4f}\")\n",
        "\n",
        "    # Extract important features from the best model\n",
        "    if best_model_name in ['Random Forest', 'Decision Tree', 'SVM']:\n",
        "        print(\"\\nTop 20 important features for classification:\")\n",
        "        top_features = extract_important_features(vectorizer, best_model)\n",
        "        print(top_features)\n",
        "\n",
        "        # Visualize top features\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.barplot(x='importance', y='feature', data=top_features)\n",
        "        plt.title(f'Top 20 Important Features for {best_model_name}')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('top_features.png')\n",
        "        plt.close()\n",
        "\n",
        "    print(\"\\nFake News Detection System completed successfully!\")\n",
        "\n",
        "    # Save the best model and vectorizer for future use\n",
        "    import joblib\n",
        "    joblib.dump(best_model, f'best_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl')\n",
        "    joblib.dump(vectorizer, 'vectorizer.pkl')\n",
        "    print(f\"Best model and vectorizer saved for future use.\")\n",
        "\n",
        "    return results, vectorizer, best_model\n",
        "\n",
        "# Function to make predictions on new data\n",
        "def predict_news(text, model, vectorizer):\n",
        "    \"\"\"\n",
        "    Make predictions on new news articles\n",
        "    \"\"\"\n",
        "    # Preprocess the text\n",
        "    stemmer = PorterStemmer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    cleaned_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
        "    processed_text = ' '.join(cleaned_tokens)\n",
        "\n",
        "    # Vectorize the text\n",
        "    text_vector = vectorizer.transform([processed_text])\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(text_vector)[0]\n",
        "    prediction_proba = model.predict_proba(text_vector)[0]\n",
        "\n",
        "    # Return result\n",
        "    if prediction == 1:\n",
        "        return \"True News\", prediction_proba[1]\n",
        "    else:\n",
        "        return \"Fake News\", prediction_proba[0]\n",
        "\n",
        "# Example usage of the system\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the main pipeline\n",
        "    results, vectorizer, best_model = main()\n",
        "\n",
        "    # Example news article for prediction\n",
        "    sample_news = \"\"\"\n",
        "    Scientists discover new treatment for cancer that shows promising results in early trials.\n",
        "    The research team at University Medical Center has developed a novel approach that targets specific cancer cells\n",
        "    without damaging healthy tissue. The treatment has been tested on a small group of patients with advanced forms of\n",
        "    cancer, and initial results show significant tumor reduction in 85% of participants. The team plans to expand the\n",
        "    clinical trials next year after receiving additional funding from the National Health Institute.\n",
        "    \"\"\"\n",
        "\n",
        "    # Make prediction\n",
        "    result, confidence = predict_news(sample_news, best_model, vectorizer)\n",
        "    print(f\"\\nPrediction for sample news: {result} (Confidence: {confidence:.4f})\")\n",
        "\n",
        "    # Create a simple function for users to input their own news articles\n",
        "    def user_input_prediction():\n",
        "        print(\"\\n=== Fake News Detector ===\")\n",
        "        print(\"Enter a news article to check if it's real or fake (type 'exit' to quit):\")\n",
        "\n",
        "        while True:\n",
        "            user_input = input(\"\\nEnter news text: \")\n",
        "\n",
        "            if user_input.lower() == 'exit':\n",
        "                print(\"Exiting fake news detector.\")\n",
        "                break\n",
        "\n",
        "            if len(user_input) < 50:\n",
        "                print(\"Please enter a longer news article for more accurate prediction.\")\n",
        "                continue\n",
        "\n",
        "            result, confidence = predict_news(user_input, best_model, vectorizer)\n",
        "            print(f\"Prediction: {result} (Confidence: {confidence:.4f})\")\n",
        "\n",
        "    # Uncomment the line below to enable interactive user input\n",
        "    # user_input_prediction()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EDbv0n-y5qiy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}