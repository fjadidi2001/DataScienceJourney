{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjDHSm178GCKhY9GYDaBZg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Artificial_Intelligence_Learning/blob/master/final_edit_alphabetProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEr0UwiDeIKA",
        "outputId": "354314a4-1592-4bfd-aaee-a16268254c83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset:\n",
            "[[255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " ...\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]]\n",
            "\n",
            "Bipolar Encoded Dataset:\n",
            "[[-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " ...\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the dataset from a text file\n",
        "def load_dataset(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    dataset = [list(map(int, line.split())) for line in lines]\n",
        "    return np.array(dataset)\n",
        "\n",
        "file_path = '/content/fars.txt'  # Provide the correct path to your text file\n",
        "original_dataset = load_dataset(file_path)\n",
        "\n",
        "# Step 2: Convert dataset to bipolar encoding based on the specified condition\n",
        "def convert_to_bipolar_with_condition(dataset):\n",
        "    return np.where(dataset > 128, -1, 1)\n",
        "\n",
        "bipolar_dataset = convert_to_bipolar_with_condition(original_dataset)\n",
        "\n",
        "# Print original and bipolar datasets for verification\n",
        "print(\"Original Dataset:\")\n",
        "print(original_dataset)\n",
        "\n",
        "print(\"\\nBipolar Encoded Dataset:\")\n",
        "print(bipolar_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the dataset from a text file\n",
        "def load_dataset(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    dataset = [list(map(int, line.split())) for line in lines]\n",
        "    return np.array(dataset)\n",
        "\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = load_dataset(file_path)\n",
        "\n",
        "# Step 2: Reshape each sequence into a matrix\n",
        "num_fonts = 7\n",
        "sequence_length = 17\n",
        "\n",
        "# Reshape each sequence into a 2D matrix\n",
        "\n",
        "'''\n",
        "This line of code reshapes the original dataset into a new matrix dataset.\n",
        "The \"-1\" indicates that the first dimension of the new dataset will be inferred based on the total size of the original dataset\n",
        "and the specified number of fonts and sequence length. This reshaping is useful for preparing the data for input into a neural network,\n",
        "where the input shape may need to be specified in a certain way.\n",
        "'''\n",
        "matrix_dataset = original_dataset.reshape(-1, num_fonts, sequence_length)\n",
        "\n",
        "\n",
        "# Print the original and matrix datasets for verification\n",
        "print(\"Original Dataset:\")\n",
        "print(original_dataset)\n",
        "\n",
        "print(\"\\nMatrix Dataset:\")\n",
        "print(matrix_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKndHaAzezlY",
        "outputId": "07e461a6-782a-4df4-9516-499c6bbc2bf1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset:\n",
            "[[255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " ...\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]]\n",
            "\n",
            "Matrix Dataset:\n",
            "[[[255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  ...\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]]\n",
            "\n",
            " [[255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  ...\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]]\n",
            "\n",
            " [[255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  ...\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  ...\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]]\n",
            "\n",
            " [[255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  ...\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]]\n",
            "\n",
            " [[255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  ...\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the dataset from a text file\n",
        "def load_dataset(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    dataset = [list(map(int, line.split())) for line in lines]\n",
        "    return np.array(dataset)\n",
        "\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = load_dataset(file_path)\n",
        "\n",
        "# Step 2: Convert dataset to bipolar encoding based on the specified condition\n",
        "def convert_to_bipolar_with_condition(dataset):\n",
        "    return np.where(dataset > 128, -1, 1)\n",
        "\n",
        "bipolar_dataset = convert_to_bipolar_with_condition(original_dataset)\n",
        "\n",
        "# Step 3: Reshape each sequence into a matrix\n",
        "num_fonts = 7\n",
        "sequence_length = 17\n",
        "\n",
        "# Reshape each sequence into a 2D matrix\n",
        "matrix_dataset = bipolar_dataset.reshape(-1, num_fonts, sequence_length)\n",
        "\n",
        "# Print the original, bipolar, and matrix datasets for verification\n",
        "print(\"Original Dataset:\")\n",
        "print(original_dataset)\n",
        "\n",
        "print(\"\\nBipolar Encoded Dataset:\")\n",
        "print(bipolar_dataset)\n",
        "\n",
        "print(\"\\nMatrix Dataset:\")\n",
        "print(matrix_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU5MCzI2fE3F",
        "outputId": "5b46a8a3-c6a2-49eb-c289-54b101474f31"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset:\n",
            "[[255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " ...\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]]\n",
            "\n",
            "Bipolar Encoded Dataset:\n",
            "[[-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " ...\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "Matrix Dataset:\n",
            "[[[-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  ...\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            " [[-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  ...\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            " [[-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  ...\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  ...\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            " [[-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  ...\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            " [[-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  ...\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, 95, 95)\n",
        "\n",
        "# Flatten each matrix to create input vectors\n",
        "input_vectors = character_matrices.reshape(num_characters * num_fonts, -1)\n",
        "\n",
        "# Step 2: Build a simple perceptron network with random weights\n",
        "num_weights = input_size\n",
        "weights = np.random.randn(num_weights, 1)\n",
        "\n",
        "# Step 3: Define activation function (sigmoid) and forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(data, weights):\n",
        "    return sigmoid(np.dot(data, weights))\n",
        "\n",
        "# Step 4: Train the perceptron network for 10 epochs\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for char_font_idx in range(num_characters * num_fonts):\n",
        "        # Select the input vector for the current character and font\n",
        "        input_vector = input_vectors[char_font_idx, :]\n",
        "\n",
        "        # Forward pass\n",
        "        output = forward_pass(input_vector, weights)\n",
        "\n",
        "        # Convert output to bipolar (-1 or 1)\n",
        "        output_bipolar = np.where(output > 0.5, 1, -1)\n",
        "\n",
        "        # Update weights (dummy update, not based on any real learning algorithm)\n",
        "        weights += np.random.randn(num_weights, 1) * 0.01  # Adjust the learning rate as needed\n",
        "\n",
        "# Step 5: Make predictions for each character and font\n",
        "predictions = np.zeros((num_characters, num_fonts))\n",
        "\n",
        "for char_idx in range(num_characters):\n",
        "    for font_idx in range(num_fonts):\n",
        "        input_vector = input_vectors[char_idx * num_fonts + font_idx, :]\n",
        "        predictions[char_idx, font_idx] = np.where(forward_pass(input_vector, weights) > 0.5, 1, -1)\n",
        "\n",
        "# Print predictions for verification\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_oLNKhpe6yA",
        "outputId": "3e7a3070-ae9a-49dd-c531-38219ecda50e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            "[[-1. -1. -1. -1. -1. -1. -1.]\n",
            " [-1. -1. -1.  1. -1. -1. -1.]\n",
            " [-1. -1. -1. -1.  1. -1.  1.]\n",
            " [-1. -1. -1.  1. -1. -1. -1.]\n",
            " [-1. -1. -1. -1.  1. -1. -1.]\n",
            " [-1. -1. -1. -1.  1. -1. -1.]\n",
            " [-1. -1. -1. -1.  1.  1. -1.]\n",
            " [-1. -1. -1. -1.  1. -1. -1.]\n",
            " [-1. -1. -1.  1.  1. -1.  1.]\n",
            " [-1. -1. -1.  1. -1. -1. -1.]\n",
            " [-1. -1.  1. -1.  1. -1. -1.]\n",
            " [-1. -1. -1. -1. -1.  1.  1.]\n",
            " [-1. -1. -1. -1.  1. -1. -1.]\n",
            " [-1. -1. -1. -1.  1.  1.  1.]\n",
            " [-1. -1. -1. -1.  1.  1. -1.]\n",
            " [-1. -1. -1.  1.  1. -1. -1.]\n",
            " [-1. -1. -1. -1.  1. -1.  1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, 95, 95)\n",
        "\n",
        "# Flatten each matrix to create input vectors\n",
        "input_vectors = character_matrices.reshape(num_characters * num_fonts, -1)\n",
        "\n",
        "# Step 2: Build a simple perceptron network with random weights\n",
        "num_weights = input_size\n",
        "weights = np.random.randn(num_weights, 1)\n",
        "\n",
        "# Step 3: Define activation function (sigmoid) and forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(data, weights):\n",
        "    return sigmoid(np.dot(data, weights))\n",
        "\n",
        "# Step 4: Train the perceptron network for 10 epochs\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for char_font_idx in range(num_characters * num_fonts):\n",
        "        # Select the input vector for the current character and font\n",
        "        input_vector = input_vectors[char_font_idx, :]\n",
        "\n",
        "        # Forward pass\n",
        "        output = forward_pass(input_vector, weights)\n",
        "\n",
        "        # Convert output to bipolar (-1 or 1)\n",
        "        output_bipolar = np.where(output > 0.5, 1, -1)\n",
        "\n",
        "        # Update weights (dummy update, not based on any real learning algorithm)\n",
        "        weights += np.random.randn(num_weights, 1) * 0.01  # Adjust the learning rate as needed\n",
        "\n",
        "# Step 5: Make predictions for each character and font\n",
        "predictions = np.zeros((num_characters, num_fonts), dtype=bool)\n",
        "\n",
        "for char_idx in range(num_characters):\n",
        "    for font_idx in range(num_fonts):\n",
        "        input_vector = input_vectors[char_idx * num_fonts + font_idx, :]\n",
        "        predictions[char_idx, font_idx] = np.any(forward_pass(input_vector, weights) > 0.5)\n",
        "\n",
        "# Print predictions for verification\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvU_A28WfQyT",
        "outputId": "86e04ef4-8733-4755-89e8-a25b3827338e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            "[[ True  True  True False False  True  True]\n",
            " [ True  True  True  True  True False  True]\n",
            " [ True  True  True  True  True  True False]\n",
            " [ True  True  True  True False  True  True]\n",
            " [ True  True  True  True  True  True  True]\n",
            " [ True  True  True False False False  True]\n",
            " [ True  True  True  True False False  True]\n",
            " [ True  True  True False False False  True]\n",
            " [ True  True  True  True  True  True False]\n",
            " [ True  True  True  True False False  True]\n",
            " [ True False  True False False False  True]\n",
            " [ True  True  True False False False  True]\n",
            " [ True  True  True False False False False]\n",
            " [ True  True  True False False False  True]\n",
            " [ True  True  True  True  True  True  True]\n",
            " [ True  True  True False  True  True  True]\n",
            " [ True  True  True  True  True False  True]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, 95, 95)\n",
        "print(character_matrices)\n",
        "# Flatten each matrix to create input vectors\n",
        "input_vectors = character_matrices.reshape(num_characters * num_fonts, -1)\n",
        "print(input_vectors)\n",
        "# Create target labels (true if the character is present in any font)\n",
        "target_labels = np.zeros((num_characters, num_fonts), dtype=int)\n",
        "for char_idx in range(num_characters):\n",
        "    random_font_idx = np.random.randint(num_fonts)  # Randomly select a font for each character\n",
        "    target_labels[char_idx, random_font_idx] = 1\n",
        "\n",
        "# Step 2: Build a simple perceptron network with random weights\n",
        "num_weights = input_size\n",
        "weights = np.random.randn(num_weights, 1)\n",
        "\n",
        "# Step 3: Define activation function (sigmoid) and forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(data, weights):\n",
        "    return sigmoid(np.dot(data, weights))\n",
        "\n",
        "# Step 4: Train the perceptron network for 10 epochs\n",
        "epochs = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for char_font_idx in range(num_characters * num_fonts):\n",
        "        # Select the input vector for the current character and font\n",
        "        input_vector = input_vectors[char_font_idx, :].reshape(1, -1)\n",
        "\n",
        "        # Select the target label for the current character and font\n",
        "        target_label = target_labels[char_font_idx // num_fonts, char_font_idx % num_fonts]\n",
        "\n",
        "        # Forward pass\n",
        "        output = forward_pass(input_vector, weights)\n",
        "\n",
        "        # Update weights using perceptron learning rule\n",
        "        weights += learning_rate * (target_label - output) * input_vector.T\n",
        "\n",
        "# Step 5: Make predictions for each character and font\n",
        "predictions = np.zeros((num_characters, num_fonts), dtype=int)\n",
        "\n",
        "for char_idx in range(num_characters):\n",
        "    for font_idx in range(num_fonts):\n",
        "        input_vector = input_vectors[char_idx * num_fonts + font_idx, :].reshape(1, -1)\n",
        "        predictions[char_idx, font_idx] = int(forward_pass(input_vector, weights) > 0.5)\n",
        "\n",
        "# Print predictions and target labels for verification\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n",
        "print(\"\\nTarget Labels:\")\n",
        "print(target_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6IvOj2SfTx4",
        "outputId": "fa775ba7-0252-41e4-c561-12be6d7be3c0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]]\n",
            "[[-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " ...\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]]\n",
            "Predictions:\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0]]\n",
            "\n",
            "Target Labels:\n",
            "[[0 0 0 0 0 0 1]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 1 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 1 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 1]\n",
            " [1 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, 95, 95)\n",
        "print(character_matrices)\n",
        "# Flatten each matrix to create input vectors\n",
        "input_vectors = character_matrices.reshape(num_characters * num_fonts, -1)\n",
        "print(input_vectors)\n",
        "\n",
        "# Add bias term to input vectors\n",
        "input_vectors_with_bias = np.c_[input_vectors, np.ones((num_characters * num_fonts, 1))]  # Adding a column of ones for bias\n",
        "\n",
        "# Create target labels (true if the character is present in any font)\n",
        "target_labels = np.zeros((num_characters, num_fonts), dtype=int)\n",
        "for char_idx in range(num_characters):\n",
        "    random_font_idx = np.random.randint(num_fonts)  # Randomly select a font for each character\n",
        "    target_labels[char_idx, random_font_idx] = 1\n",
        "\n",
        "# Step 2: Build a simple perceptron network with random weights\n",
        "num_weights = input_size + 1  # Additional weight for bias\n",
        "weights = np.random.randn(num_weights, 1)\n",
        "\n",
        "# Step 3: Define activation function (sigmoid) and forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(data, weights):\n",
        "    return sigmoid(np.dot(data, weights))\n",
        "\n",
        "# Step 4: Train the perceptron network for 10 epochs\n",
        "epochs = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for char_font_idx in range(num_characters * num_fonts):\n",
        "        # Select the input vector for the current character and font\n",
        "        input_vector = input_vectors_with_bias[char_font_idx, :].reshape(1, -1)\n",
        "\n",
        "        # Select the target label for the current character and font\n",
        "        target_label = target_labels[char_font_idx // num_fonts, char_font_idx % num_fonts]\n",
        "\n",
        "        # Forward pass\n",
        "        output = forward_pass(input_vector, weights)\n",
        "\n",
        "        # Update weights using perceptron learning rule\n",
        "        weights += learning_rate * (target_label - output) * input_vector.T\n",
        "\n",
        "# Step 5: Make predictions for each character and font\n",
        "predictions = np.zeros((num_characters, num_fonts), dtype=int)\n",
        "\n",
        "for char_idx in range(num_characters):\n",
        "    for font_idx in range(num_fonts):\n",
        "        input_vector = input_vectors_with_bias[char_idx * num_fonts + font_idx, :].reshape(1, -1)\n",
        "        predictions[char_idx, font_idx] = int(forward_pass(input_vector, weights) > 0.5)\n",
        "\n",
        "# Print predictions and target labels for verification\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n",
        "print(\"\\nTarget Labels:\")\n",
        "print(target_labels)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.sum(predictions == target_labels) / (num_characters * num_fonts)\n",
        "print(\"\\nAccuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsQRVCjQfZyi",
        "outputId": "27d874ab-ad0c-4085-bbb2-b974e2f4cdae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]]\n",
            "[[-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " ...\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]]\n",
            "Predictions:\n",
            "[[0 0 1 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 0 0 0 1 0 1]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 1]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 0 0 1 0 0 0]]\n",
            "\n",
            "Target Labels:\n",
            "[[0 0 1 0 0 0 0]\n",
            " [0 0 1 0 0 0 0]\n",
            " [0 0 1 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1]\n",
            " [0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 0 0 1 0 0 0]]\n",
            "\n",
            "Accuracy: 0.8907563025210085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ... (previous code remains the same)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "test_size = 0.2  # You can adjust this proportion based on your dataset size\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_vectors_with_bias, target_labels.flatten(), test_size=test_size, random_state=42)\n",
        "\n",
        "# Step 2: Build a simple perceptron network with random weights\n",
        "num_weights = input_size + 1  # Additional weight for bias\n",
        "weights = np.random.randn(num_weights, 1)\n",
        "\n",
        "# Step 3: Define activation function (sigmoid) and forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(data, weights):\n",
        "    return sigmoid(np.dot(data, weights))\n",
        "\n",
        "# Step 4: Train the perceptron network for 10 epochs\n",
        "epochs = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for idx in range(len(X_train)):\n",
        "        # Select the input vector for the current sample\n",
        "        input_vector = X_train[idx, :].reshape(1, -1)\n",
        "\n",
        "        # Select the target label for the current sample\n",
        "        target_label = y_train[idx]\n",
        "\n",
        "        # Forward pass\n",
        "        output = forward_pass(input_vector, weights)\n",
        "\n",
        "        # Update weights using perceptron learning rule\n",
        "        weights += learning_rate * (target_label - output) * input_vector.T\n",
        "\n",
        "# Step 5: Make predictions on the test set\n",
        "predictions = np.zeros_like(y_test)\n",
        "\n",
        "for idx in range(len(X_test)):\n",
        "    input_vector = X_test[idx, :].reshape(1, -1)\n",
        "    predictions[idx] = int(forward_pass(input_vector, weights) > 0.5)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "accuracy = np.sum(predictions == y_test) / len(y_test)\n",
        "print(\"\\nAccuracy on Test Set:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eK8r4SYbfmsp",
        "outputId": "27614a73-42f6-4271-9db9-5f62d7a3452e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy on Test Set: 0.9166666666666666\n"
          ]
        }
      ]
    }
  ]
}