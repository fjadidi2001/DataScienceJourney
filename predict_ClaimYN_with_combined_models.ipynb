{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMDculpBnUIRDwmh2gac++f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/DataScienceJourney/blob/master/predict_ClaimYN_with_combined_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the dataset\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify file path\n",
        "file_path = '/content/drive/My Drive/telematics_syn.csv'\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Step 2: Explore the data\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(data.head())\n",
        "print(\"\\nDataset information:\")\n",
        "print(data.info())\n",
        "print(\"\\nChecking for missing values:\")\n",
        "print(data.isnull().sum())\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(data.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY_-bCvrFehd",
        "outputId": "67258daf-6ec9-4190-c1ef-60d1bd3e0089"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "First few rows of the dataset:\n",
            "   Duration  Insured.age Insured.sex  Car.age  Marital  Car.use  Credit.score  \\\n",
            "0       366           45        Male       -1  Married  Commute         609.0   \n",
            "1       182           44      Female        3  Married  Commute         575.0   \n",
            "2       184           48      Female        6  Married  Commute         847.0   \n",
            "3       183           71        Male        6  Married  Private         842.0   \n",
            "4       183           84        Male       10  Married  Private         856.0   \n",
            "\n",
            "  Region  Annual.miles.drive  Years.noclaims  ...  Left.turn.intensity10  \\\n",
            "0  Urban             6213.71              25  ...                    1.0   \n",
            "1  Urban            12427.42              20  ...                   58.0   \n",
            "2  Urban            12427.42              14  ...                    0.0   \n",
            "3  Urban             6213.71              43  ...                    0.0   \n",
            "4  Urban             6213.71              65  ...                    2.0   \n",
            "\n",
            "   Left.turn.intensity11  Left.turn.intensity12  Right.turn.intensity08  \\\n",
            "0                    0.0                    0.0                     3.0   \n",
            "1                   24.0                   11.0                  1099.0   \n",
            "2                    0.0                    0.0                     0.0   \n",
            "3                    0.0                    0.0                     0.0   \n",
            "4                    0.0                    0.0                   325.0   \n",
            "\n",
            "   Right.turn.intensity09  Right.turn.intensity10  Right.turn.intensity11  \\\n",
            "0                     1.0                     0.0                     0.0   \n",
            "1                   615.0                   219.0                   101.0   \n",
            "2                     0.0                     0.0                     0.0   \n",
            "3                     0.0                     0.0                     0.0   \n",
            "4                   111.0                    18.0                     4.0   \n",
            "\n",
            "   Right.turn.intensity12  NB_Claim    AMT_Claim  \n",
            "0                     0.0         1  5100.171753  \n",
            "1                    40.0         1   883.554840  \n",
            "2                     0.0         0     0.000000  \n",
            "3                     0.0         0     0.000000  \n",
            "4                     2.0         0     0.000000  \n",
            "\n",
            "[5 rows x 52 columns]\n",
            "\n",
            "Dataset information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100000 entries, 0 to 99999\n",
            "Data columns (total 52 columns):\n",
            " #   Column                  Non-Null Count   Dtype  \n",
            "---  ------                  --------------   -----  \n",
            " 0   Duration                100000 non-null  int64  \n",
            " 1   Insured.age             100000 non-null  int64  \n",
            " 2   Insured.sex             100000 non-null  object \n",
            " 3   Car.age                 100000 non-null  int64  \n",
            " 4   Marital                 100000 non-null  object \n",
            " 5   Car.use                 100000 non-null  object \n",
            " 6   Credit.score            100000 non-null  float64\n",
            " 7   Region                  100000 non-null  object \n",
            " 8   Annual.miles.drive      100000 non-null  float64\n",
            " 9   Years.noclaims          100000 non-null  int64  \n",
            " 10  Territory               100000 non-null  int64  \n",
            " 11  Annual.pct.driven       100000 non-null  float64\n",
            " 12  Total.miles.driven      100000 non-null  float64\n",
            " 13  Pct.drive.mon           100000 non-null  float64\n",
            " 14  Pct.drive.tue           100000 non-null  float64\n",
            " 15  Pct.drive.wed           100000 non-null  float64\n",
            " 16  Pct.drive.thr           100000 non-null  float64\n",
            " 17  Pct.drive.fri           100000 non-null  float64\n",
            " 18  Pct.drive.sat           100000 non-null  float64\n",
            " 19  Pct.drive.sun           100000 non-null  float64\n",
            " 20  Pct.drive.2hrs          100000 non-null  float64\n",
            " 21  Pct.drive.3hrs          100000 non-null  float64\n",
            " 22  Pct.drive.4hrs          100000 non-null  float64\n",
            " 23  Pct.drive.wkday         100000 non-null  float64\n",
            " 24  Pct.drive.wkend         100000 non-null  float64\n",
            " 25  Pct.drive.rush am       100000 non-null  float64\n",
            " 26  Pct.drive.rush pm       100000 non-null  float64\n",
            " 27  Avgdays.week            100000 non-null  float64\n",
            " 28  Accel.06miles           100000 non-null  float64\n",
            " 29  Accel.08miles           100000 non-null  float64\n",
            " 30  Accel.09miles           100000 non-null  float64\n",
            " 31  Accel.11miles           100000 non-null  float64\n",
            " 32  Accel.12miles           100000 non-null  float64\n",
            " 33  Accel.14miles           100000 non-null  float64\n",
            " 34  Brake.06miles           100000 non-null  float64\n",
            " 35  Brake.08miles           100000 non-null  float64\n",
            " 36  Brake.09miles           100000 non-null  float64\n",
            " 37  Brake.11miles           100000 non-null  float64\n",
            " 38  Brake.12miles           100000 non-null  float64\n",
            " 39  Brake.14miles           100000 non-null  float64\n",
            " 40  Left.turn.intensity08   100000 non-null  float64\n",
            " 41  Left.turn.intensity09   100000 non-null  float64\n",
            " 42  Left.turn.intensity10   100000 non-null  float64\n",
            " 43  Left.turn.intensity11   100000 non-null  float64\n",
            " 44  Left.turn.intensity12   100000 non-null  float64\n",
            " 45  Right.turn.intensity08  100000 non-null  float64\n",
            " 46  Right.turn.intensity09  100000 non-null  float64\n",
            " 47  Right.turn.intensity10  100000 non-null  float64\n",
            " 48  Right.turn.intensity11  100000 non-null  float64\n",
            " 49  Right.turn.intensity12  100000 non-null  float64\n",
            " 50  NB_Claim                100000 non-null  int64  \n",
            " 51  AMT_Claim               100000 non-null  float64\n",
            "dtypes: float64(42), int64(6), object(4)\n",
            "memory usage: 39.7+ MB\n",
            "None\n",
            "\n",
            "Checking for missing values:\n",
            "Duration                  0\n",
            "Insured.age               0\n",
            "Insured.sex               0\n",
            "Car.age                   0\n",
            "Marital                   0\n",
            "Car.use                   0\n",
            "Credit.score              0\n",
            "Region                    0\n",
            "Annual.miles.drive        0\n",
            "Years.noclaims            0\n",
            "Territory                 0\n",
            "Annual.pct.driven         0\n",
            "Total.miles.driven        0\n",
            "Pct.drive.mon             0\n",
            "Pct.drive.tue             0\n",
            "Pct.drive.wed             0\n",
            "Pct.drive.thr             0\n",
            "Pct.drive.fri             0\n",
            "Pct.drive.sat             0\n",
            "Pct.drive.sun             0\n",
            "Pct.drive.2hrs            0\n",
            "Pct.drive.3hrs            0\n",
            "Pct.drive.4hrs            0\n",
            "Pct.drive.wkday           0\n",
            "Pct.drive.wkend           0\n",
            "Pct.drive.rush am         0\n",
            "Pct.drive.rush pm         0\n",
            "Avgdays.week              0\n",
            "Accel.06miles             0\n",
            "Accel.08miles             0\n",
            "Accel.09miles             0\n",
            "Accel.11miles             0\n",
            "Accel.12miles             0\n",
            "Accel.14miles             0\n",
            "Brake.06miles             0\n",
            "Brake.08miles             0\n",
            "Brake.09miles             0\n",
            "Brake.11miles             0\n",
            "Brake.12miles             0\n",
            "Brake.14miles             0\n",
            "Left.turn.intensity08     0\n",
            "Left.turn.intensity09     0\n",
            "Left.turn.intensity10     0\n",
            "Left.turn.intensity11     0\n",
            "Left.turn.intensity12     0\n",
            "Right.turn.intensity08    0\n",
            "Right.turn.intensity09    0\n",
            "Right.turn.intensity10    0\n",
            "Right.turn.intensity11    0\n",
            "Right.turn.intensity12    0\n",
            "NB_Claim                  0\n",
            "AMT_Claim                 0\n",
            "dtype: int64\n",
            "\n",
            "Basic statistics:\n",
            "            Duration    Insured.age        Car.age   Credit.score  \\\n",
            "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
            "mean      314.204060      51.378950       5.639720     800.888870   \n",
            "std        79.746222      15.467075       4.062135      83.382316   \n",
            "min        27.000000      16.000000      -2.000000     422.000000   \n",
            "25%       200.000000      39.000000       2.000000     766.000000   \n",
            "50%       365.000000      51.000000       5.000000     825.000000   \n",
            "75%       366.000000      63.000000       8.000000     856.000000   \n",
            "max       366.000000     103.000000      20.000000     900.000000   \n",
            "\n",
            "       Annual.miles.drive  Years.noclaims      Territory  Annual.pct.driven  \\\n",
            "count       100000.000000   100000.000000  100000.000000      100000.000000   \n",
            "mean          9124.122908       28.839960      56.531390           0.502294   \n",
            "std           3826.144730       16.123717      24.036518           0.299189   \n",
            "min              0.000000        0.000000      11.000000           0.002740   \n",
            "25%           6213.710000       15.000000      35.000000           0.249315   \n",
            "50%           7456.452000       29.000000      62.000000           0.490411   \n",
            "75%          12427.420000       41.000000      78.000000           0.753425   \n",
            "max          56731.172300       79.000000      91.000000           1.000000   \n",
            "\n",
            "       Total.miles.driven  Pct.drive.mon  ...  Left.turn.intensity10  \\\n",
            "count       100000.000000  100000.000000  ...          100000.000000   \n",
            "mean          4833.575303       0.139365  ...             551.574010   \n",
            "std           4545.943016       0.042807  ...           14687.929802   \n",
            "min              0.095298       0.000000  ...               0.000000   \n",
            "25%           1529.897500       0.120894  ...               0.000000   \n",
            "50%           3468.287765       0.137909  ...               3.000000   \n",
            "75%           6779.876842       0.155203  ...              30.000000   \n",
            "max          47282.603936       0.998172  ...          794380.000000   \n",
            "\n",
            "       Left.turn.intensity11  Left.turn.intensity12  Right.turn.intensity08  \\\n",
            "count          100000.000000          100000.000000           100000.000000   \n",
            "mean              487.340690             447.758420              843.461830   \n",
            "std             14198.331308           13719.790281            11630.185503   \n",
            "min                 0.000000               0.000000                0.000000   \n",
            "25%                 0.000000               0.000000               11.000000   \n",
            "50%                 1.000000               0.000000              122.000000   \n",
            "75%                 9.000000               2.000000              680.000000   \n",
            "max            793926.000000          793170.000000           841210.000000   \n",
            "\n",
            "       Right.turn.intensity09  Right.turn.intensity10  Right.turn.intensity11  \\\n",
            "count           100000.000000           100000.000000           100000.000000   \n",
            "mean               565.056100              326.654840              246.713120   \n",
            "std              10657.402935             9460.244357             8977.569994   \n",
            "min                  0.000000                0.000000                0.000000   \n",
            "25%                  3.000000                0.000000                0.000000   \n",
            "50%                 43.000000                7.000000                2.000000   \n",
            "75%                321.000000               81.000000               27.000000   \n",
            "max             841207.000000           841200.000000           841176.000000   \n",
            "\n",
            "       Right.turn.intensity12      NB_Claim      AMT_Claim  \n",
            "count           100000.000000  100000.00000  100000.000000  \n",
            "mean               198.753690       0.04494     137.602253  \n",
            "std               8585.177049       0.21813    1264.320056  \n",
            "min                  0.000000       0.00000       0.000000  \n",
            "25%                  0.000000       0.00000       0.000000  \n",
            "50%                  0.000000       0.00000       0.000000  \n",
            "75%                  9.000000       0.00000       0.000000  \n",
            "max             841144.000000       3.00000  104074.886700  \n",
            "\n",
            "[8 rows x 48 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 3: Create target variable ClaimYN\n",
        "data['ClaimYN'] = ((data['NB_Claim'] >= 1) & (data['AMT_Claim'] > 1000)).astype(int)\n",
        "\n",
        "# Display distribution of ClaimYN\n",
        "print(\"\\nDistribution of ClaimYN:\")\n",
        "print(data['ClaimYN'].value_counts(normalize=True) * 100)\n",
        "print(\"\\nAbsolute counts:\")\n",
        "print(data['ClaimYN'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUkifxGCF_Bq",
        "outputId": "b06dd651-3b26-489c-895b-ec022a6983b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Distribution of ClaimYN:\n",
            "ClaimYN\n",
            "0    97.302\n",
            "1     2.698\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Absolute counts:\n",
            "ClaimYN\n",
            "0    97302\n",
            "1     2698\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import additional required libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# Separate features and target\n",
        "# Exclude NB_Claim and AMT_Claim as they were used to create the target\n",
        "features = data.drop(['ClaimYN', 'NB_Claim', 'AMT_Claim'], axis=1)\n",
        "\n",
        "# Convert categorical variables to numeric using one-hot encoding\n",
        "categorical_columns = ['Insured.sex', 'Marital', 'Car.use', 'Region']\n",
        "features = pd.get_dummies(features, columns=categorical_columns)\n",
        "\n",
        "X = features\n",
        "y = data['ClaimYN']\n",
        "\n",
        "# First split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Apply SMOTE to balance the training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Print the distribution of classes before and after SMOTE\n",
        "print(\"\\nClass distribution before SMOTE:\")\n",
        "print(Counter(y_train))\n",
        "\n",
        "print(\"\\nClass distribution after SMOTE:\")\n",
        "print(Counter(y_train_balanced))\n",
        "\n",
        "# Print the shapes of the resulting datasets\n",
        "print(\"\\nDataset shapes:\")\n",
        "print(f\"X_train_balanced: {X_train_balanced.shape}\")\n",
        "print(f\"y_train_balanced: {y_train_balanced.shape}\")\n",
        "print(f\"X_test: {X_test.shape}\")\n",
        "print(f\"y_test: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUBdH8RLIlUE",
        "outputId": "251a5d37-eb73-4d3f-f97f-c419d0e3a81b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Class distribution before SMOTE:\n",
            "Counter({0: 77842, 1: 2158})\n",
            "\n",
            "Class distribution after SMOTE:\n",
            "Counter({0: 77842, 1: 77842})\n",
            "\n",
            "Dataset shapes:\n",
            "X_train_balanced: (155684, 56)\n",
            "y_train_balanced: (155684,)\n",
            "X_test: (20000, 56)\n",
            "y_test: (20000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Separate features and target\n",
        "# Exclude NB_Claim and AMT_Claim as they were used to create the target\n",
        "features = data.drop(['ClaimYN', 'NB_Claim', 'AMT_Claim'], axis=1)\n",
        "\n",
        "# Convert categorical variables to numeric using one-hot encoding\n",
        "categorical_columns = ['Insured.sex', 'Marital', 'Car.use', 'Region']\n",
        "features = pd.get_dummies(features, columns=categorical_columns)\n",
        "\n",
        "X = features\n",
        "y = data['ClaimYN']\n",
        "\n",
        "# Print initial class distribution\n",
        "print(\"Initial class distribution:\")\n",
        "print(Counter(y))\n",
        "\n",
        "# First split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Print distribution after split\n",
        "print(\"\\nClass distribution after split:\")\n",
        "print(\"Training set:\", Counter(y_train))\n",
        "print(\"Test set:\", Counter(y_test))\n",
        "\n",
        "# Apply SMOTE to balance the training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Print the distribution after SMOTE\n",
        "print(\"\\nClass distribution after SMOTE (training set only):\")\n",
        "print(Counter(y_train_balanced))\n",
        "\n",
        "# Print the shapes of the resulting datasets\n",
        "print(\"\\nDataset shapes:\")\n",
        "print(f\"Original dataset: {X.shape}\")\n",
        "print(f\"X_train_balanced: {X_train_balanced.shape}\")\n",
        "print(f\"y_train_balanced: {y_train_balanced.shape}\")\n",
        "print(f\"X_test: {X_test.shape}\")\n",
        "print(f\"y_test: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSZKURJ14mEG",
        "outputId": "aba1c13a-bcef-4472-e5f4-d6e9ce0482f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial class distribution:\n",
            "Counter({0: 97302, 1: 2698})\n",
            "\n",
            "Class distribution after split:\n",
            "Training set: Counter({0: 77842, 1: 2158})\n",
            "Test set: Counter({0: 19460, 1: 540})\n",
            "\n",
            "Class distribution after SMOTE (training set only):\n",
            "Counter({0: 77842, 1: 77842})\n",
            "\n",
            "Dataset shapes:\n",
            "Original dataset: (100000, 56)\n",
            "X_train_balanced: (155684, 56)\n",
            "y_train_balanced: (155684,)\n",
            "X_test: (20000, 56)\n",
            "y_test: (20000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Print initial column count\n",
        "print(\"Initial number of columns:\", len(data.columns))\n",
        "\n",
        "# Separate features and target\n",
        "# Exclude NB_Claim and AMT_Claim as they were used to create the target\n",
        "features = data.drop(['ClaimYN', 'NB_Claim', 'AMT_Claim'], axis=1)\n",
        "print(\"\\nNumber of columns after dropping target and claim columns:\", len(features.columns))\n",
        "\n",
        "# Print categorical columns unique values\n",
        "print(\"\\nUnique values in categorical columns:\")\n",
        "categorical_columns = ['Insured.sex', 'Marital', 'Car.use', 'Region']\n",
        "for col in categorical_columns:\n",
        "    print(f\"{col}: {features[col].unique()}\")\n",
        "\n",
        "# Convert categorical variables to numeric using one-hot encoding\n",
        "features = pd.get_dummies(features, columns=categorical_columns)\n",
        "print(\"\\nColumns after one-hot encoding:\")\n",
        "print(features.columns.tolist())\n",
        "print(\"\\nTotal number of columns after encoding:\", len(features.columns))\n",
        "\n",
        "X = features\n",
        "y = data['ClaimYN']\n",
        "\n",
        "# Proceed with train-test split and SMOTE\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Print final shapes\n",
        "print(\"\\nFinal dataset shapes:\")\n",
        "print(f\"Original features: {X.shape}\")\n",
        "print(f\"X_train_balanced: {X_train_balanced.shape}\")\n",
        "print(f\"X_test: {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVyANxKf6BEv",
        "outputId": "02250205-7ac0-4af0-f57f-e0da7e8627d5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial number of columns: 53\n",
            "\n",
            "Number of columns after dropping target and claim columns: 50\n",
            "\n",
            "Unique values in categorical columns:\n",
            "Insured.sex: ['Male' 'Female']\n",
            "Marital: ['Married' 'Single']\n",
            "Car.use: ['Commute' 'Private' 'Commercial' 'Farmer']\n",
            "Region: ['Urban' 'Rural']\n",
            "\n",
            "Columns after one-hot encoding:\n",
            "['Duration', 'Insured.age', 'Car.age', 'Credit.score', 'Annual.miles.drive', 'Years.noclaims', 'Territory', 'Annual.pct.driven', 'Total.miles.driven', 'Pct.drive.mon', 'Pct.drive.tue', 'Pct.drive.wed', 'Pct.drive.thr', 'Pct.drive.fri', 'Pct.drive.sat', 'Pct.drive.sun', 'Pct.drive.2hrs', 'Pct.drive.3hrs', 'Pct.drive.4hrs', 'Pct.drive.wkday', 'Pct.drive.wkend', 'Pct.drive.rush am', 'Pct.drive.rush pm', 'Avgdays.week', 'Accel.06miles', 'Accel.08miles', 'Accel.09miles', 'Accel.11miles', 'Accel.12miles', 'Accel.14miles', 'Brake.06miles', 'Brake.08miles', 'Brake.09miles', 'Brake.11miles', 'Brake.12miles', 'Brake.14miles', 'Left.turn.intensity08', 'Left.turn.intensity09', 'Left.turn.intensity10', 'Left.turn.intensity11', 'Left.turn.intensity12', 'Right.turn.intensity08', 'Right.turn.intensity09', 'Right.turn.intensity10', 'Right.turn.intensity11', 'Right.turn.intensity12', 'Insured.sex_Female', 'Insured.sex_Male', 'Marital_Married', 'Marital_Single', 'Car.use_Commercial', 'Car.use_Commute', 'Car.use_Farmer', 'Car.use_Private', 'Region_Rural', 'Region_Urban']\n",
            "\n",
            "Total number of columns after encoding: 56\n",
            "\n",
            "Final dataset shapes:\n",
            "Original features: (100000, 56)\n",
            "X_train_balanced: (155684, 56)\n",
            "X_test: (20000, 56)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Feature Engineering\n",
        "def create_aggregated_features(df):\n",
        "    \"\"\"Create aggregated features from existing ones\"\"\"\n",
        "\n",
        "    # Aggregate acceleration features\n",
        "    df['Agg_Accel'] = df[[\n",
        "        'Accel.06miles', 'Accel.08miles', 'Accel.09miles',\n",
        "        'Accel.11miles', 'Accel.12miles', 'Accel.14miles'\n",
        "    ]].mean(axis=1)\n",
        "\n",
        "    # Aggregate braking features\n",
        "    df['Agg_Brake'] = df[[\n",
        "        'Brake.06miles', 'Brake.08miles', 'Brake.09miles',\n",
        "        'Brake.11miles', 'Brake.12miles', 'Brake.14miles'\n",
        "    ]].mean(axis=1)\n",
        "\n",
        "    # Aggregate left turn features\n",
        "    df['Agg_Left_Turn'] = df[[\n",
        "        'Left.turn.intensity08', 'Left.turn.intensity09', 'Left.turn.intensity10',\n",
        "        'Left.turn.intensity11', 'Left.turn.intensity12'\n",
        "    ]].mean(axis=1)\n",
        "\n",
        "    # Aggregate right turn features\n",
        "    df['Agg_Right_Turn'] = df[[\n",
        "        'Right.turn.intensity08', 'Right.turn.intensity09', 'Right.turn.intensity10',\n",
        "        'Right.turn.intensity11', 'Right.turn.intensity12'\n",
        "    ]].mean(axis=1)\n",
        "\n",
        "    # Create overall harsh driving score\n",
        "    df['Harsh_Driving_Score'] = (\n",
        "        df['Agg_Accel'] + df['Agg_Brake'] +\n",
        "        df['Agg_Left_Turn'] + df['Agg_Right_Turn']\n",
        "    ) / 4\n",
        "\n",
        "    # Create rush hour driving ratio\n",
        "    df['Rush_Hour_Ratio'] = (\n",
        "        df['Pct.drive.rush am'] + df['Pct.drive.rush pm']\n",
        "    ) / df['Total.miles.driven']\n",
        "\n",
        "    return df\n",
        "\n",
        "# Standardization function\n",
        "def standardize_features(X_train, X_test):\n",
        "    \"\"\"Standardize numerical features\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Get numerical columns (exclude dummy variables)\n",
        "    numerical_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "    # Fit and transform training data\n",
        "    X_train_scaled = X_train.copy()\n",
        "    X_train_scaled[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
        "\n",
        "    # Transform test data\n",
        "    X_test_scaled = X_test.copy()\n",
        "    X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, scaler\n",
        "\n",
        "# Apply preprocessing pipeline\n",
        "def preprocess_data(X_train, X_test):\n",
        "    \"\"\"Complete preprocessing pipeline\"\"\"\n",
        "\n",
        "    # 1. Feature Engineering\n",
        "    print(\"Applying feature engineering...\")\n",
        "    X_train = create_aggregated_features(X_train)\n",
        "    X_test = create_aggregated_features(X_test)\n",
        "\n",
        "    # 2. Standardization\n",
        "    print(\"Standardizing features...\")\n",
        "    X_train_scaled, X_test_scaled, scaler = standardize_features(X_train, X_test)\n",
        "\n",
        "    # Print feature names and their shapes\n",
        "    print(\"\\nFinal feature set:\")\n",
        "    print(f\"Training set shape: {X_train_scaled.shape}\")\n",
        "    print(f\"Test set shape: {X_test_scaled.shape}\")\n",
        "    print(\"\\nFeatures included:\")\n",
        "    print(X_train_scaled.columns.tolist())\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, scaler\n",
        "\n",
        "# Apply preprocessing to our balanced dataset\n",
        "X_train_processed, X_test_processed, scaler = preprocess_data(X_train_balanced, X_test)\n",
        "\n",
        "# Print sample statistics to verify preprocessing\n",
        "print(\"\\nSample statistics after preprocessing:\")\n",
        "print(X_train_processed.describe().round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLL64lOS7up1",
        "outputId": "6c24984a-de85-47b8-841a-1dd1a52740d8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying feature engineering...\n",
            "Standardizing features...\n",
            "\n",
            "Final feature set:\n",
            "Training set shape: (155684, 62)\n",
            "Test set shape: (20000, 62)\n",
            "\n",
            "Features included:\n",
            "['Duration', 'Insured.age', 'Car.age', 'Credit.score', 'Annual.miles.drive', 'Years.noclaims', 'Territory', 'Annual.pct.driven', 'Total.miles.driven', 'Pct.drive.mon', 'Pct.drive.tue', 'Pct.drive.wed', 'Pct.drive.thr', 'Pct.drive.fri', 'Pct.drive.sat', 'Pct.drive.sun', 'Pct.drive.2hrs', 'Pct.drive.3hrs', 'Pct.drive.4hrs', 'Pct.drive.wkday', 'Pct.drive.wkend', 'Pct.drive.rush am', 'Pct.drive.rush pm', 'Avgdays.week', 'Accel.06miles', 'Accel.08miles', 'Accel.09miles', 'Accel.11miles', 'Accel.12miles', 'Accel.14miles', 'Brake.06miles', 'Brake.08miles', 'Brake.09miles', 'Brake.11miles', 'Brake.12miles', 'Brake.14miles', 'Left.turn.intensity08', 'Left.turn.intensity09', 'Left.turn.intensity10', 'Left.turn.intensity11', 'Left.turn.intensity12', 'Right.turn.intensity08', 'Right.turn.intensity09', 'Right.turn.intensity10', 'Right.turn.intensity11', 'Right.turn.intensity12', 'Insured.sex_Female', 'Insured.sex_Male', 'Marital_Married', 'Marital_Single', 'Car.use_Commercial', 'Car.use_Commute', 'Car.use_Farmer', 'Car.use_Private', 'Region_Rural', 'Region_Urban', 'Agg_Accel', 'Agg_Brake', 'Agg_Left_Turn', 'Agg_Right_Turn', 'Harsh_Driving_Score', 'Rush_Hour_Ratio']\n",
            "\n",
            "Sample statistics after preprocessing:\n",
            "        Duration  Insured.age    Car.age  Credit.score  Annual.miles.drive  \\\n",
            "count  155684.00    155684.00  155684.00     155684.00           155684.00   \n",
            "mean       -0.00         0.00       0.00          0.00               -0.00   \n",
            "std         1.00         1.00       1.00          1.00                1.00   \n",
            "min        -4.70        -2.19      -1.85         -4.21               -2.44   \n",
            "25%         0.40        -0.82      -0.78         -0.57               -0.84   \n",
            "50%         0.48        -0.06      -0.25          0.23               -0.04   \n",
            "75%         0.50         0.70       0.55          0.73                0.76   \n",
            "max         0.50         3.72       4.02          1.41               12.17   \n",
            "\n",
            "       Years.noclaims  Territory  Annual.pct.driven  Total.miles.driven  \\\n",
            "count       155684.00  155684.00          155684.00           155684.00   \n",
            "mean             0.00       0.00               0.00               -0.00   \n",
            "std              1.00       1.00               1.00                1.00   \n",
            "min             -1.65      -2.07              -2.09               -1.29   \n",
            "25%             -0.86      -0.89              -0.76               -0.77   \n",
            "50%             -0.12       0.25               0.09               -0.21   \n",
            "75%              0.74       0.84               0.98                0.55   \n",
            "max              3.54       1.56               1.25                7.87   \n",
            "\n",
            "       Pct.drive.mon  ...  Right.turn.intensity09  Right.turn.intensity10  \\\n",
            "count      155684.00  ...               155684.00               155684.00   \n",
            "mean            0.00  ...                    0.00                    0.00   \n",
            "std             1.00  ...                    1.00                    1.00   \n",
            "min            -4.12  ...                   -0.07                   -0.05   \n",
            "25%            -0.44  ...                   -0.06                   -0.05   \n",
            "50%            -0.03  ...                   -0.06                   -0.05   \n",
            "75%             0.39  ...                   -0.03                   -0.04   \n",
            "max            25.26  ...                   53.94                   56.68   \n",
            "\n",
            "       Right.turn.intensity11  Right.turn.intensity12  Agg_Accel  Agg_Brake  \\\n",
            "count               155684.00               155684.00  155684.00  155684.00   \n",
            "mean                    -0.00                    0.00      -0.00       0.00   \n",
            "std                      1.00                    1.00       1.00       1.00   \n",
            "min                     -0.04                   -0.04      -0.65      -1.10   \n",
            "25%                     -0.04                   -0.04      -0.49      -0.62   \n",
            "50%                     -0.04                   -0.04      -0.26      -0.25   \n",
            "75%                     -0.04                   -0.04       0.13       0.32   \n",
            "max                     60.77                   71.58      40.62      34.67   \n",
            "\n",
            "       Agg_Left_Turn  Agg_Right_Turn  Harsh_Driving_Score  Rush_Hour_Ratio  \n",
            "count      155684.00       155684.00            155684.00        155684.00  \n",
            "mean            0.00           -0.00                 0.00            -0.00  \n",
            "std             1.00            1.00                 1.00             1.00  \n",
            "min            -0.05           -0.06                -0.08            -0.02  \n",
            "25%            -0.05           -0.06                -0.08            -0.02  \n",
            "50%            -0.05           -0.05                -0.07            -0.02  \n",
            "75%            -0.04           -0.03                -0.05            -0.01  \n",
            "max            51.07           59.07                38.06           349.51  \n",
            "\n",
            "[8 rows x 52 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This preprocessing pipeline:\n",
        "\n",
        "1. **Feature Engineering**:\n",
        "   - Aggregates acceleration/braking features\n",
        "   - Creates composite turn intensity metrics\n",
        "   - Calculates a harsh driving score\n",
        "   - Computes rush hour driving ratio\n",
        "\n",
        "2. **Standardization**:\n",
        "   - Applies StandardScaler to numerical features\n",
        "   - Preserves dummy variables from one-hot encoding\n",
        "\n",
        "3. **Organization**:\n",
        "   - Uses functions for modularity and reusability\n",
        "   - Includes progress printing and verification steps\n",
        "\n",
        "The code will output:\n",
        "- The shape of processed datasets\n",
        "- List of final features\n",
        "- Basic statistics of processed features\n"
      ],
      "metadata": {
        "id": "47-Ti7JX8SDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_tabnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJfrO8zk90fI",
        "outputId": "c8e9495d-5489-4f28-a358-26d6e50b3375"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_tabnet\n",
            "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.26.4)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.5.2)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (4.66.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch_tabnet) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->pytorch_tabnet) (1.3.0)\n",
            "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch_tabnet\n",
            "Successfully installed pytorch_tabnet-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert data to numpy arrays for TabNet\n",
        "X_train_tab = X_train_processed.values\n",
        "X_test_tab = X_test_processed.values\n",
        "y_train_tab = y_train_balanced.values\n",
        "y_test_tab = y_test.values\n",
        "\n",
        "# Initialize TabNet classifier\n",
        "tabnet_model = TabNetClassifier(\n",
        "    n_d=64, n_a=64,\n",
        "    n_steps=5,\n",
        "    gamma=1.5,\n",
        "    n_independent=2,\n",
        "    n_shared=2,\n",
        "    cat_idxs=[],\n",
        "    cat_dims=[],\n",
        "    cat_emb_dim=[],\n",
        "    lambda_sparse=1e-4,\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=2e-2),\n",
        "    mask_type=\"entmax\",\n",
        "    scheduler_params=dict(\n",
        "        mode=\"min\",\n",
        "        patience=5,\n",
        "        min_lr=1e-5,\n",
        "        factor=0.9,\n",
        "    ),\n",
        "    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
        "    verbose=10,\n",
        "    device_name='cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_fIJhyk9yda",
        "outputId": "485c6d0f-6638-44d3-897f-67bd014137d2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import roc_auc_score, matthews_corrcoef, roc_curve\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert data to numpy arrays and ensure correct data types\n",
        "X_train_tab = X_train_processed.astype(np.float32).values\n",
        "X_test_tab = X_test_processed.astype(np.float32).values\n",
        "y_train_tab = y_train_balanced.astype(np.int64).values\n",
        "y_test_tab = y_test.astype(np.int64).values\n",
        "\n",
        "# Print shapes and data types for verification\n",
        "print(\"Training data shape:\", X_train_tab.shape)\n",
        "print(\"Test data shape:\", X_test_tab.shape)\n",
        "print(\"Training labels shape:\", y_train_tab.shape)\n",
        "print(\"Test labels shape:\", y_test_tab.shape)\n",
        "print(\"\\nData types:\")\n",
        "print(\"X_train_tab dtype:\", X_train_tab.dtype)\n",
        "print(\"y_train_tab dtype:\", y_train_tab.dtype)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDX65hwX-07v",
        "outputId": "83bb77ca-c892-43c3-f5a0-19d6f35fab6c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (155684, 62)\n",
            "Test data shape: (20000, 62)\n",
            "Training labels shape: (155684,)\n",
            "Test labels shape: (20000,)\n",
            "\n",
            "Data types:\n",
            "X_train_tab dtype: float32\n",
            "y_train_tab dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize TabNet classifier\n",
        "tabnet_model = TabNetClassifier(\n",
        "    n_d=64,  # Width of the decision prediction layer\n",
        "    n_a=64,  # Width of the attention embedding for each mask\n",
        "    n_steps=5,  # Number of steps in the architecture\n",
        "    gamma=1.5,\n",
        "    n_independent=2,\n",
        "    n_shared=2,\n",
        "    cat_idxs=[],  # No categorical indices in our preprocessed data\n",
        "    cat_dims=[],  # No categorical dimensions\n",
        "    cat_emb_dim=[],\n",
        "    lambda_sparse=1e-4,\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=2e-2),\n",
        "    mask_type=\"entmax\",\n",
        "    scheduler_params=dict(\n",
        "        mode=\"min\",\n",
        "        patience=5,\n",
        "        min_lr=1e-5,\n",
        "        factor=0.9,\n",
        "    ),\n",
        "    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
        "    verbose=1,\n",
        "    device_name='cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")\n",
        "\n",
        "try:\n",
        "    # Train TabNet with error handling\n",
        "    print(\"\\nStarting TabNet training...\")\n",
        "    tabnet_model.fit(\n",
        "        X_train_tab, y_train_tab,\n",
        "        eval_set=[(X_test_tab, y_test_tab)],\n",
        "        max_epochs=10,\n",
        "        patience=10,\n",
        "        batch_size=1024,\n",
        "        virtual_batch_size=128\n",
        "    )\n",
        "\n",
        "    # Make predictions\n",
        "    print(\"\\nMaking predictions...\")\n",
        "    y_pred_tabnet = tabnet_model.predict(X_test_tab)\n",
        "    y_pred_proba_tabnet = tabnet_model.predict_proba(X_test_tab)[:, 1]\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics_tabnet = {\n",
        "        'accuracy': accuracy_score(y_test_tab, y_pred_tabnet),\n",
        "        'precision': precision_score(y_test_tab, y_pred_tabnet),\n",
        "        'recall': recall_score(y_test_tab, y_pred_tabnet),\n",
        "        'f1': f1_score(y_test_tab, y_pred_tabnet),\n",
        "        'auc': roc_auc_score(y_test_tab, y_pred_proba_tabnet),\n",
        "        'matthews_corr': matthews_corrcoef(y_test_tab, y_pred_tabnet)\n",
        "    }\n",
        "\n",
        "    # Calculate ROC curve\n",
        "    fpr_tabnet, tpr_tabnet, _ = roc_curve(y_test_tab, y_pred_proba_tabnet)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\nTabNet Performance Metrics:\")\n",
        "    for metric, value in metrics_tabnet.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    # Save ROC curve data\n",
        "    tabnet_roc = {\n",
        "        'fpr': fpr_tabnet,\n",
        "        'tpr': tpr_tabnet,\n",
        "        'auc': metrics_tabnet['auc']\n",
        "    }\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred: {str(e)}\")\n",
        "    print(\"\\nDebug information:\")\n",
        "    print(\"X_train_tab unique values:\", np.unique(X_train_tab))\n",
        "    print(\"y_train_tab unique values:\", np.unique(y_train_tab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrsdwrL3A0lY",
        "outputId": "1a4e5f90-4d20-4181-b105-401c6a733e49"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting TabNet training...\n",
            "epoch 0  | loss: 0.50332 | val_0_auc: 0.77315 |  0:00:09s\n",
            "epoch 1  | loss: 0.30626 | val_0_auc: 0.79722 |  0:00:19s\n",
            "epoch 2  | loss: 0.25764 | val_0_auc: 0.82711 |  0:00:30s\n",
            "epoch 3  | loss: 0.25087 | val_0_auc: 0.81428 |  0:00:40s\n",
            "epoch 4  | loss: 0.24288 | val_0_auc: 0.82244 |  0:00:50s\n",
            "epoch 5  | loss: 0.22325 | val_0_auc: 0.83404 |  0:01:00s\n",
            "epoch 6  | loss: 0.19628 | val_0_auc: 0.83454 |  0:01:11s\n",
            "epoch 7  | loss: 0.15717 | val_0_auc: 0.85791 |  0:01:21s\n",
            "epoch 8  | loss: 0.1465  | val_0_auc: 0.84498 |  0:01:31s\n",
            "epoch 9  | loss: 0.15832 | val_0_auc: 0.8435  |  0:01:41s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 7 and best_val_0_auc = 0.85791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Making predictions...\n",
            "\n",
            "TabNet Performance Metrics:\n",
            "accuracy: 0.9262\n",
            "precision: 0.1812\n",
            "recall: 0.4926\n",
            "f1: 0.2649\n",
            "auc: 0.8579\n",
            "matthews_corr: 0.2678\n"
          ]
        }
      ]
    }
  ]
}