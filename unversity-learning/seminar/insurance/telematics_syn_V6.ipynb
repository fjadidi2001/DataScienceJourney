{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fjadidi2001/Artificial_Intelligence_Learning/blob/master/Copy_of_telematics_syn_V5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbHLksJUkNON",
    "outputId": "889ee3f5-a2e1-400c-8bfd-10bda2dceec2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "(100000, 52)\n",
      "   Duration  Insured.age Insured.sex  Car.age  Marital  Car.use  Credit.score  \\\n",
      "0       366           45        Male       -1  Married  Commute         609.0   \n",
      "1       182           44      Female        3  Married  Commute         575.0   \n",
      "2       184           48      Female        6  Married  Commute         847.0   \n",
      "3       183           71        Male        6  Married  Private         842.0   \n",
      "4       183           84        Male       10  Married  Private         856.0   \n",
      "\n",
      "  Region  Annual.miles.drive  Years.noclaims  ...  Left.turn.intensity10  \\\n",
      "0  Urban             6213.71              25  ...                    1.0   \n",
      "1  Urban            12427.42              20  ...                   58.0   \n",
      "2  Urban            12427.42              14  ...                    0.0   \n",
      "3  Urban             6213.71              43  ...                    0.0   \n",
      "4  Urban             6213.71              65  ...                    2.0   \n",
      "\n",
      "   Left.turn.intensity11  Left.turn.intensity12  Right.turn.intensity08  \\\n",
      "0                    0.0                    0.0                     3.0   \n",
      "1                   24.0                   11.0                  1099.0   \n",
      "2                    0.0                    0.0                     0.0   \n",
      "3                    0.0                    0.0                     0.0   \n",
      "4                    0.0                    0.0                   325.0   \n",
      "\n",
      "   Right.turn.intensity09  Right.turn.intensity10  Right.turn.intensity11  \\\n",
      "0                     1.0                     0.0                     0.0   \n",
      "1                   615.0                   219.0                   101.0   \n",
      "2                     0.0                     0.0                     0.0   \n",
      "3                     0.0                     0.0                     0.0   \n",
      "4                   111.0                    18.0                     4.0   \n",
      "\n",
      "   Right.turn.intensity12  NB_Claim    AMT_Claim  \n",
      "0                     0.0         1  5100.171753  \n",
      "1                    40.0         1   883.554840  \n",
      "2                     0.0         0     0.000000  \n",
      "3                     0.0         0     0.000000  \n",
      "4                     2.0         0     0.000000  \n",
      "\n",
      "[5 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Specify file path\n",
    "file_path = '/content/drive/My Drive/telematics_syn.csv'\n",
    "\n",
    "# Import pandas (assuming you want to use it to read the CSV)\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.shape)  # Should print (100000, 52)\n",
    "print(df.head()) # To check the first few rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mphEoc3iOfOr"
   },
   "source": [
    "# pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_UNDVweYpQFO",
    "outputId": "2d39dcf9-a13d-4fbb-c4e8-74bce0a13661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "(100000, 52)\n",
      "Index(['Duration', 'Insured.age', 'Insured.sex', 'Car.age', 'Marital',\n",
      "       'Car.use', 'Credit.score', 'Region', 'Annual.miles.drive',\n",
      "       'Years.noclaims', 'Territory', 'Annual.pct.driven',\n",
      "       'Total.miles.driven', 'Pct.drive.mon', 'Pct.drive.tue', 'Pct.drive.wed',\n",
      "       'Pct.drive.thr', 'Pct.drive.fri', 'Pct.drive.sat', 'Pct.drive.sun',\n",
      "       'Pct.drive.2hrs', 'Pct.drive.3hrs', 'Pct.drive.4hrs', 'Pct.drive.wkday',\n",
      "       'Pct.drive.wkend', 'Pct.drive.rush am', 'Pct.drive.rush pm',\n",
      "       'Avgdays.week', 'Accel.06miles', 'Accel.08miles', 'Accel.09miles',\n",
      "       'Accel.11miles', 'Accel.12miles', 'Accel.14miles', 'Brake.06miles',\n",
      "       'Brake.08miles', 'Brake.09miles', 'Brake.11miles', 'Brake.12miles',\n",
      "       'Brake.14miles', 'Left.turn.intensity08', 'Left.turn.intensity09',\n",
      "       'Left.turn.intensity10', 'Left.turn.intensity11',\n",
      "       'Left.turn.intensity12', 'Right.turn.intensity08',\n",
      "       'Right.turn.intensity09', 'Right.turn.intensity10',\n",
      "       'Right.turn.intensity11', 'Right.turn.intensity12', 'NB_Claim',\n",
      "       'AMT_Claim'],\n",
      "      dtype='object')\n",
      "(100000, 113)\n",
      "(100000, 113)\n",
      "(100000, 114)\n",
      "(64000, 113) (16000, 113) (20000, 113)\n",
      "(64000, 113) (16000, 113) (20000, 113)\n",
      "(64000, 114) (16000, 114) (20000, 114)\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Specify file path\n",
    "file_path = '/content/drive/My Drive/telematics_syn.csv'\n",
    "\n",
    "# Import pandas (assuming you want to use it to read the CSV)\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.shape)  # Should print (100000, 52)\n",
    "print(df.columns) # To check the column names\n",
    "\n",
    "# Encoding categorical columns using one-hot encoding\n",
    "categorical_cols = ['Marital', 'Insured.sex', 'Car.use', 'Region', 'Territory']\n",
    "df_level1 = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "# Creating the ClaimYN variable\n",
    "df_level1['ClaimYN'] = df_level1['NB_Claim'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Save the preprocessed data to a new file\n",
    "preprocessed_file_path_level1 = '/content/drive/My Drive/pre_telematics_syn_level1.csv'\n",
    "df_level1.to_csv(preprocessed_file_path_level1, index=False)\n",
    "print(df_level1.shape)  # Should print (100000, number of columns after encoding)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Copy the DataFrame from Level 1\n",
    "df_level2 = df_level1.copy()\n",
    "\n",
    "# Feature columns (excluding response columns)\n",
    "feature_cols = [col for col in df_level2.columns if col not in ['NB_Claim', 'AMT_Claim', 'ClaimYN']]\n",
    "\n",
    "# Applying StandardScaler and MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "df_level2[feature_cols] = scaler.fit_transform(df_level2[feature_cols])\n",
    "df_level2[feature_cols] = minmax_scaler.fit_transform(df_level2[feature_cols])\n",
    "\n",
    "# Save the preprocessed data to a new file\n",
    "preprocessed_file_path_level2 = '/content/drive/My Drive/pre_telematics_syn_level2.csv'\n",
    "df_level2.to_csv(preprocessed_file_path_level2, index=False)\n",
    "print(df_level2.shape)  # Should print (100000, number of columns after scaling and normalization)\n",
    "\n",
    "# Copy the DataFrame from Level 2\n",
    "df_level3 = df_level2.copy()\n",
    "\n",
    "# Use provided columns for feature engineering\n",
    "feature_cols_to_sum = ['Annual.pct.driven', 'Annual.miles.drive', 'Pct.drive.rush am']\n",
    "\n",
    "# Advanced feature engineering steps\n",
    "df_level3['DrivingIntensity'] = df_level3[feature_cols_to_sum].sum(axis=1)\n",
    "\n",
    "# Save the fully preprocessed data to a new file\n",
    "preprocessed_file_path_level3 = '/content/drive/My Drive/pre_telematics_syn_level3.csv'\n",
    "df_level3.to_csv(preprocessed_file_path_level3, index=False)\n",
    "print(df_level3.shape)  # Should print (100000, number of columns after feature engineering)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the response and feature columns\n",
    "response_cols = ['NB_Claim', 'AMT_Claim', 'ClaimYN']\n",
    "\n",
    "\n",
    "def split_data(df):\n",
    "    feature_cols = [col for col in df.columns if col not in response_cols]\n",
    "\n",
    "    # Split into 80% train and 20% test\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Further split train into train and validation\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "    return train_df, val_df, test_df, feature_cols\n",
    "\n",
    "train_df_level1, val_df_level1, test_df_level1, feature_cols_level1 = split_data(df_level1)\n",
    "train_df_level2, val_df_level2, test_df_level2, feature_cols_level2 = split_data(df_level2)\n",
    "train_df_level3, val_df_level3, test_df_level3, feature_cols_level3 = split_data(df_level3)\n",
    "\n",
    "print(train_df_level1.shape, val_df_level1.shape, test_df_level1.shape)\n",
    "print(train_df_level2.shape, val_df_level2.shape, test_df_level2.shape)\n",
    "print(train_df_level3.shape, val_df_level3.shape, test_df_level3.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0PTXTvJRoa2",
    "outputId": "6d62e3ca-637d-43c6-c759-b89e58c60daa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-tabnet in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.25.2)\n",
      "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.2.2)\n",
      "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.11.4)\n",
      "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (2.3.1+cu121)\n",
      "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (4.66.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3->pytorch-tabnet) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->pytorch-tabnet) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the library\n",
    "!pip install pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "440_zDLWRjhA",
    "outputId": "4d927592-6115-443b-a774-14e60fef4821"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "X_train: (64000, 110), y_train: (64000,)\n",
      "X_val: (16000, 110), y_val: (16000,)\n",
      "Data types:\n",
      "X_train type: <class 'numpy.ndarray'>, y_train type: <class 'numpy.ndarray'>\n",
      "X_val type: <class 'numpy.ndarray'>, y_val type: <class 'numpy.ndarray'>\n",
      "Sample data:\n",
      "X_train sample: [[3.66000000e+02 6.40000000e+01 2.00000000e+00 6.95000000e+02\n",
      "  1.24274200e+04 8.00000000e+00 4.82191781e-01 3.25681330e+03\n",
      "  1.51102077e-01 1.67734049e-01 1.68922553e-01 1.75627304e-01\n",
      "  1.71069433e-01 1.31185908e-01 3.43586750e-02 2.19352600e-03\n",
      "  0.00000000e+00 0.00000000e+00 8.38064741e-01 1.61935259e-01\n",
      "  2.23870517e-01 2.12878450e-02 6.31935259e+00 1.10000000e+01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 3.40000000e+01 2.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.10000000e+01\n",
      "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  5.80000000e+01 1.70000000e+01 2.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [3.65000000e+02 4.00000000e+01 2.00000000e+00 8.63000000e+02\n",
      "  1.24274200e+04 2.20000000e+01 2.93150685e-01 1.52216078e+03\n",
      "  1.61433545e-01 2.23668297e-01 1.94913279e-01 1.36081388e-01\n",
      "  1.27065831e-01 6.02825760e-02 9.65550840e-02 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 8.47309517e-01 1.52690483e-01\n",
      "  3.31928550e-01 2.29402115e-01 5.83722206e+00 1.07000000e+02\n",
      "  4.00000000e+00 1.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 3.18000000e+02 3.00000000e+01 1.10000000e+01\n",
      "  4.00000000e+00 1.00000000e+00 1.00000000e+00 7.60000000e+01\n",
      "  3.70000000e+01 2.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.22000000e+02 4.30000000e+01 1.00000000e+01 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [3.66000000e+02 5.40000000e+01 5.00000000e+00 7.87000000e+02\n",
      "  6.21371000e+03 2.40000000e+01 9.04109589e-01 7.10045454e+03\n",
      "  1.56547235e-01 1.49720028e-01 1.37540583e-01 1.24756803e-01\n",
      "  1.45322931e-01 1.38513004e-01 1.47599416e-01 2.37388400e-03\n",
      "  1.24925600e-03 0.00000000e+00 7.16261158e-01 2.83738842e-01\n",
      "  7.37090880e-02 1.50000000e-01 6.21246281e+00 1.86000000e+02\n",
      "  1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 5.80000000e+01 1.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.07000000e+02\n",
      "  2.04000000e+02 4.30000000e+01 1.20000000e+01 3.00000000e+00\n",
      "  5.23000000e+02 2.36000000e+02 4.70000000e+01 1.50000000e+01\n",
      "  7.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [3.65000000e+02 4.50000000e+01 1.10000000e+01 5.62000000e+02\n",
      "  6.21371000e+03 1.20000000e+01 6.00000000e-01 1.00649719e+03\n",
      "  1.33933591e-01 1.06856154e-01 7.79606660e-02 2.46001783e-01\n",
      "  2.28300366e-01 1.79100010e-01 2.78474310e-02 2.77996500e-03\n",
      "  0.00000000e+00 0.00000000e+00 7.96332754e-01 2.03667246e-01\n",
      "  1.46689850e-02 2.37531363e-01 2.81538318e+00 5.00000000e+01\n",
      "  1.40000000e+01 6.00000000e+00 4.00000000e+00 2.00000000e+00\n",
      "  1.00000000e+00 7.30000000e+01 1.60000000e+01 7.00000000e+00\n",
      "  4.00000000e+00 4.00000000e+00 2.00000000e+00 4.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.10000000e+01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [3.65000000e+02 3.90000000e+01 9.00000000e+00 8.68000000e+02\n",
      "  5.59233900e+03 2.10000000e+01 6.65753425e-01 2.82347975e+03\n",
      "  1.21512271e-01 1.53928355e-01 1.73023920e-01 1.65669613e-01\n",
      "  1.44707674e-01 1.33125939e-01 1.08032229e-01 7.58538400e-03\n",
      "  1.58028800e-03 0.00000000e+00 7.60000000e-01 2.40000000e-01\n",
      "  7.15548080e-02 1.94197116e-01 5.38861540e+00 1.40000000e+01\n",
      "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 5.90000000e+01 4.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 2.13000000e+02\n",
      "  4.90000000e+01 5.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  5.82000000e+02 1.65000000e+02 1.90000000e+01 1.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "y_train sample: [0 0 0 0 0]\n",
      "X_val sample: [[ 3.65000000e+02  3.40000000e+01  4.00000000e+00  7.89000000e+02\n",
      "   7.45645200e+03  1.30000000e+01  5.23287671e-01  1.14759416e+04\n",
      "   1.32058537e-01  1.43728185e-01  1.52066370e-01  1.30492342e-01\n",
      "   1.65778693e-01  1.45362760e-01  1.30513113e-01  1.72065600e-03\n",
      "   0.00000000e+00  0.00000000e+00  7.27206559e-01  2.72793441e-01\n",
      "   1.79272144e-01  1.72511495e-01  6.67206559e+00  3.00000000e+01\n",
      "   2.00000000e+00  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  3.00000000e+01  2.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  0.00000000e+00  3.60000000e+01\n",
      "   1.40000000e+01  3.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   9.60000000e+01  2.90000000e+01  4.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 2.12000000e+02  5.40000000e+01  1.40000000e+01  8.49000000e+02\n",
      "   6.21371000e+03  3.80000000e+01  2.35616438e-01  1.31122997e+02\n",
      "   1.00086050e-01  2.39464479e-01  1.23006605e-01  2.80466681e-01\n",
      "   2.32915750e-01  2.40604360e-02 -8.34000000e-10  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  9.73348457e-01  2.66515430e-02\n",
      "   3.26764972e-01  2.43393829e-01  1.40000000e+00  7.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  5.20000000e+01  4.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  8.49000000e+02\n",
      "   3.19000000e+02  3.50000000e+01  5.00000000e+00  0.00000000e+00\n",
      "   1.17700000e+03  6.52000000e+02  2.81000000e+02  9.90000000e+01\n",
      "   3.50000000e+01  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 1.83000000e+02  7.30000000e+01  2.00000000e+00  8.49000000e+02\n",
      "   1.11846780e+04  5.70000000e+01  4.98630137e-01  7.71924570e+03\n",
      "   1.39007394e-01  1.19617491e-01  1.47182711e-01  1.60819588e-01\n",
      "   1.37971831e-01  1.79996341e-01  1.15404644e-01  1.50554960e-02\n",
      "   0.00000000e+00  0.00000000e+00  7.05833512e-01  2.94166488e-01\n",
      "   1.00000000e-02  1.45833512e-01  4.73889008e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  2.00000000e+01  2.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  2.50000000e+01\n",
      "   1.00000000e+01  2.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   1.60000000e+01  1.00000000e+01  2.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 3.65000000e+02  7.40000000e+01  1.10000000e+01  5.98000000e+02\n",
      "   6.21371000e+03  5.80000000e+01  1.64383562e-01  5.15040281e+02\n",
      "   2.15970544e-01  1.13488507e-01  1.23033338e-01  1.71054756e-01\n",
      "   2.16495747e-01  1.00312843e-01  5.96442660e-02  3.90336300e-03\n",
      "   0.00000000e+00  0.00000000e+00  8.35892912e-01  1.64107088e-01\n",
      "   3.90336260e-02  5.85504400e-02  5.68886703e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  1.40000000e+01  2.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  8.30000000e+01\n",
      "   4.40000000e+01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.21000000e+02  4.50000000e+01  2.20000000e+01  7.00000000e+00\n",
      "   7.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 3.65000000e+02  4.40000000e+01  7.00000000e+00  8.83000000e+02\n",
      "   1.24274200e+04  2.00000000e+00  1.00000000e+00  1.08518681e+04\n",
      "   1.62783879e-01  1.40086460e-01  1.39289343e-01  1.37837726e-01\n",
      "   1.52943024e-01  1.56192820e-01  1.10866747e-01  5.92557200e-03\n",
      "   2.09255700e-03  1.00000000e-03  7.31851144e-01  2.68148856e-01\n",
      "   6.55534320e-02  1.45372140e-01  6.23520996e+00  5.70000000e+01\n",
      "   9.00000000e+00  5.00000000e+00  2.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  8.30000000e+01  6.00000000e+00  2.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  2.31000000e+02\n",
      "   8.30000000e+01  1.70000000e+01  6.00000000e+00  1.00000000e+00\n",
      "   2.81000000e+02  1.03000000e+02  1.40000000e+01  4.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n",
      "y_val sample: [0 0 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.6311  | train_accuracy: 0.80648 | val_accuracy: 0.80731 |  0:00:12s\n",
      "epoch 1  | loss: 0.57773 | train_accuracy: 0.65441 | val_accuracy: 0.6605  |  0:00:25s\n",
      "epoch 2  | loss: 0.57379 | train_accuracy: 0.69975 | val_accuracy: 0.70388 |  0:00:37s\n",
      "epoch 3  | loss: 0.57232 | train_accuracy: 0.64903 | val_accuracy: 0.65175 |  0:00:48s\n",
      "epoch 4  | loss: 0.57012 | train_accuracy: 0.66997 | val_accuracy: 0.6735  |  0:01:00s\n",
      "epoch 5  | loss: 0.5668  | train_accuracy: 0.6293  | val_accuracy: 0.6325  |  0:01:13s\n",
      "epoch 6  | loss: 0.5678  | train_accuracy: 0.70294 | val_accuracy: 0.70219 |  0:01:25s\n",
      "epoch 7  | loss: 0.56312 | train_accuracy: 0.65875 | val_accuracy: 0.66094 |  0:01:36s\n",
      "epoch 8  | loss: 0.55687 | train_accuracy: 0.66803 | val_accuracy: 0.66856 |  0:01:47s\n",
      "epoch 9  | loss: 0.55789 | train_accuracy: 0.68745 | val_accuracy: 0.69056 |  0:01:59s\n",
      "epoch 10 | loss: 0.55493 | train_accuracy: 0.66344 | val_accuracy: 0.65981 |  0:02:11s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_accuracy = 0.80731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "X_train: (64000, 110), y_train: (64000,)\n",
      "X_val: (16000, 110), y_val: (16000,)\n",
      "Data types:\n",
      "X_train type: <class 'numpy.ndarray'>, y_train type: <class 'numpy.ndarray'>\n",
      "X_val type: <class 'numpy.ndarray'>, y_val type: <class 'numpy.ndarray'>\n",
      "Sample data:\n",
      "X_train sample: [[1.00000000e+00 5.51724138e-01 1.81818182e-01 5.71129707e-01\n",
      "  2.19058050e-01 1.01265823e-01 4.80769231e-01 6.88778598e-02\n",
      "  1.51378842e-01 1.67734049e-01 1.68922553e-01 1.75996906e-01\n",
      "  1.71306277e-01 1.38587015e-01 3.52012753e-02 4.81308685e-03\n",
      "  0.00000000e+00 0.00000000e+00 8.38064741e-01 1.61935259e-01\n",
      "  2.26579899e-01 2.14340683e-02 8.99891509e-01 1.77133655e-02\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 5.47504026e-02 3.22061192e-03 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.38410046e-05\n",
      "  1.25837448e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  6.89483007e-05 2.02090567e-05 2.37755587e-06 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [9.97050147e-01 2.75862069e-01 1.81818182e-01 9.22594142e-01\n",
      "  2.19058050e-01 2.78481013e-01 2.91208791e-01 3.21908784e-02\n",
      "  1.61729234e-01 2.23668297e-01 1.94913279e-01 1.36367767e-01\n",
      "  1.27241753e-01 6.36835341e-02 9.89229641e-02 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 8.47309517e-01 1.52690483e-01\n",
      "  3.35945700e-01 2.30977847e-01 8.28980552e-01 1.72302738e-01\n",
      "  6.44122383e-03 1.61030596e-03 1.61030596e-03 0.00000000e+00\n",
      "  0.00000000e+00 5.12077295e-01 4.83091787e-02 1.77133655e-02\n",
      "  6.44122383e-03 1.61030596e-03 1.61030596e-03 9.56287591e-05\n",
      "  4.65598558e-05 2.51768675e-06 0.00000000e+00 0.00000000e+00\n",
      "  1.45029184e-04 5.11170259e-05 1.18877794e-05 1.18881185e-06\n",
      "  1.18885708e-06 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 4.36781609e-01 3.18181818e-01 7.63598326e-01\n",
      "  1.09529025e-01 3.03797468e-01 9.03846154e-01 1.50168835e-01\n",
      "  1.56833974e-01 1.49720028e-01 1.37540583e-01 1.25019350e-01\n",
      "  1.45524129e-01 1.46327483e-01 1.51219087e-01 5.20883266e-03\n",
      "  3.85773525e-03 0.00000000e+00 7.16261158e-01 2.83738842e-01\n",
      "  7.46011488e-02 1.51030330e-01 8.84170338e-01 2.99516908e-01\n",
      "  1.61030596e-03 1.61030596e-03 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 9.33977456e-02 1.61030596e-03 1.61030596e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 6.37944485e-04\n",
      "  2.56708394e-04 5.41302651e-05 1.51147588e-05 3.78229131e-06\n",
      "  6.21723470e-04 2.80549258e-04 5.58725630e-05 1.78321778e-05\n",
      "  8.32199956e-06 0.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [9.97050147e-01 3.33333333e-01 5.90909091e-01 2.92887029e-01\n",
      "  1.09529025e-01 1.51898734e-01 5.98901099e-01 2.12848667e-02\n",
      "  1.34178910e-01 1.06856154e-01 7.79606660e-02 2.46519486e-01\n",
      "  2.28616446e-01 1.89204283e-01 2.85303522e-02 6.09986523e-03\n",
      "  0.00000000e+00 0.00000000e+00 7.96332754e-01 2.03667246e-01\n",
      "  1.48465157e-02 2.39162934e-01 3.84533512e-01 8.05152979e-02\n",
      "  2.25442834e-02 9.66183575e-03 6.44122383e-03 3.22061192e-03\n",
      "  1.61030596e-03 1.17552335e-01 2.57648953e-02 1.12721417e-02\n",
      "  6.44122383e-03 6.44122383e-03 3.22061192e-03 5.03309258e-06\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.30764018e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [9.97050147e-01 2.64367816e-01 5.00000000e-01 9.33054393e-01\n",
      "  9.85761227e-02 2.65822785e-01 6.64835165e-01 5.97130850e-02\n",
      "  1.21734838e-01 1.53928355e-01 1.73023920e-01 1.66018259e-01\n",
      "  1.44908021e-01 1.40636496e-01 1.10681570e-01 1.66440297e-02\n",
      "  4.87997074e-03 0.00000000e+00 7.60000000e-01 2.40000000e-01\n",
      "  7.24207967e-02 1.95531030e-01 7.63000231e-01 2.25442834e-02\n",
      "  1.61030596e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 9.50080515e-02 6.44122383e-03 1.61030596e-03\n",
      "  1.61030596e-03 1.61030596e-03 1.61030596e-03 2.68012180e-04\n",
      "  6.16603496e-05 6.29421687e-06 1.25956323e-06 0.00000000e+00\n",
      "  6.91860534e-04 1.96146727e-04 2.25867808e-05 1.18881185e-06\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "y_train sample: [0 0 0 0 0]\n",
      "X_val sample: [[9.97050147e-01 2.06896552e-01 2.72727273e-01 7.67782427e-01\n",
      "  1.31434830e-01 1.64556962e-01 5.21978022e-01 2.42708067e-01\n",
      "  1.32300421e-01 1.43728185e-01 1.52066370e-01 1.30766959e-01\n",
      "  1.66008212e-01 1.53563681e-01 1.33713766e-01 3.77550426e-03\n",
      "  0.00000000e+00 0.00000000e+00 7.27206559e-01 2.72793441e-01\n",
      "  1.81441777e-01 1.73696453e-01 9.51767951e-01 4.83091787e-02\n",
      "  3.22061192e-03 1.61030596e-03 1.61030596e-03 0.00000000e+00\n",
      "  0.00000000e+00 4.83091787e-02 3.22061192e-03 1.61030596e-03\n",
      "  1.61030596e-03 1.61030596e-03 0.00000000e+00 4.52978333e-05\n",
      "  1.76172428e-05 3.77653012e-06 1.25956323e-06 1.26076377e-06\n",
      "  1.14121325e-04 3.44742733e-05 4.75511175e-06 1.18881185e-06\n",
      "  1.18885708e-06 0.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [5.45722714e-01 4.36781609e-01 7.27272727e-01 8.93305439e-01\n",
      "  1.09529025e-01 4.81012658e-01 2.33516483e-01 2.77116639e-03\n",
      "  1.00269372e-01 2.39464479e-01 1.23006605e-01 2.81056914e-01\n",
      "  2.33238220e-01 2.54178520e-02 1.07165168e-09 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 9.73348457e-01 2.66515430e-02\n",
      "  3.30719630e-01 2.45065668e-01 1.76361306e-01 1.12721417e-02\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 8.37359098e-02 6.44122383e-03 1.61030596e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.06827390e-03\n",
      "  4.01421460e-04 4.40595181e-05 6.29781617e-06 0.00000000e+00\n",
      "  1.39917500e-03 7.75076765e-04 3.34046600e-04 1.17692374e-04\n",
      "  4.16099978e-05 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [4.60176991e-01 6.55172414e-01 1.81818182e-01 8.93305439e-01\n",
      "  1.97152245e-01 7.21518987e-01 4.97252747e-01 1.63255940e-01\n",
      "  1.39262006e-01 1.19617491e-01 1.47182711e-01 1.61158028e-01\n",
      "  1.38162852e-01 1.90151183e-01 1.18234783e-01 3.30351269e-02\n",
      "  0.00000000e+00 0.00000000e+00 7.05833512e-01 2.94166488e-01\n",
      "  1.01210245e-02 1.46835223e-01 6.67439710e-01 1.61030596e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 3.22061192e-02 3.22061192e-03 1.61030596e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.14568286e-05\n",
      "  1.25837448e-05 2.51768675e-06 1.25956323e-06 0.00000000e+00\n",
      "  1.90202209e-05 1.18876804e-05 2.37755587e-06 1.18881185e-06\n",
      "  1.18885708e-06 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [9.97050147e-01 6.66666667e-01 5.90909091e-01 3.68200837e-01\n",
      "  1.09529025e-01 7.34177215e-01 1.62087912e-01 1.08908135e-02\n",
      "  2.16366125e-01 1.13488507e-01 1.23033338e-01 1.71414735e-01\n",
      "  2.16795484e-01 1.05972186e-01 6.11069600e-02 8.56485181e-03\n",
      "  0.00000000e+00 0.00000000e+00 8.35892912e-01 1.64107088e-01\n",
      "  3.95060286e-02 5.89526150e-02 8.07160742e-01 1.61030596e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 2.25442834e-02 3.22061192e-03 1.61030596e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.04436671e-04\n",
      "  5.53684772e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.43840420e-04 5.34945620e-05 2.61531146e-05 8.32168298e-06\n",
      "  8.32199956e-06 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [9.97050147e-01 3.21839080e-01 4.09090909e-01 9.64435146e-01\n",
      "  2.19058050e-01 2.53164557e-02 1.00000000e+00 2.29509244e-01\n",
      "  1.63082041e-01 1.40086460e-01 1.39289343e-01 1.38127801e-01\n",
      "  1.53154772e-01 1.65004740e-01 1.13585601e-01 1.30020308e-02\n",
      "  6.46187083e-03 3.76099569e-03 7.31851144e-01 2.68148856e-01\n",
      "  6.63467893e-02 1.46370682e-01 8.87515952e-01 9.17874396e-02\n",
      "  1.44927536e-02 8.05152979e-03 3.22061192e-03 1.61030596e-03\n",
      "  1.61030596e-03 1.33655395e-01 9.66183575e-03 3.22061192e-03\n",
      "  1.61030596e-03 1.61030596e-03 1.61030596e-03 2.90661097e-04\n",
      "  1.04445082e-04 2.14003374e-05 7.55737940e-06 1.26076377e-06\n",
      "  3.34042629e-04 1.22443109e-04 1.66428911e-05 4.75524742e-06\n",
      "  1.18885708e-06 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "y_val sample: [0 0 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.64125 | train_accuracy: 0.19758 | val_accuracy: 0.20319 |  0:00:11s\n",
      "epoch 1  | loss: 0.57605 | train_accuracy: 0.13955 | val_accuracy: 0.14269 |  0:00:22s\n",
      "epoch 2  | loss: 0.56285 | train_accuracy: 0.22433 | val_accuracy: 0.22725 |  0:00:34s\n",
      "epoch 3  | loss: 0.55817 | train_accuracy: 0.3637  | val_accuracy: 0.36531 |  0:00:46s\n",
      "epoch 4  | loss: 0.55079 | train_accuracy: 0.57991 | val_accuracy: 0.57969 |  0:00:59s\n",
      "epoch 5  | loss: 0.54498 | train_accuracy: 0.52409 | val_accuracy: 0.525   |  0:01:11s\n",
      "epoch 6  | loss: 0.5384  | train_accuracy: 0.62292 | val_accuracy: 0.624   |  0:01:22s\n",
      "epoch 7  | loss: 0.53232 | train_accuracy: 0.63564 | val_accuracy: 0.63525 |  0:01:34s\n",
      "epoch 8  | loss: 0.53003 | train_accuracy: 0.63997 | val_accuracy: 0.63694 |  0:01:46s\n",
      "epoch 9  | loss: 0.52534 | train_accuracy: 0.67819 | val_accuracy: 0.67494 |  0:01:59s\n",
      "epoch 10 | loss: 0.52049 | train_accuracy: 0.67628 | val_accuracy: 0.67094 |  0:02:13s\n",
      "epoch 11 | loss: 0.51506 | train_accuracy: 0.70188 | val_accuracy: 0.70025 |  0:02:25s\n",
      "epoch 12 | loss: 0.5142  | train_accuracy: 0.66061 | val_accuracy: 0.65481 |  0:02:37s\n",
      "epoch 13 | loss: 0.51686 | train_accuracy: 0.65788 | val_accuracy: 0.6565  |  0:02:49s\n",
      "epoch 14 | loss: 0.52363 | train_accuracy: 0.67878 | val_accuracy: 0.67662 |  0:03:01s\n",
      "epoch 15 | loss: 0.5072  | train_accuracy: 0.67895 | val_accuracy: 0.67194 |  0:03:13s\n",
      "epoch 16 | loss: 0.50394 | train_accuracy: 0.65642 | val_accuracy: 0.64719 |  0:03:25s\n",
      "epoch 17 | loss: 0.50038 | train_accuracy: 0.69506 | val_accuracy: 0.69244 |  0:03:37s\n",
      "epoch 18 | loss: 0.50654 | train_accuracy: 0.68081 | val_accuracy: 0.67738 |  0:03:48s\n",
      "epoch 19 | loss: 0.50118 | train_accuracy: 0.72308 | val_accuracy: 0.71931 |  0:04:00s\n",
      "epoch 20 | loss: 0.49038 | train_accuracy: 0.67264 | val_accuracy: 0.66644 |  0:04:12s\n",
      "epoch 21 | loss: 0.48418 | train_accuracy: 0.70406 | val_accuracy: 0.69669 |  0:04:24s\n",
      "epoch 22 | loss: 0.47234 | train_accuracy: 0.6938  | val_accuracy: 0.68756 |  0:04:36s\n",
      "epoch 23 | loss: 0.47083 | train_accuracy: 0.73272 | val_accuracy: 0.72406 |  0:04:47s\n",
      "epoch 24 | loss: 0.46314 | train_accuracy: 0.75612 | val_accuracy: 0.74712 |  0:04:59s\n",
      "epoch 25 | loss: 0.46512 | train_accuracy: 0.72127 | val_accuracy: 0.71075 |  0:05:11s\n",
      "epoch 26 | loss: 0.45999 | train_accuracy: 0.70141 | val_accuracy: 0.6915  |  0:05:23s\n",
      "epoch 27 | loss: 0.44355 | train_accuracy: 0.75623 | val_accuracy: 0.74356 |  0:05:34s\n",
      "epoch 28 | loss: 0.4361  | train_accuracy: 0.75797 | val_accuracy: 0.74856 |  0:05:45s\n",
      "epoch 29 | loss: 0.4333  | train_accuracy: 0.74797 | val_accuracy: 0.74069 |  0:05:57s\n",
      "epoch 30 | loss: 0.4385  | train_accuracy: 0.74477 | val_accuracy: 0.72894 |  0:06:10s\n",
      "epoch 31 | loss: 0.42559 | train_accuracy: 0.7602  | val_accuracy: 0.74688 |  0:06:23s\n",
      "epoch 32 | loss: 0.41916 | train_accuracy: 0.76166 | val_accuracy: 0.75019 |  0:06:35s\n",
      "epoch 33 | loss: 0.40875 | train_accuracy: 0.75847 | val_accuracy: 0.74762 |  0:06:46s\n",
      "epoch 34 | loss: 0.41076 | train_accuracy: 0.75633 | val_accuracy: 0.73869 |  0:06:57s\n",
      "epoch 35 | loss: 0.40138 | train_accuracy: 0.78123 | val_accuracy: 0.76331 |  0:07:09s\n",
      "epoch 36 | loss: 0.40295 | train_accuracy: 0.75889 | val_accuracy: 0.74388 |  0:07:21s\n",
      "epoch 37 | loss: 0.4033  | train_accuracy: 0.76306 | val_accuracy: 0.74719 |  0:07:34s\n",
      "epoch 38 | loss: 0.39102 | train_accuracy: 0.76992 | val_accuracy: 0.75381 |  0:07:45s\n",
      "epoch 39 | loss: 0.39691 | train_accuracy: 0.76414 | val_accuracy: 0.74581 |  0:07:56s\n",
      "epoch 40 | loss: 0.38429 | train_accuracy: 0.78936 | val_accuracy: 0.76838 |  0:08:08s\n",
      "epoch 41 | loss: 0.3738  | train_accuracy: 0.79912 | val_accuracy: 0.77825 |  0:08:20s\n",
      "epoch 42 | loss: 0.3769  | train_accuracy: 0.79811 | val_accuracy: 0.77938 |  0:08:32s\n",
      "epoch 43 | loss: 0.36951 | train_accuracy: 0.79217 | val_accuracy: 0.77012 |  0:08:45s\n",
      "epoch 44 | loss: 0.37047 | train_accuracy: 0.79434 | val_accuracy: 0.77625 |  0:08:56s\n",
      "epoch 45 | loss: 0.36942 | train_accuracy: 0.78239 | val_accuracy: 0.76288 |  0:09:07s\n",
      "epoch 46 | loss: 0.35507 | train_accuracy: 0.8082  | val_accuracy: 0.78419 |  0:09:19s\n",
      "epoch 47 | loss: 0.35537 | train_accuracy: 0.79478 | val_accuracy: 0.76731 |  0:09:32s\n",
      "epoch 48 | loss: 0.36304 | train_accuracy: 0.79503 | val_accuracy: 0.771   |  0:09:44s\n",
      "epoch 49 | loss: 0.341   | train_accuracy: 0.80708 | val_accuracy: 0.77856 |  0:09:55s\n",
      "Stop training because you reached max_epochs = 50 with best_epoch = 46 and best_val_accuracy = 0.78419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "X_train: (64000, 111), y_train: (64000,)\n",
      "X_val: (16000, 111), y_val: (16000,)\n",
      "Data types:\n",
      "X_train type: <class 'numpy.ndarray'>, y_train type: <class 'numpy.ndarray'>\n",
      "X_val type: <class 'numpy.ndarray'>, y_val type: <class 'numpy.ndarray'>\n",
      "Sample data:\n",
      "X_train sample: [[1.00000000e+00 5.51724138e-01 1.81818182e-01 5.71129707e-01\n",
      "  2.19058050e-01 1.01265823e-01 4.80769231e-01 6.88778598e-02\n",
      "  1.51378842e-01 1.67734049e-01 1.68922553e-01 1.75996906e-01\n",
      "  1.71306277e-01 1.38587015e-01 3.52012753e-02 4.81308685e-03\n",
      "  0.00000000e+00 0.00000000e+00 8.38064741e-01 1.61935259e-01\n",
      "  2.26579899e-01 2.14340683e-02 8.99891509e-01 1.77133655e-02\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 5.47504026e-02 3.22061192e-03 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.38410046e-05\n",
      "  1.25837448e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  6.89483007e-05 2.02090567e-05 2.37755587e-06 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 9.26407181e-01]\n",
      " [9.97050147e-01 2.75862069e-01 1.81818182e-01 9.22594142e-01\n",
      "  2.19058050e-01 2.78481013e-01 2.91208791e-01 3.21908784e-02\n",
      "  1.61729234e-01 2.23668297e-01 1.94913279e-01 1.36367767e-01\n",
      "  1.27241753e-01 6.36835341e-02 9.89229641e-02 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 8.47309517e-01 1.52690483e-01\n",
      "  3.35945700e-01 2.30977847e-01 8.28980552e-01 1.72302738e-01\n",
      "  6.44122383e-03 1.61030596e-03 1.61030596e-03 0.00000000e+00\n",
      "  0.00000000e+00 5.12077295e-01 4.83091787e-02 1.77133655e-02\n",
      "  6.44122383e-03 1.61030596e-03 1.61030596e-03 9.56287591e-05\n",
      "  4.65598558e-05 2.51768675e-06 0.00000000e+00 0.00000000e+00\n",
      "  1.45029184e-04 5.11170259e-05 1.18877794e-05 1.18881185e-06\n",
      "  1.18885708e-06 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 8.46212541e-01]\n",
      " [1.00000000e+00 4.36781609e-01 3.18181818e-01 7.63598326e-01\n",
      "  1.09529025e-01 3.03797468e-01 9.03846154e-01 1.50168835e-01\n",
      "  1.56833974e-01 1.49720028e-01 1.37540583e-01 1.25019350e-01\n",
      "  1.45524129e-01 1.46327483e-01 1.51219087e-01 5.20883266e-03\n",
      "  3.85773525e-03 0.00000000e+00 7.16261158e-01 2.83738842e-01\n",
      "  7.46011488e-02 1.51030330e-01 8.84170338e-01 2.99516908e-01\n",
      "  1.61030596e-03 1.61030596e-03 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 9.33977456e-02 1.61030596e-03 1.61030596e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 6.37944485e-04\n",
      "  2.56708394e-04 5.41302651e-05 1.51147588e-05 3.78229131e-06\n",
      "  6.21723470e-04 2.80549258e-04 5.58725630e-05 1.78321778e-05\n",
      "  8.32199956e-06 0.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.08797633e+00]\n",
      " [9.97050147e-01 3.33333333e-01 5.90909091e-01 2.92887029e-01\n",
      "  1.09529025e-01 1.51898734e-01 5.98901099e-01 2.12848667e-02\n",
      "  1.34178910e-01 1.06856154e-01 7.79606660e-02 2.46519486e-01\n",
      "  2.28616446e-01 1.89204283e-01 2.85303522e-02 6.09986523e-03\n",
      "  0.00000000e+00 0.00000000e+00 7.96332754e-01 2.03667246e-01\n",
      "  1.48465157e-02 2.39162934e-01 3.84533512e-01 8.05152979e-02\n",
      "  2.25442834e-02 9.66183575e-03 6.44122383e-03 3.22061192e-03\n",
      "  1.61030596e-03 1.17552335e-01 2.57648953e-02 1.12721417e-02\n",
      "  6.44122383e-03 6.44122383e-03 3.22061192e-03 5.03309258e-06\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.30764018e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 7.23276640e-01]\n",
      " [9.97050147e-01 2.64367816e-01 5.00000000e-01 9.33054393e-01\n",
      "  9.85761227e-02 2.65822785e-01 6.64835165e-01 5.97130850e-02\n",
      "  1.21734838e-01 1.53928355e-01 1.73023920e-01 1.66018259e-01\n",
      "  1.44908021e-01 1.40636496e-01 1.10681570e-01 1.66440297e-02\n",
      "  4.87997074e-03 0.00000000e+00 7.60000000e-01 2.40000000e-01\n",
      "  7.24207967e-02 1.95531030e-01 7.63000231e-01 2.25442834e-02\n",
      "  1.61030596e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 9.50080515e-02 6.44122383e-03 1.61030596e-03\n",
      "  1.61030596e-03 1.61030596e-03 1.61030596e-03 2.68012180e-04\n",
      "  6.16603496e-05 6.29421687e-06 1.25956323e-06 0.00000000e+00\n",
      "  6.91860534e-04 1.96146727e-04 2.25867808e-05 1.18881185e-06\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 8.35832085e-01]]\n",
      "y_train sample: [0 0 0 0 0]\n",
      "X_val sample: [[9.97050147e-01 2.06896552e-01 2.72727273e-01 7.67782427e-01\n",
      "  1.31434830e-01 1.64556962e-01 5.21978022e-01 2.42708067e-01\n",
      "  1.32300421e-01 1.43728185e-01 1.52066370e-01 1.30766959e-01\n",
      "  1.66008212e-01 1.53563681e-01 1.33713766e-01 3.77550426e-03\n",
      "  0.00000000e+00 0.00000000e+00 7.27206559e-01 2.72793441e-01\n",
      "  1.81441777e-01 1.73696453e-01 9.51767951e-01 4.83091787e-02\n",
      "  3.22061192e-03 1.61030596e-03 1.61030596e-03 0.00000000e+00\n",
      "  0.00000000e+00 4.83091787e-02 3.22061192e-03 1.61030596e-03\n",
      "  1.61030596e-03 1.61030596e-03 0.00000000e+00 4.52978333e-05\n",
      "  1.76172428e-05 3.77653012e-06 1.25956323e-06 1.26076377e-06\n",
      "  1.14121325e-04 3.44742733e-05 4.75511175e-06 1.18881185e-06\n",
      "  1.18885708e-06 0.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 8.34854629e-01]\n",
      " [5.45722714e-01 4.36781609e-01 7.27272727e-01 8.93305439e-01\n",
      "  1.09529025e-01 4.81012658e-01 2.33516483e-01 2.77116639e-03\n",
      "  1.00269372e-01 2.39464479e-01 1.23006605e-01 2.81056914e-01\n",
      "  2.33238220e-01 2.54178520e-02 1.07165168e-09 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 9.73348457e-01 2.66515430e-02\n",
      "  3.30719630e-01 2.45065668e-01 1.76361306e-01 1.12721417e-02\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 8.37359098e-02 6.44122383e-03 1.61030596e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.06827390e-03\n",
      "  4.01421460e-04 4.40595181e-05 6.29781617e-06 0.00000000e+00\n",
      "  1.39917500e-03 7.75076765e-04 3.34046600e-04 1.17692374e-04\n",
      "  4.16099978e-05 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 6.73765138e-01]\n",
      " [4.60176991e-01 6.55172414e-01 1.81818182e-01 8.93305439e-01\n",
      "  1.97152245e-01 7.21518987e-01 4.97252747e-01 1.63255940e-01\n",
      "  1.39262006e-01 1.19617491e-01 1.47182711e-01 1.61158028e-01\n",
      "  1.38162852e-01 1.90151183e-01 1.18234783e-01 3.30351269e-02\n",
      "  0.00000000e+00 0.00000000e+00 7.05833512e-01 2.94166488e-01\n",
      "  1.01210245e-02 1.46835223e-01 6.67439710e-01 1.61030596e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 3.22061192e-02 3.22061192e-03 1.61030596e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.14568286e-05\n",
      "  1.25837448e-05 2.51768675e-06 1.25956323e-06 0.00000000e+00\n",
      "  1.90202209e-05 1.18876804e-05 2.37755587e-06 1.18881185e-06\n",
      "  1.18885708e-06 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 7.04526017e-01]\n",
      " [9.97050147e-01 6.66666667e-01 5.90909091e-01 3.68200837e-01\n",
      "  1.09529025e-01 7.34177215e-01 1.62087912e-01 1.08908135e-02\n",
      "  2.16366125e-01 1.13488507e-01 1.23033338e-01 1.71414735e-01\n",
      "  2.16795484e-01 1.05972186e-01 6.11069600e-02 8.56485181e-03\n",
      "  0.00000000e+00 0.00000000e+00 8.35892912e-01 1.64107088e-01\n",
      "  3.95060286e-02 5.89526150e-02 8.07160742e-01 1.61030596e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 2.25442834e-02 3.22061192e-03 1.61030596e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.04436671e-04\n",
      "  5.53684772e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.43840420e-04 5.34945620e-05 2.61531146e-05 8.32168298e-06\n",
      "  8.32199956e-06 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 3.11122966e-01]\n",
      " [9.97050147e-01 3.21839080e-01 4.09090909e-01 9.64435146e-01\n",
      "  2.19058050e-01 2.53164557e-02 1.00000000e+00 2.29509244e-01\n",
      "  1.63082041e-01 1.40086460e-01 1.39289343e-01 1.38127801e-01\n",
      "  1.53154772e-01 1.65004740e-01 1.13585601e-01 1.30020308e-02\n",
      "  6.46187083e-03 3.76099569e-03 7.31851144e-01 2.68148856e-01\n",
      "  6.63467893e-02 1.46370682e-01 8.87515952e-01 9.17874396e-02\n",
      "  1.44927536e-02 8.05152979e-03 3.22061192e-03 1.61030596e-03\n",
      "  1.61030596e-03 1.33655395e-01 9.66183575e-03 3.22061192e-03\n",
      "  1.61030596e-03 1.61030596e-03 1.61030596e-03 2.90661097e-04\n",
      "  1.04445082e-04 2.14003374e-05 7.55737940e-06 1.26076377e-06\n",
      "  3.34042629e-04 1.22443109e-04 1.66428911e-05 4.75524742e-06\n",
      "  1.18885708e-06 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.28540484e+00]]\n",
      "y_val sample: [0 0 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.67298 | train_accuracy: 0.04625 | val_accuracy: 0.04844 |  0:00:12s\n",
      "epoch 1  | loss: 0.5835  | train_accuracy: 0.20473 | val_accuracy: 0.20606 |  0:00:24s\n",
      "epoch 2  | loss: 0.57073 | train_accuracy: 0.28394 | val_accuracy: 0.28338 |  0:00:36s\n",
      "epoch 3  | loss: 0.56892 | train_accuracy: 0.21244 | val_accuracy: 0.21612 |  0:00:48s\n",
      "epoch 4  | loss: 0.5627  | train_accuracy: 0.48192 | val_accuracy: 0.48144 |  0:01:00s\n",
      "epoch 5  | loss: 0.56309 | train_accuracy: 0.5188  | val_accuracy: 0.51944 |  0:01:12s\n",
      "epoch 6  | loss: 0.55898 | train_accuracy: 0.61725 | val_accuracy: 0.61669 |  0:01:23s\n",
      "epoch 7  | loss: 0.55041 | train_accuracy: 0.61731 | val_accuracy: 0.62031 |  0:01:35s\n",
      "epoch 8  | loss: 0.54303 | train_accuracy: 0.64264 | val_accuracy: 0.64281 |  0:01:48s\n",
      "epoch 9  | loss: 0.54164 | train_accuracy: 0.65517 | val_accuracy: 0.65556 |  0:02:00s\n",
      "epoch 10 | loss: 0.54387 | train_accuracy: 0.65323 | val_accuracy: 0.65294 |  0:02:12s\n",
      "epoch 11 | loss: 0.53634 | train_accuracy: 0.68334 | val_accuracy: 0.68188 |  0:02:23s\n",
      "epoch 12 | loss: 0.53655 | train_accuracy: 0.64725 | val_accuracy: 0.64438 |  0:02:34s\n",
      "epoch 13 | loss: 0.52794 | train_accuracy: 0.6717  | val_accuracy: 0.66894 |  0:02:47s\n",
      "epoch 14 | loss: 0.52925 | train_accuracy: 0.69192 | val_accuracy: 0.68788 |  0:03:00s\n",
      "epoch 15 | loss: 0.52316 | train_accuracy: 0.66906 | val_accuracy: 0.66819 |  0:03:12s\n",
      "epoch 16 | loss: 0.51347 | train_accuracy: 0.65453 | val_accuracy: 0.65088 |  0:03:24s\n",
      "epoch 17 | loss: 0.50855 | train_accuracy: 0.72544 | val_accuracy: 0.71919 |  0:03:36s\n",
      "epoch 18 | loss: 0.50681 | train_accuracy: 0.67134 | val_accuracy: 0.67    |  0:03:47s\n",
      "epoch 19 | loss: 0.50057 | train_accuracy: 0.7412  | val_accuracy: 0.73475 |  0:04:00s\n",
      "epoch 20 | loss: 0.49801 | train_accuracy: 0.70861 | val_accuracy: 0.70406 |  0:04:14s\n",
      "epoch 21 | loss: 0.49296 | train_accuracy: 0.69614 | val_accuracy: 0.68931 |  0:04:26s\n",
      "epoch 22 | loss: 0.48822 | train_accuracy: 0.70211 | val_accuracy: 0.69694 |  0:04:37s\n",
      "epoch 23 | loss: 0.48256 | train_accuracy: 0.71238 | val_accuracy: 0.7025  |  0:04:49s\n",
      "epoch 24 | loss: 0.4731  | train_accuracy: 0.75294 | val_accuracy: 0.74319 |  0:05:01s\n",
      "epoch 25 | loss: 0.46991 | train_accuracy: 0.73752 | val_accuracy: 0.72894 |  0:05:13s\n",
      "epoch 26 | loss: 0.46528 | train_accuracy: 0.69264 | val_accuracy: 0.68325 |  0:05:25s\n",
      "epoch 27 | loss: 0.45865 | train_accuracy: 0.74877 | val_accuracy: 0.73694 |  0:05:37s\n",
      "epoch 28 | loss: 0.44979 | train_accuracy: 0.76697 | val_accuracy: 0.754   |  0:05:49s\n",
      "epoch 29 | loss: 0.44509 | train_accuracy: 0.78133 | val_accuracy: 0.76731 |  0:06:01s\n",
      "epoch 30 | loss: 0.44051 | train_accuracy: 0.7133  | val_accuracy: 0.7005  |  0:06:13s\n",
      "epoch 31 | loss: 0.43312 | train_accuracy: 0.75464 | val_accuracy: 0.73938 |  0:06:26s\n",
      "epoch 32 | loss: 0.43337 | train_accuracy: 0.76461 | val_accuracy: 0.75144 |  0:06:39s\n",
      "epoch 33 | loss: 0.43059 | train_accuracy: 0.76592 | val_accuracy: 0.74994 |  0:06:51s\n",
      "epoch 34 | loss: 0.42755 | train_accuracy: 0.76856 | val_accuracy: 0.74912 |  0:07:02s\n",
      "epoch 35 | loss: 0.41481 | train_accuracy: 0.74866 | val_accuracy: 0.73044 |  0:07:14s\n",
      "epoch 36 | loss: 0.41062 | train_accuracy: 0.74423 | val_accuracy: 0.72762 |  0:07:26s\n",
      "epoch 37 | loss: 0.40493 | train_accuracy: 0.75831 | val_accuracy: 0.74131 |  0:07:38s\n",
      "epoch 38 | loss: 0.3974  | train_accuracy: 0.76412 | val_accuracy: 0.74406 |  0:07:50s\n",
      "epoch 39 | loss: 0.39391 | train_accuracy: 0.74405 | val_accuracy: 0.72688 |  0:08:01s\n",
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_accuracy = 0.76731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def train_tabnet(train_df, val_df, feature_cols, response_col='ClaimYN', epochs=50):\n",
    "    # Convert boolean columns to integers\n",
    "    train_df = train_df.applymap(lambda x: int(x) if isinstance(x, bool) else x)\n",
    "    val_df = val_df.applymap(lambda x: int(x) if isinstance(x, bool) else x)\n",
    "\n",
    "    # Ensure all data is numeric and convert to numpy arrays\n",
    "    X_train = train_df[feature_cols].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
    "    y_train = train_df[response_col].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
    "    X_val = val_df[feature_cols].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
    "    y_val = val_df[response_col].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
    "\n",
    "    # Print shapes of the arrays\n",
    "    print(\"Shapes of the datasets:\")\n",
    "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "\n",
    "    # Verify the types of the arrays\n",
    "    print(\"Data types:\")\n",
    "    print(f\"X_train type: {type(X_train)}, y_train type: {type(y_train)}\")\n",
    "    print(f\"X_val type: {type(X_val)}, y_val type: {type(y_val)}\")\n",
    "\n",
    "    # Print sample data\n",
    "    print(\"Sample data:\")\n",
    "    print(f\"X_train sample: {X_train[:5]}\")\n",
    "    print(f\"y_train sample: {y_train[:5]}\")\n",
    "    print(f\"X_val sample: {X_val[:5]}\")\n",
    "    print(f\"y_val sample: {y_val[:5]}\")\n",
    "\n",
    "    # Assert that the arrays contain only numeric data\n",
    "    assert np.issubdtype(X_train.dtype, np.number), \"X_train contains non-numeric data\"\n",
    "    assert np.issubdtype(y_train.dtype, np.number), \"y_train contains non-numeric data\"\n",
    "    assert np.issubdtype(X_val.dtype, np.number), \"X_val contains non-numeric data\"\n",
    "    assert np.issubdtype(y_val.dtype, np.number), \"y_val contains non-numeric data\"\n",
    "\n",
    "    clf = TabNetClassifier()\n",
    "\n",
    "    clf.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "        eval_name=['train', 'val'],\n",
    "        eval_metric=['accuracy'],\n",
    "        max_epochs=epochs,\n",
    "        patience=10,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        weights=1,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    return clf\n",
    "\n",
    "# Assuming train_df_level1, val_df_level1, feature_cols_level1, etc., are already defined\n",
    "clf_level1 = train_tabnet(train_df_level1, val_df_level1, feature_cols_level1)\n",
    "clf_level2 = train_tabnet(train_df_level2, val_df_level2, feature_cols_level2)\n",
    "clf_level3 = train_tabnet(train_df_level3, val_df_level3, feature_cols_level3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3uXLYZM2VdG4",
    "outputId": "5d1df7e4-0c33-4ad3-c885-ab85873672e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for Level 1:\n",
      "Accuracy: 0.8129\n",
      "Recall: 0.46206896551724136\n",
      "Precision: 0.10935799782372144\n",
      "F1-Score: 0.1768587769467664\n",
      "MCC: 0.15321308807641093\n",
      "AUC: 0.7289220157302427\n",
      "No training history available for this model.\n",
      "Evaluation for Level 2:\n",
      "Accuracy: 0.78205\n",
      "Recall: 0.6471264367816092\n",
      "Precision: 0.12199349945828819\n",
      "F1-Score: 0.20528714676390153\n",
      "MCC: 0.21075781056375845\n",
      "AUC: 0.7892570494679477\n",
      "No training history available for this model.\n",
      "Evaluation for Level 3:\n",
      "Accuracy: 0.7658\n",
      "Recall: 0.6241379310344828\n",
      "Precision: 0.11081632653061224\n",
      "F1-Score: 0.1882149046793761\n",
      "MCC: 0.18799351107858148\n",
      "AUC: 0.776971177244624\n",
      "No training history available for this model.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for evaluation\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, matthews_corrcoef, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_model(clf, test_df, feature_cols, response_col='ClaimYN'):\n",
    "    # Ensure all data is numeric\n",
    "    X_test = test_df[feature_cols].apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(np.float32)\n",
    "    y_test = test_df[response_col].apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(np.float32)\n",
    "\n",
    "    # Calculate predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'F1-Score: {f1}')\n",
    "    print(f'MCC: {mcc}')\n",
    "    print(f'AUC: {auc}')\n",
    "\n",
    "    # Plot loss and accuracy if available\n",
    "    if hasattr(clf, 'history_'):\n",
    "        history = clf.history_\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        if 'loss' in history:\n",
    "            plt.plot(history['loss'], label='train_loss')\n",
    "        if 'val_loss' in history:\n",
    "            plt.plot(history['val_loss'], label='val_loss')\n",
    "        plt.legend()\n",
    "        plt.title('Loss')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if 'train_accuracy' in history:\n",
    "            plt.plot(history['train_accuracy'], label='train_accuracy')\n",
    "        if 'val_accuracy' in history:\n",
    "            plt.plot(history['val_accuracy'], label='val_accuracy')\n",
    "        plt.legend()\n",
    "        plt.title('Accuracy')\n",
    "\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No training history available for this model.\")\n",
    "\n",
    "print(\"Evaluation for Level 1:\")\n",
    "evaluate_model(clf_level1, test_df_level1, feature_cols_level1)\n",
    "\n",
    "print(\"Evaluation for Level 2:\")\n",
    "evaluate_model(clf_level2, test_df_level2, feature_cols_level2)\n",
    "\n",
    "print(\"Evaluation for Level 3:\")\n",
    "evaluate_model(clf_level3, test_df_level3, feature_cols_level3)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNR3n0txJ7tirVQDTBmgB6m",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
