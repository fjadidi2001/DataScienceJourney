{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/skVHhW1ZtPvqjqS3d9uM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Artificial_Intelligence_Learning/blob/master/soft_computing_alphabet_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6RylE-aE9fj",
        "outputId": "85dfcfdf-5be3-4cd0-f633-18df60f33385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 1.0\n",
            "a belongs to the lowercase alphabet\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample dataset of alphabets\n",
        "data = {\n",
        "    'character': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'],\n",
        "    'label': ['lowercase'] * 26\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Feature extraction\n",
        "vectorizer = CountVectorizer(analyzer='char')\n",
        "X = vectorizer.fit_transform(df['character'])\n",
        "\n",
        "# Target variable\n",
        "y = df['label']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy}\")\n",
        "\n",
        "# Recognize alphabet function\n",
        "def recognize_alphabet(input_char):\n",
        "    input_vector = vectorizer.transform([input_char])\n",
        "    prediction = model.predict(input_vector)\n",
        "    if prediction[0] == 'lowercase':\n",
        "        print(f\"{input_char} belongs to the lowercase alphabet\")\n",
        "    else:\n",
        "        print(f\"{input_char} does not belong to the lowercase alphabet\")\n",
        "\n",
        "# Test the recognition function\n",
        "input_char = 'a'\n",
        "recognize_alphabet(input_char)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2jbtIEUlwzFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Read the text file\n",
        "file_path = '/content/fars.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    data = file.read().splitlines()\n",
        "\n",
        "# Create a pandas dataframe from the data\n",
        "df = pd.DataFrame(data, columns=['character'])\n",
        "\n",
        "# Add labels to the dataframe\n",
        "df['label'] = df['character'].apply(lambda x: 'lowercase' if x.islower() else 'uppercase')\n",
        "\n",
        "# Feature extraction\n",
        "vectorizer = CountVectorizer(analyzer='word', lowercase=False)\n",
        "X = vectorizer.fit_transform(df['character'])\n",
        "\n",
        "# Target variable\n",
        "y = df['label']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy}\")\n",
        "\n",
        "# Recognize alphabet function\n",
        "def recognize_alphabet(input_char):\n",
        "    if input_char.islower():\n",
        "        print(f\"{input_char} belongs to the lowercase alphabet\")\n",
        "    else:\n",
        "        print(f\"{input_char} belongs to the uppercase alphabet\")\n",
        "\n",
        "# Test the recognition function\n",
        "input_char = 'a'\n",
        "recognize_alphabet(input_char)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoW5Djy_F7nh",
        "outputId": "36e2caee-8a0a-48cf-d3b7-817080a880f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model accuracy: 1.0\n",
            "a belongs to the lowercase alphabet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Preprocess the code snippets and convert them into numerical sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(code_snippets)\n",
        "sequences = tokenizer.texts_to_sequences(code_snippets)\n",
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Model accuracy: {accuracy}\")\n",
        "\n",
        "# Make predictions\n",
        "new_code_snippet = preprocess_and_tokenize(new_code)\n",
        "prediction = model.predict(new_code_snippet)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "In this example, you would need to replace code_snippets\n",
        "with your actual preprocessed data and labels with the corresponding labels\n",
        "for pixel manipulation. Additionally, you may need to adjust the model architecture,\n",
        "hyperparameters, and preprocessing steps based on the specifics of your problem and data.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "LGTOvsKnKRTa",
        "outputId": "63e99373-4330-4789-c5c9-459306d737fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-eb51ef9f081a>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Preprocess the code snippets and convert them into numerical sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_snippets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_snippets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmax_sequence_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'code_snippets' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "\n",
        "# Load the pixel representations from the text file and convert them to a matrix format\n",
        "# Assuming the data is in a suitable format, you may need to customize this step based on the actual data format\n",
        "\n",
        "# Preprocess the data and split it into features (X) and labels (y)\n",
        "X = ...  # Pixel matrix representations\n",
        "y = ...  # Uppercase/lowercase labels\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Design the RBF network architecture\n",
        "num_rbf_neurons = 50  # Number of RBF neurons\n",
        "encoder = LabelEncoder()\n",
        "y_train_encoded = encoder.fit_transform(y_train)\n",
        "\n",
        "# Initialize RBF neuron centers using K-means clustering\n",
        "kmeans = KMeans(n_clusters=num_rbf_neurons, random_state=0).fit(X_train)\n",
        "rbf_centers = kmeans.cluster_centers_\n",
        "\n",
        "# Compute RBF neuron widths\n",
        "distances = cdist(X_train, rbf_centers, 'euclidean')\n",
        "rbf_widths = np.std(distances)\n",
        "\n",
        "# Compute RBF neuron activations\n",
        "rbf_activations = rbf_kernel(X_train, rbf_centers, gamma=1.0 / (2 * (rbf_widths ** 2)))\n",
        "\n",
        "# Solve for the output weights using Ridge regression\n",
        "ridge = Ridge(alpha=1e-6, fit_intercept=True)\n",
        "ridge.fit(rbf_activations, y_train_encoded)\n",
        "\n",
        "# Test the trained RBF network\n",
        "# Compute RBF neuron activations for the testing data\n",
        "rbf_activations_test = rbf_kernel(X_test, rbf_centers, gamma=1.0 / (2 * (rbf_widths ** 2)))\n",
        "\n",
        "# Make predictions using the output weights\n",
        "y_pred_encoded = ridge.predict(rbf_activations_test)\n",
        "y_pred = encoder.inverse_transform(y_pred_encoded)\n",
        "\n",
        "# Evaluate the performance of the RBF network\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"RBF network accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "9QOO5t15NVFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the dataset from a text file\n",
        "def load_dataset(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    dataset = [list(map(int, line.split())) for line in lines]\n",
        "    return np.array(dataset)\n",
        "\n",
        "file_path = '/content/fars.txt'  # Provide the correct path to your text file\n",
        "original_dataset = load_dataset(file_path)\n",
        "\n",
        "# Step 2: Convert dataset to bipolar encoding based on the specified condition\n",
        "def convert_to_bipolar_with_condition(dataset):\n",
        "    return np.where(dataset > 128, -1, 1)\n",
        "\n",
        "bipolar_dataset = convert_to_bipolar_with_condition(original_dataset)\n",
        "\n",
        "# Print original and bipolar datasets for verification\n",
        "print(\"Original Dataset:\")\n",
        "print(original_dataset)\n",
        "\n",
        "print(\"\\nBipolar Encoded Dataset:\")\n",
        "print(bipolar_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrSfF6x5MnxS",
        "outputId": "567db952-3a59-45af-9d8d-f60fb476ef11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset:\n",
            "[[255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " ...\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]]\n",
            "\n",
            "Bipolar Encoded Dataset:\n",
            "[[-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " ...\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the dataset from a text file\n",
        "def load_dataset(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    dataset = [list(map(int, line.split())) for line in lines]\n",
        "    return np.array(dataset)\n",
        "\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = load_dataset(file_path)\n",
        "\n",
        "# Step 2: Reshape each sequence into a matrix\n",
        "num_fonts = 7\n",
        "sequence_length = 17\n",
        "\n",
        "# Reshape each sequence into a 2D matrix\n",
        "matrix_dataset = original_dataset.reshape(-1, num_fonts, sequence_length)\n",
        "\n",
        "# Print the original and matrix datasets for verification\n",
        "print(\"Original Dataset:\")\n",
        "print(original_dataset)\n",
        "\n",
        "print(\"\\nMatrix Dataset:\")\n",
        "print(matrix_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrlkXaOOOyl0",
        "outputId": "159f951e-b824-4153-df97-c53f68396365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset:\n",
            "[[255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " ...\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]]\n",
            "\n",
            "Matrix Dataset:\n",
            "[[[255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  ...\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]]\n",
            "\n",
            " [[255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  ...\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]]\n",
            "\n",
            " [[255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  ...\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  ...\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]]\n",
            "\n",
            " [[255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  ...\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]]\n",
            "\n",
            " [[255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  ...\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]\n",
            "  [255 255 255 ... 255 255 255]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the dataset from a text file\n",
        "def load_dataset(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    dataset = [list(map(int, line.split())) for line in lines]\n",
        "    return np.array(dataset)\n",
        "\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = load_dataset(file_path)\n",
        "\n",
        "# Step 2: Convert dataset to bipolar encoding based on the specified condition\n",
        "def convert_to_bipolar_with_condition(dataset):\n",
        "    return np.where(dataset > 128, -1, 1)\n",
        "\n",
        "bipolar_dataset = convert_to_bipolar_with_condition(original_dataset)\n",
        "\n",
        "# Step 3: Reshape each sequence into a matrix\n",
        "num_fonts = 7\n",
        "sequence_length = 17\n",
        "\n",
        "# Reshape each sequence into a 2D matrix\n",
        "matrix_dataset = bipolar_dataset.reshape(-1, num_fonts, sequence_length)\n",
        "\n",
        "# Print the original, bipolar, and matrix datasets for verification\n",
        "print(\"Original Dataset:\")\n",
        "print(original_dataset)\n",
        "\n",
        "print(\"\\nBipolar Encoded Dataset:\")\n",
        "print(bipolar_dataset)\n",
        "\n",
        "print(\"\\nMatrix Dataset:\")\n",
        "print(matrix_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqOLFLB3PlYU",
        "outputId": "333957f3-a4be-4be0-992d-cb9be328262d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset:\n",
            "[[255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " ...\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]]\n",
            "\n",
            "Bipolar Encoded Dataset:\n",
            "[[-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " ...\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "Matrix Dataset:\n",
            "[[[-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  ...\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            " [[-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  ...\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            " [[-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  ...\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  ...\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            " [[-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  ...\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            " [[-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  ...\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]\n",
            "  [-1 -1 -1 ... -1 -1 -1]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rLur539QcEeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "def convert_to_bipolar_with_condition(dataset):\n",
        "    return np.where(dataset > 128, -1, 1)\n",
        "\n",
        "bipolar_dataset = convert_to_bipolar_with_condition(original_dataset)\n",
        "\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each sequence into a 2D matrix\n",
        "input_data = bipolar_dataset.reshape(-1, num_fonts, 95, 95)\n",
        "\n",
        "# Step 2: Build a simple neural network with random weights\n",
        "hidden_size = 64\n",
        "output_size = 1\n",
        "\n",
        "# Initialize random weights and biases\n",
        "weights_hidden = np.random.randn(input_size, hidden_size)\n",
        "biases_hidden = np.zeros((1, hidden_size))\n",
        "weights_output = np.random.randn(hidden_size, output_size)\n",
        "biases_output = np.zeros((1, output_size))\n",
        "\n",
        "# Step 3: Define activation function (sigmoid) and forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(data):\n",
        "    data_flattened = data.reshape(-1, input_size)\n",
        "    hidden_layer_input = np.dot(data_flattened, weights_hidden) + biases_hidden\n",
        "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "    output_layer_input = np.dot(hidden_layer_output, weights_output) + biases_output\n",
        "    output = sigmoid(output_layer_input)\n",
        "    return output\n",
        "\n",
        "# Step 4: Train the neural network (random weights, so not meaningful)\n",
        "# Assuming you have labels for your data (binary classification)\n",
        "labels = np.random.randint(0, 2, size=(input_data.shape[0], 1))\n",
        "\n",
        "# Perform a forward pass (no backpropagation or weight updates in this simple example)\n",
        "predictions = forward_pass(input_data)\n",
        "\n",
        "# Print predictions for verification\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUWq-u7URVfO",
        "outputId": "73c6c12c-dfa7-4d09-dcc6-e5f5c8ee088f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            "[[0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.0854868 ]\n",
            " [0.93299217]\n",
            " [0.82351583]\n",
            " [0.34827895]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.07285459]\n",
            " [0.18752605]\n",
            " [0.28799974]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.87022639]\n",
            " [0.00725343]\n",
            " [0.94424744]\n",
            " [0.99094004]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.4592532 ]\n",
            " [0.92735167]\n",
            " [0.93935015]\n",
            " [0.83563282]\n",
            " [0.27773484]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.17637049]\n",
            " [0.48409341]\n",
            " [0.04765104]\n",
            " [0.13602017]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.9331633 ]\n",
            " [0.95340968]\n",
            " [0.10107802]\n",
            " [0.06086342]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.76620982]\n",
            " [0.99637036]\n",
            " [0.96667494]\n",
            " [0.04062386]\n",
            " [0.3291297 ]\n",
            " [0.2830179 ]\n",
            " [0.04601785]\n",
            " [0.00645116]\n",
            " [0.13320435]\n",
            " [0.91156307]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.17961895]\n",
            " [0.53269958]\n",
            " [0.89264076]\n",
            " [0.99355953]\n",
            " [0.91357556]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.24412919]\n",
            " [0.00365142]\n",
            " [0.49640472]\n",
            " [0.569527  ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.35527208]\n",
            " [0.99495094]\n",
            " [0.00540253]\n",
            " [0.78645302]\n",
            " [0.65629949]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.23124683]\n",
            " [0.20746955]\n",
            " [0.17632808]\n",
            " [0.56253533]\n",
            " [0.02455448]\n",
            " [0.15189764]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.93483881]\n",
            " [0.19737088]\n",
            " [0.40009532]\n",
            " [0.21709153]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.09405521]\n",
            " [0.24585903]\n",
            " [0.14622599]\n",
            " [0.15056528]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.72340406]\n",
            " [0.58618648]\n",
            " [0.99326318]\n",
            " [0.16971218]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.88223483]\n",
            " [0.01325084]\n",
            " [0.94230349]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.3291297 ]\n",
            " [0.14765882]\n",
            " [0.96568416]\n",
            " [0.04516033]\n",
            " [0.26691514]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, input_size)\n",
        "# print(character_matrices)\n",
        "\n",
        "# Flatten each matrix to create input vectors\n",
        "input_vectors = character_matrices.reshape(num_characters, -1)\n",
        "\n",
        "# Step 2: Build a simple perceptron network with random weights\n",
        "num_weights = input_size\n",
        "weights = np.random.randn(num_weights, 1)\n",
        "\n",
        "# Step 3: Define activation function (sigmoid) and forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(data, weights):\n",
        "    return sigmoid(np.dot(data, weights))\n",
        "\n",
        "# Step 4: Train the perceptron network for 10 epochs\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for char_idx in range(num_characters):\n",
        "        for font_idx in range(num_fonts):\n",
        "            # Select the input vector for the current character and font\n",
        "            input_vector = input_vectors[char_idx, font_idx, :]\n",
        "\n",
        "            # Forward pass\n",
        "            output = forward_pass(input_vector, weights)\n",
        "\n",
        "            # Update weights (dummy update, not based on any real learning algorithm)\n",
        "            weights += np.random.randn(num_weights, 1) * 0.01  # Adjust the learning rate as needed\n",
        "\n",
        "# Step 5: Make predictions for each character and font\n",
        "predictions = np.zeros((num_characters, num_fonts))\n",
        "\n",
        "for char_idx in range(num_characters):\n",
        "    for font_idx in range(num_fonts):\n",
        "        input_vector = input_vectors[char_idx, font_idx, :]\n",
        "        predictions[char_idx, font_idx] = forward_pass(input_vector, weights)\n",
        "\n",
        "# Print predictions for verification\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "81ZLv7T1cFvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, 95, 95)\n",
        "\n",
        "# Flatten each matrix to create input vectors\n",
        "input_vectors = character_matrices.reshape(num_characters * num_fonts, -1)\n",
        "\n",
        "# Step 2: Build a simple perceptron network with random weights\n",
        "num_weights = input_size\n",
        "weights = np.random.randn(num_weights, 1)\n",
        "\n",
        "# Step 3: Define activation function (sigmoid) and forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(data, weights):\n",
        "    return sigmoid(np.dot(data, weights))\n",
        "\n",
        "# Step 4: Train the perceptron network for 10 epochs\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for char_font_idx in range(num_characters * num_fonts):\n",
        "        # Select the input vector for the current character and font\n",
        "        input_vector = input_vectors[char_font_idx, :]\n",
        "\n",
        "        # Forward pass\n",
        "        output = forward_pass(input_vector, weights)\n",
        "\n",
        "        # Update weights (dummy update, not based on any real learning algorithm)\n",
        "        weights += np.random.randn(num_weights, 1) * 0.01  # Adjust the learning rate as needed\n",
        "\n",
        "# Step 5: Make predictions for each character and font\n",
        "predictions = np.zeros((num_characters, num_fonts))\n",
        "\n",
        "for char_idx in range(num_characters):\n",
        "    for font_idx in range(num_fonts):\n",
        "        input_vector = input_vectors[char_idx * num_fonts + font_idx, :]\n",
        "        predictions[char_idx, font_idx] = forward_pass(input_vector, weights)\n",
        "\n",
        "# Print predictions for verification\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PST6XckVcg7g",
        "outputId": "7b319559-bcc6-44bd-abe1-9e96b0e89663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            "[[9.99999999e-01 9.99999999e-01 9.99990211e-01 1.00000000e+00\n",
            "  6.93071461e-29 1.00000000e+00 9.99999999e-01]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 1.00000000e+00\n",
            "  6.30675936e-25 1.00000000e+00 9.99999999e-01]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 1.00000000e+00\n",
            "  2.12161405e-08 1.00000000e+00 3.50337551e-42]\n",
            " [9.99999999e-01 9.99999999e-01 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 9.99999999e-01]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 1.00000000e+00\n",
            "  1.00000000e+00 9.99999610e-01 9.99999992e-01]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 1.00000000e+00\n",
            "  2.94083991e-10 2.50683270e-17 9.92651736e-01]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 1.00000000e+00\n",
            "  1.00000000e+00 2.53696036e-12 1.02337196e-09]\n",
            " [9.99999999e-01 9.99999695e-01 1.48815743e-04 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 9.99999999e-01]\n",
            " [9.99999999e-01 9.99999999e-01 1.00000000e+00 9.75734483e-01\n",
            "  1.00000000e+00 1.00000000e+00 9.99999620e-01]\n",
            " [9.99999999e-01 9.99999999e-01 1.00000000e+00 9.99999993e-01\n",
            "  1.17339290e-33 1.00000000e+00 9.99999999e-01]\n",
            " [9.99999999e-01 1.00000000e+00 1.00000000e+00 8.77370107e-15\n",
            "  1.84884252e-62 1.00000000e+00 9.99999999e-01]\n",
            " [9.99999999e-01 9.99999999e-01 1.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 3.44079809e-04]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999991e-01\n",
            "  1.00000000e+00 1.00000000e+00 9.99999999e-01]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99248312e-01\n",
            "  9.99991699e-01 1.00000000e+00 9.99799667e-01]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999995e-01\n",
            "  1.00000000e+00 9.99999310e-01 9.99999972e-01]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 9.99999999e-01]\n",
            " [9.99999999e-01 9.99999999e-01 9.99999999e-01 4.75425242e-21\n",
            "  1.93806973e-11 4.44989157e-24 4.60938679e-03]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, 95, 95)\n",
        "\n",
        "# Flatten each matrix to create input vectors\n",
        "input_vectors = character_matrices.reshape(num_characters * num_fonts, -1)\n",
        "\n",
        "# Step 2: Build a simple perceptron network with random weights\n",
        "num_weights = input_size\n",
        "weights = np.random.randn(num_weights, 1)\n",
        "\n",
        "# Step 3: Define activation function (sigmoid) and forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(data, weights):\n",
        "    return sigmoid(np.dot(data, weights))\n",
        "\n",
        "# Step 4: Train the perceptron network for 10 epochs\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for char_font_idx in range(num_characters * num_fonts):\n",
        "        # Select the input vector for the current character and font\n",
        "        input_vector = input_vectors[char_font_idx, :]\n",
        "\n",
        "        # Forward pass\n",
        "        output = forward_pass(input_vector, weights)\n",
        "\n",
        "        # Convert output to bipolar (-1 or 1)\n",
        "        output_bipolar = np.where(output > 0.5, 1, -1)\n",
        "\n",
        "        # Update weights (dummy update, not based on any real learning algorithm)\n",
        "        weights += np.random.randn(num_weights, 1) * 0.01  # Adjust the learning rate as needed\n",
        "\n",
        "# Step 5: Make predictions for each character and font\n",
        "predictions = np.zeros((num_characters, num_fonts))\n",
        "\n",
        "for char_idx in range(num_characters):\n",
        "    for font_idx in range(num_fonts):\n",
        "        input_vector = input_vectors[char_idx * num_fonts + font_idx, :]\n",
        "        predictions[char_idx, font_idx] = np.where(forward_pass(input_vector, weights) > 0.5, 1, -1)\n",
        "\n",
        "# Print predictions for verification\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-3uN3mwfYlf",
        "outputId": "3ed5d736-0eb3-4d87-ecc4-be2e0dec2aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            "[[ 1.  1.  1.  1.  1.  1.  1.]\n",
            " [ 1.  1.  1.  1. -1.  1.  1.]\n",
            " [ 1.  1.  1.  1.  1. -1. -1.]\n",
            " [ 1.  1.  1. -1.  1.  1.  1.]\n",
            " [ 1.  1.  1. -1.  1.  1. -1.]\n",
            " [ 1.  1.  1.  1.  1.  1. -1.]\n",
            " [ 1.  1.  1.  1. -1. -1. -1.]\n",
            " [ 1.  1.  1. -1.  1.  1.  1.]\n",
            " [ 1.  1.  1. -1.  1. -1. -1.]\n",
            " [ 1.  1. -1.  1. -1.  1.  1.]\n",
            " [ 1.  1. -1. -1. -1.  1.  1.]\n",
            " [ 1.  1. -1. -1. -1.  1. -1.]\n",
            " [ 1.  1.  1.  1. -1. -1. -1.]\n",
            " [ 1.  1.  1. -1. -1.  1. -1.]\n",
            " [ 1.  1.  1. -1. -1. -1.  1.]\n",
            " [ 1.  1.  1.  1. -1.  1.  1.]\n",
            " [ 1.  1.  1. -1.  1. -1. -1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, 95, 95)\n",
        "\n",
        "# Flatten each matrix to create input vectors\n",
        "input_vectors = character_matrices.reshape(num_characters * num_fonts, -1)\n",
        "\n",
        "# Step 2: Build a simple perceptron network with random weights\n",
        "num_weights = input_size\n",
        "weights = np.random.randn(num_weights, 1)\n",
        "\n",
        "# Step 3: Define activation function (sigmoid) and forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(data, weights):\n",
        "    return sigmoid(np.dot(data, weights))\n",
        "\n",
        "# Step 4: Train the perceptron network for 10 epochs\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for char_font_idx in range(num_characters * num_fonts):\n",
        "        # Select the input vector for the current character and font\n",
        "        input_vector = input_vectors[char_font_idx, :]\n",
        "\n",
        "        # Forward pass\n",
        "        output = forward_pass(input_vector, weights)\n",
        "\n",
        "        # Convert output to bipolar (-1 or 1)\n",
        "        output_bipolar = np.where(output > 0.5, 1, -1)\n",
        "\n",
        "        # Update weights (dummy update, not based on any real learning algorithm)\n",
        "        weights += np.random.randn(num_weights, 1) * 0.01  # Adjust the learning rate as needed\n",
        "\n",
        "# Step 5: Make predictions for each character and font\n",
        "predictions = np.zeros((num_characters, num_fonts), dtype=bool)\n",
        "\n",
        "for char_idx in range(num_characters):\n",
        "    for font_idx in range(num_fonts):\n",
        "        input_vector = input_vectors[char_idx * num_fonts + font_idx, :]\n",
        "        predictions[char_idx, font_idx] = np.any(forward_pass(input_vector, weights) > 0.5)\n",
        "\n",
        "# Print predictions for verification\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20185e25-abcd-49db-c43d-2b3ec73942e0",
        "id": "0NFqqvMAgLr4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            "[[False False  True  True False False False]\n",
            " [False False False  True False False False]\n",
            " [False False False  True  True False False]\n",
            " [False False False  True False False False]\n",
            " [False False False False False False False]\n",
            " [False False False False  True False False]\n",
            " [False False False  True  True False False]\n",
            " [False False False  True  True False False]\n",
            " [False False False False  True False False]\n",
            " [False False False  True  True False False]\n",
            " [False False False  True  True False False]\n",
            " [False False False False False False False]\n",
            " [False False False False  True False False]\n",
            " [False False False False  True False False]\n",
            " [False False False False False  True False]\n",
            " [False False False  True  True False False]\n",
            " [False False False False  True False False]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, 95, 95)\n",
        "print(character_matrices)\n",
        "# Flatten each matrix to create input vectors\n",
        "input_vectors = character_matrices.reshape(num_characters * num_fonts, -1)\n",
        "print(input_vectors)\n",
        "# Create target labels (true if the character is present in any font)\n",
        "target_labels = np.zeros((num_characters, num_fonts), dtype=int)\n",
        "for char_idx in range(num_characters):\n",
        "    random_font_idx = np.random.randint(num_fonts)  # Randomly select a font for each character\n",
        "    target_labels[char_idx, random_font_idx] = 1\n",
        "\n",
        "# Step 2: Build a simple perceptron network with random weights\n",
        "num_weights = input_size\n",
        "weights = np.random.randn(num_weights, 1)\n",
        "\n",
        "# Step 3: Define activation function (sigmoid) and forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(data, weights):\n",
        "    return sigmoid(np.dot(data, weights))\n",
        "\n",
        "# Step 4: Train the perceptron network for 10 epochs\n",
        "epochs = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for char_font_idx in range(num_characters * num_fonts):\n",
        "        # Select the input vector for the current character and font\n",
        "        input_vector = input_vectors[char_font_idx, :].reshape(1, -1)\n",
        "\n",
        "        # Select the target label for the current character and font\n",
        "        target_label = target_labels[char_font_idx // num_fonts, char_font_idx % num_fonts]\n",
        "\n",
        "        # Forward pass\n",
        "        output = forward_pass(input_vector, weights)\n",
        "\n",
        "        # Update weights using perceptron learning rule\n",
        "        weights += learning_rate * (target_label - output) * input_vector.T\n",
        "\n",
        "# Step 5: Make predictions for each character and font\n",
        "predictions = np.zeros((num_characters, num_fonts), dtype=int)\n",
        "\n",
        "for char_idx in range(num_characters):\n",
        "    for font_idx in range(num_fonts):\n",
        "        input_vector = input_vectors[char_idx * num_fonts + font_idx, :].reshape(1, -1)\n",
        "        predictions[char_idx, font_idx] = int(forward_pass(input_vector, weights) > 0.5)\n",
        "\n",
        "# Print predictions and target labels for verification\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n",
        "print(\"\\nTarget Labels:\")\n",
        "print(target_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4PR54CVjYS-",
        "outputId": "ca17bca5-43ee-4886-b119-63323da695d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]]\n",
            "[[-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " ...\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]]\n",
            "Predictions:\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "\n",
            "Target Labels:\n",
            "[[0 0 0 0 0 1 0]\n",
            " [1 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 0 1 0 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 1]\n",
            " [0 0 1 0 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 1]\n",
            " [1 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def mean_squared_error(matrix1, matrix2):\n",
        "    \"\"\"\n",
        "    Calculates the mean squared error between two matrices.\n",
        "\n",
        "    Parameters:\n",
        "    - matrix1: NumPy array, the first matrix.\n",
        "    - matrix2: NumPy array, the second matrix.\n",
        "\n",
        "    Returns:\n",
        "    - float: The mean squared error between the matrices.\n",
        "    \"\"\"\n",
        "    return np.mean((matrix1 - matrix2) ** 2)\n",
        "\n",
        "# Example usage:\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, 95, 95)\n",
        "\n",
        "# Create another matrix (for demonstration purposes, you can replace this with your actual comparison)\n",
        "# Here, I'm creating a matrix with the same shape and random values.\n",
        "random_matrices = np.random.choice([-1, 1], size=character_matrices.shape)\n",
        "\n",
        "# Calculate mean squared error\n",
        "mse = mean_squared_error(character_matrices, random_matrices)\n",
        "\n",
        "# Print the result\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlMR1cBHrF4M",
        "outputId": "5411455a-9cf6-443f-cfb8-463e71182841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 2.0013091552410436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def accuracy(matrix1, matrix2):\n",
        "    \"\"\"\n",
        "    Calculates the accuracy between two matrices.\n",
        "\n",
        "    Parameters:\n",
        "    - matrix1: NumPy array, the first matrix.\n",
        "    - matrix2: NumPy array, the second matrix.\n",
        "\n",
        "    Returns:\n",
        "    - float: The accuracy between the matrices (percentage of matching elements).\n",
        "    \"\"\"\n",
        "    matching_elements = np.sum(matrix1 == matrix2)\n",
        "    total_elements = np.prod(matrix1.shape)\n",
        "    return (matching_elements / total_elements) * 100\n",
        "\n",
        "# Example usage:\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, 95, 95)\n",
        "\n",
        "# Create another matrix (for demonstration purposes, you can replace this with your actual comparison)\n",
        "# Here, I'm creating a matrix with the same shape and random values.\n",
        "random_matrices = np.random.choice([-1, 1], size=character_matrices.shape)\n",
        "\n",
        "# Calculate accuracy\n",
        "acc = accuracy(character_matrices, random_matrices)\n",
        "\n",
        "# Print the result\n",
        "print(\"Accuracy:\", acc, \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2RD7ZqRrZH6",
        "outputId": "584d02ff-e5e5-4e5d-d1a5-8cc7ee036d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 49.95730813100864 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, 95, 95)\n",
        "\n",
        "# Flatten each matrix to create input vectors\n",
        "input_vectors = character_matrices.reshape(num_characters * num_fonts, -1)\n",
        "\n",
        "# Create labels for the data (assuming you have labels for each font and character)\n",
        "labels = np.array([[i, j] for i in range(num_characters) for j in range(num_fonts)])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_vectors, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a perceptron model\n",
        "perceptron = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500)\n",
        "\n",
        "# Train the perceptron model\n",
        "perceptron.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = perceptron.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "acc = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "riqvR-iysZa2",
        "outputId": "e51d1d67-d67c-4b3b-a772-5b9f07fc49cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-72df6fa9da7e>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Train the perceptron model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mperceptron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Make predictions on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_solver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    435\u001b[0m         )\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_pass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, y, incremental, reset)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"classes_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarm_start\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_label_binarizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_label_binarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_label_binarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"multioutput\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_type_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0;34m\"Multioutput target data is not supported with label binarization\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Multioutput target data is not supported with label binarization"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, 95, 95)\n",
        "\n",
        "# Flatten each matrix to create input vectors\n",
        "input_vectors = character_matrices.reshape(num_characters * num_fonts, -1)\n",
        "\n",
        "# Create labels for the data (assuming you have labels for each font and character)\n",
        "labels = np.array([[i, j] for i in range(num_characters) for j in range(num_fonts)])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_vectors, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an MLP Regressor model\n",
        "mlp_regressor = MLPRegressor(hidden_layer_sizes=(100,), max_iter=500)\n",
        "\n",
        "# Train the MLP Regressor model\n",
        "mlp_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = np.round(mlp_regressor.predict(X_test))  # Rounding to get integer predictions\n",
        "\n",
        "# Calculate accuracy (for demonstration purposes, you can replace this with an appropriate evaluation metric for regression)\n",
        "acc = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "rU6ZIKXJst9e",
        "outputId": "7ddf2bb5-d401-463b-8b3e-5e3ceeae7330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-62402b4c1a46>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Calculate accuracy (for demonstration purposes, you can replace this with an appropriate evaluation metric for regression)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# No metrics support \"multiclass-multioutput\" format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: multiclass-multioutput is not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, 95, 95)\n",
        "\n",
        "# Flatten each matrix to create input vectors\n",
        "input_vectors = character_matrices.reshape(num_characters * num_fonts, -1)\n",
        "\n",
        "# Create labels for the data (assuming you have labels for each font and character)\n",
        "labels = np.array([[i, j] for i in range(num_characters) for j in range(num_fonts)])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_vectors, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a list to store MLPClassifier instances\n",
        "classifiers = []\n",
        "\n",
        "# Train an MLPClassifier for each font or character\n",
        "for i in range(num_fonts):\n",
        "    clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500)\n",
        "    y_train_single_font = y_train[:, 1] == i  # True for the current font, False for others\n",
        "    clf.fit(X_train, y_train_single_font)\n",
        "    classifiers.append(clf)\n",
        "\n",
        "# Make predictions for each font\n",
        "predictions = np.column_stack([clf.predict(X_test) for clf in classifiers])\n",
        "\n",
        "# Find the font with the maximum predicted probability for each sample\n",
        "predicted_fonts = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "acc = accuracy_score(y_test[:, 1], predicted_fonts)\n",
        "print(\"Accuracy:\", acc)\n"
      ],
      "metadata": {
        "id": "mEjNbi8xs8N6",
        "outputId": "121c7c13-e424-4bf0-9539-a85dce7f8599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.16666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, 95, 95)\n",
        "print(character_matrices)\n",
        "# Flatten each matrix to create input vectors\n",
        "input_vectors = character_matrices.reshape(num_characters * num_fonts, -1)\n",
        "print(input_vectors)\n",
        "\n",
        "# Add bias term to input vectors\n",
        "input_vectors_with_bias = np.c_[input_vectors, np.ones((num_characters * num_fonts, 1))]  # Adding a column of ones for bias\n",
        "\n",
        "# Create target labels (true if the character is present in any font)\n",
        "target_labels = np.zeros((num_characters, num_fonts), dtype=int)\n",
        "for char_idx in range(num_characters):\n",
        "    random_font_idx = np.random.randint(num_fonts)  # Randomly select a font for each character\n",
        "    target_labels[char_idx, random_font_idx] = 1\n",
        "\n",
        "# Step 2: Build a simple perceptron network with random weights\n",
        "num_weights = input_size + 1  # Additional weight for bias\n",
        "weights = np.random.randn(num_weights, 1)\n",
        "\n",
        "# Step 3: Define activation function (sigmoid) and forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(data, weights):\n",
        "    return sigmoid(np.dot(data, weights))\n",
        "\n",
        "# Step 4: Train the perceptron network for 10 epochs\n",
        "epochs = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for char_font_idx in range(num_characters * num_fonts):\n",
        "        # Select the input vector for the current character and font\n",
        "        input_vector = input_vectors_with_bias[char_font_idx, :].reshape(1, -1)\n",
        "\n",
        "        # Select the target label for the current character and font\n",
        "        target_label = target_labels[char_font_idx // num_fonts, char_font_idx % num_fonts]\n",
        "\n",
        "        # Forward pass\n",
        "        output = forward_pass(input_vector, weights)\n",
        "\n",
        "        # Update weights using perceptron learning rule\n",
        "        weights += learning_rate * (target_label - output) * input_vector.T\n",
        "\n",
        "# Step 5: Make predictions for each character and font\n",
        "predictions = np.zeros((num_characters, num_fonts), dtype=int)\n",
        "\n",
        "for char_idx in range(num_characters):\n",
        "    for font_idx in range(num_fonts):\n",
        "        input_vector = input_vectors_with_bias[char_idx * num_fonts + font_idx, :].reshape(1, -1)\n",
        "        predictions[char_idx, font_idx] = int(forward_pass(input_vector, weights) > 0.5)\n",
        "\n",
        "# Print predictions and target labels for verification\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n",
        "print(\"\\nTarget Labels:\")\n",
        "print(target_labels)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.sum(predictions == target_labels) / (num_characters * num_fonts)\n",
        "print(\"\\nAccuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR2ZO_Pcw5cH",
        "outputId": "ba3dab13-65f2-4a89-e649-bfe718dc94c2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]]\n",
            "[[-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " ...\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]]\n",
            "Predictions:\n",
            "[[1 1 1 0 1 1 1]\n",
            " [1 1 1 1 1 1 1]\n",
            " [1 1 1 1 0 1 0]\n",
            " [1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1]\n",
            " [1 1 1 1 0 0 0]\n",
            " [1 1 1 1 1 0 1]\n",
            " [1 1 0 0 0 1 1]\n",
            " [1 1 1 0 0 1 0]\n",
            " [1 1 1 1 0 1 1]\n",
            " [1 1 1 1 0 1 1]\n",
            " [1 1 1 1 1 0 1]\n",
            " [1 1 1 1 0 1 1]\n",
            " [1 1 1 0 0 0 1]\n",
            " [1 1 1 1 1 1 1]\n",
            " [1 1 1 1 0 1 1]\n",
            " [1 1 1 1 1 1 1]]\n",
            "\n",
            "Target Labels:\n",
            "[[0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 1 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [1 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [1 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 1]]\n",
            "\n",
            "Accuracy: 0.31932773109243695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "file_path = '/content/fars.txt'  # Replace with the correct path to your text file\n",
        "original_dataset = np.loadtxt(file_path)\n",
        "\n",
        "# Convert to bipolar encoding (-1 or 1)\n",
        "bipolar_dataset = np.where(original_dataset > 128, -1, 1)\n",
        "\n",
        "num_characters = 17\n",
        "num_fonts = 7\n",
        "input_size = 95 * 95\n",
        "\n",
        "# Reshape each character with different fonts into a matrix\n",
        "character_matrices = bipolar_dataset.reshape(num_characters, num_fonts, 95, 95)\n",
        "print(character_matrices)\n",
        "# Flatten each matrix to create input vectors\n",
        "input_vectors = character_matrices.reshape(num_characters * num_fonts, -1)\n",
        "print(input_vectors)\n",
        "\n",
        "# Add bias term to input vectors\n",
        "input_vectors_with_bias = np.c_[input_vectors, np.ones((num_characters * num_fonts, 1))]  # Adding a column of ones for bias\n",
        "\n",
        "# Create target labels (true if the character is present in any font)\n",
        "target_labels = np.zeros((num_characters, num_fonts), dtype=int)\n",
        "for char_idx in range(num_characters):\n",
        "    random_font_idx = np.random.randint(num_fonts)  # Randomly select a font for each character\n",
        "    target_labels[char_idx, random_font_idx] = 1\n",
        "\n",
        "# Step 2: Build a simple perceptron network with random weights\n",
        "num_weights = input_size + 1  # Additional weight for bias\n",
        "weights = np.random.randn(num_weights, 1)\n",
        "\n",
        "# Step 3: Define activation function (sigmoid) and forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(data, weights):\n",
        "    return sigmoid(np.dot(data, weights))\n",
        "\n",
        "# Step 4: Train the perceptron network for 10 epochs\n",
        "epochs = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for char_font_idx in range(num_characters * num_fonts):\n",
        "        # Select the input vector for the current character and font\n",
        "        input_vector = input_vectors_with_bias[char_font_idx, :].reshape(1, -1)\n",
        "\n",
        "        # Select the target label for the current character and font\n",
        "        target_label = target_labels[char_font_idx // num_fonts, char_font_idx % num_fonts]\n",
        "\n",
        "        # Forward pass\n",
        "        output = forward_pass(input_vector, weights)\n",
        "\n",
        "        # Update weights using perceptron learning rule\n",
        "        weights += learning_rate * (target_label - output) * input_vector.T\n",
        "\n",
        "# Step 5: Make predictions for each character and font\n",
        "predictions = np.zeros((num_characters, num_fonts), dtype=int)\n",
        "\n",
        "for char_idx in range(num_characters):\n",
        "    for font_idx in range(num_fonts):\n",
        "        input_vector = input_vectors_with_bias[char_idx * num_fonts + font_idx, :].reshape(1, -1)\n",
        "        predictions[char_idx, font_idx] = int(forward_pass(input_vector, weights) > 0.5)\n",
        "\n",
        "# Print predictions and target labels for verification\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n",
        "print(\"\\nTarget Labels:\")\n",
        "print(target_labels)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.sum(predictions == target_labels) / (num_characters * num_fonts)\n",
        "print(\"\\nAccuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKdcq72gyF-h",
        "outputId": "5ba73b9c-95dd-43c4-fdea-cbf35bc85086"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]\n",
            "\n",
            "\n",
            " [[[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]]]]\n",
            "[[-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " ...\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]\n",
            " [-1 -1 -1 ... -1 -1 -1]]\n",
            "Predictions:\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "\n",
            "Target Labels:\n",
            "[[0 1 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 1 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 1 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 1 0 0 0 0 0]]\n",
            "\n",
            "Accuracy: 0.907563025210084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ... (previous code remains the same)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "test_size = 0.2  # You can adjust this proportion based on your dataset size\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_vectors_with_bias, target_labels.flatten(), test_size=test_size, random_state=42)\n",
        "\n",
        "# Step 2: Build a simple perceptron network with random weights\n",
        "num_weights = input_size + 1  # Additional weight for bias\n",
        "weights = np.random.randn(num_weights, 1)\n",
        "\n",
        "# Step 3: Define activation function (sigmoid) and forward pass\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(data, weights):\n",
        "    return sigmoid(np.dot(data, weights))\n",
        "\n",
        "# Step 4: Train the perceptron network for 10 epochs\n",
        "epochs = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for idx in range(len(X_train)):\n",
        "        # Select the input vector for the current sample\n",
        "        input_vector = X_train[idx, :].reshape(1, -1)\n",
        "\n",
        "        # Select the target label for the current sample\n",
        "        target_label = y_train[idx]\n",
        "\n",
        "        # Forward pass\n",
        "        output = forward_pass(input_vector, weights)\n",
        "\n",
        "        # Update weights using perceptron learning rule\n",
        "        weights += learning_rate * (target_label - output) * input_vector.T\n",
        "\n",
        "# Step 5: Make predictions on the test set\n",
        "predictions = np.zeros_like(y_test)\n",
        "\n",
        "for idx in range(len(X_test)):\n",
        "    input_vector = X_test[idx, :].reshape(1, -1)\n",
        "    predictions[idx] = int(forward_pass(input_vector, weights) > 0.5)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "accuracy = np.sum(predictions == y_test) / len(y_test)\n",
        "print(\"\\nAccuracy on Test Set:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM5wrJt_y6AA",
        "outputId": "e0abb337-c6b2-4a0d-8964-04e214b07397"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy on Test Set: 0.8333333333333334\n"
          ]
        }
      ]
    }
  ]
}