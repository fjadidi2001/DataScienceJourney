{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Artificial_Intelligence_Learning/blob/master/svd_computational_data_mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0R4_yIJsIj4",
        "outputId": "d491d369-7e6a-4ac6-c1cd-bdff2e3d1ead"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "y = y.astype(np.int8)\n",
        "\n",
        "# Normalize the data to have values between 0 and 1\n",
        "X = X / 255.0\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5CDhFbOCmUQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5RiA1Ynseig"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Choose the number of components\n",
        "n_components = 50\n",
        "\n",
        "# Apply Truncated SVD\n",
        "svd = TruncatedSVD(n_components=n_components)\n",
        "X_train_svd = svd.fit_transform(X_train)\n",
        "X_test_svd = svd.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5c3bDjCsosd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91a57b2e-e157-482b-ed39-6993aa47bbf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9085714285714286\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.97      1343\n",
            "           1       0.94      0.96      0.95      1600\n",
            "           2       0.90      0.89      0.89      1380\n",
            "           3       0.88      0.88      0.88      1433\n",
            "           4       0.91      0.92      0.91      1295\n",
            "           5       0.86      0.86      0.86      1273\n",
            "           6       0.93      0.95      0.94      1396\n",
            "           7       0.92      0.93      0.92      1503\n",
            "           8       0.88      0.84      0.86      1357\n",
            "           9       0.89      0.88      0.88      1420\n",
            "\n",
            "    accuracy                           0.91     14000\n",
            "   macro avg       0.91      0.91      0.91     14000\n",
            "weighted avg       0.91      0.91      0.91     14000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Train a Logistic Regression classifier\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf.fit(X_train_svd, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test_svd)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZjt4kd8sp7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c0193eb-0167-4dcf-f01f-3824b9d161f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9086428571428572\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97      1343\n",
            "           1       0.94      0.96      0.95      1600\n",
            "           2       0.90      0.89      0.89      1380\n",
            "           3       0.88      0.88      0.88      1433\n",
            "           4       0.91      0.91      0.91      1295\n",
            "           5       0.86      0.86      0.86      1273\n",
            "           6       0.93      0.95      0.94      1396\n",
            "           7       0.92      0.93      0.93      1503\n",
            "           8       0.88      0.85      0.86      1357\n",
            "           9       0.89      0.88      0.89      1420\n",
            "\n",
            "    accuracy                           0.91     14000\n",
            "   macro avg       0.91      0.91      0.91     14000\n",
            "weighted avg       0.91      0.91      0.91     14000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Load the MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "y = y.astype(np.int8)\n",
        "\n",
        "# Normalize the data to have values between 0 and 1\n",
        "X = X / 255.0\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Apply SVD to the data\n",
        "n_components = 50\n",
        "svd = TruncatedSVD(n_components=n_components)\n",
        "X_train_svd = svd.fit_transform(X_train)\n",
        "X_test_svd = svd.transform(X_test)\n",
        "\n",
        "# Step 3: Classification using SVD Components\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf.fit(X_train_svd, y_train)\n",
        "y_pred = clf.predict(X_test_svd)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbScaAkyst1T"
      },
      "source": [
        "Explanation\n",
        "\n",
        "    Loading the Dataset: We fetch the MNIST dataset and split it into training and test sets.\n",
        "    Data Normalization: The pixel values are scaled to the range [0, 1].\n",
        "    SVD Application: We use TruncatedSVD to reduce the dimensionality of the dataset to 50 components.\n",
        "    Classification: Logistic Regression is used to classify the SVD-reduced data.\n",
        "    Evaluation: The accuracy and classification report of the classifier on the test set are printed.\n",
        "\n",
        "This code provides a basic implementation of a classification algorithm using SVD on the MNIST dataset. You can further tune parameters, try different classifiers, or use cross-validation for a more robust model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist['data'], mnist['target']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Divide the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Perform Singular Value Decomposition (SVD) on the training data\n",
        "U, s, VT = np.linalg.svd(X_train, full_matrices=False)\n",
        "\n",
        "# Specify the number of bases to use\n",
        "n_bases = 100\n",
        "U_bases = U[:, :n_bases]\n",
        "VT_bases = VT[:n_bases, :]\n",
        "\n",
        "# Calculate the reconstruction of the training data using the bases\n",
        "X_train_reconstructed = np.dot(U_bases, np.dot(np.diag(s[:n_bases]), VT_bases))\n",
        "\n",
        "# Calculate the difference of the test data images with respect to each of the bases\n",
        "threshold = 0.5  # Set a threshold for the reconstruction error\n",
        "test_distances = np.zeros((X_test.shape[0], n_bases))\n",
        "for i in range(n_bases):\n",
        "    test_projection = np.dot(X_test, VT_bases[i, :][:, np.newaxis])  # Project test data onto the ith basis\n",
        "    test_reconstructed = np.dot(test_projection, VT_bases[i, :][np.newaxis, :])  # Reconstruct test data using the ith basis\n",
        "    test_distances[:, i] = np.linalg.norm(X_test - test_reconstructed, axis=1)  # Calculate the reconstruction error\n",
        "\n",
        "# Assign a new class based on the minimum difference\n",
        "predicted_classes = np.argmin(test_distances, axis=1)\n",
        "\n",
        "# Calculate the accuracy of the predicted classes\n",
        "correct_predictions = np.sum(np.min(test_distances, axis=1) < threshold)\n",
        "accuracy = correct_predictions / len(y_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "naz3vSMfmWwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29a6c7b0-1f72-4a19-866c-6323ea5e4e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist['data'], mnist['target']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1XYEUEcH9RW",
        "outputId": "01a17514-c8fd-4c55-cda8-b28fc1a85647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist['data'], mnist['target']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Perform SVD to obtain the rotation, coordinate scaling, and reflection matrix\n",
        "svd = TruncatedSVD(n_components=784, random_state=42)\n",
        "X_transformed = svd.fit_transform(X_scaled)\n",
        "\n",
        "# Print the transformation matrix\n",
        "print(\"Transformation Matrix:\")\n",
        "print(X_transformed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUj1_ZtqQfnN",
        "outputId": "6a52d24a-0eb6-4f54-d8fc-a8219d8ccc34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformation Matrix:\n",
            "[[-8.91719847e-01 -4.92971090e+00 -9.30375793e-02 ... -3.56633747e-15\n",
            "  -3.60487263e-15 -1.51361541e-15]\n",
            " [ 8.81353003e+00 -7.51756046e+00 -3.71418494e+00 ... -3.48509123e-15\n",
            "   2.30536553e-15 -1.75716367e-15]\n",
            " [ 2.20483541e+00  9.82446089e+00 -5.75248779e+00 ...  8.25347788e-15\n",
            "  -2.07034860e-15 -3.78515313e-15]\n",
            " ...\n",
            " [-5.22259497e+00 -1.18431579e+00 -4.75472539e+00 ...  3.84402095e-15\n",
            "  -1.50492375e-15 -2.33040000e-16]\n",
            " [-2.17772858e+00 -6.99503251e+00 -3.22873680e+00 ... -9.35170273e-15\n",
            "  -3.67411036e-15 -1.35535379e-15]\n",
            " [ 1.17778717e+01 -5.40298122e+00  2.32272519e+00 ... -5.48412132e-15\n",
            "   4.28139300e-15 -1.91461240e-15]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}