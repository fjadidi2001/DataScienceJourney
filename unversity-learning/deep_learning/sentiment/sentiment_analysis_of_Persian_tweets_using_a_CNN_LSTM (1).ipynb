{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Load Dataset\n"
      ],
      "metadata": {
        "id": "GDTR12O3HjLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Path to the ZIP file\n",
        "zip_file_path = '/content/drive/My Drive/Deep learning/persian-tweets-emotional-dataset.zip'\n",
        "extract_dir = '/content/persian_tweets_dataset/'\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "# Load each CSV file and concatenate them into a single DataFrame\n",
        "emotion_files = ['anger.csv', 'disgust.csv', 'fear.csv', 'joy.csv', 'sad.csv', 'surprise.csv']\n",
        "emotion_labels = ['anger', 'disgust', 'fear', 'joy', 'sad', 'surprise']\n",
        "\n",
        "df_list = []\n",
        "for file, label in zip(emotion_files, emotion_labels):\n",
        "    temp_df = pd.read_csv(os.path.join(extract_dir, file))\n",
        "    print(f\"Columns in {file}: {temp_df.columns.tolist()}\")\n",
        "    temp_df['emotion'] = label\n",
        "    df_list.append(temp_df)\n",
        "\n",
        "df = pd.concat(df_list, ignore_index=True)\n",
        "print(\"Combined DataFrame Columns:\", df.columns.tolist())\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "vxMC7PM1HiDU",
        "outputId": "5f9cd5b0-22cc-4e2f-a184-5f54eab9a19e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/Deep learning/persian-tweets-emotional-dataset.zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-880a66e37b0b>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Extract the ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Deep learning/persian-tweets-emotional-dataset.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Display a Bar Graph of Class Distribution\n"
      ],
      "metadata": {
        "id": "4xo-7yocHnD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Display the class distribution\n",
        "df['emotion'].value_counts().plot(kind='bar')\n",
        "plt.xlabel('Emotion')\n",
        "plt.ylabel('Number of examples')\n",
        "plt.title('Class distribution in Persian Tweets Emotional Dataset')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a7f4TnckHpny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Text Preprocessing\n"
      ],
      "metadata": {
        "id": "29wA9hmdHwzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm\n"
      ],
      "metadata": {
        "id": "F5dwqsKAIYD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from hazm import Normalizer, word_tokenize, Stemmer\n",
        "\n",
        "# Initialize the normalizer and stemmer\n",
        "normalizer = Normalizer()\n",
        "stemmer = Stemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Step 1: Normalize text\n",
        "    text = normalizer.normalize(text)\n",
        "\n",
        "    # Step 2: Remove HTML tags and URLs\n",
        "    text = re.sub(r'<.*?>', '', text)  # Removing HTML tags\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Step 3: Remove repetitive characters\n",
        "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
        "\n",
        "    # Step 4: Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Step 5: Remove stop words (if needed, use a list of Persian stop words)\n",
        "    stop_words = set(['این', 'آن', 'و', 'در', 'به'])  # Example stop words\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Step 6: Remove emojis (optional)\n",
        "    text = re.sub(r'[\\U0001F600-\\U0001F64F]', '', text)  # Remove emojis\n",
        "\n",
        "    # Step 7: Stemming\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # Join tokens back to string\n",
        "    text = ' '.join(stemmed_tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Check columns of the DataFrame to identify the correct text column\n",
        "print(\"DataFrame Columns:\", df.columns.tolist())\n",
        "\n",
        "# Replace 'tweet' with the actual name of the text column in your DataFrame\n",
        "text_column_name = 'tweet'  # Adjust based on your dataset\n",
        "\n",
        "# Apply preprocessing to the entire dataset\n",
        "df[text_column_name] = df[text_column_name].apply(preprocess_text)\n",
        "\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "RBglxICEHxog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Convert Text to Numeric Format using BOW and ParsBERT\n",
        "\n"
      ],
      "metadata": {
        "id": "BlkZu6tPH9RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "\n",
        "# Bag of Words (BOW)\n",
        "vectorizer = CountVectorizer(max_features=5000)  # Adjust max_features as needed\n",
        "X_bow = vectorizer.fit_transform(df['tweet']).toarray()  # Use 'tweet' instead of 'text'\n",
        "\n",
        "# ParsBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "parsbert = TFAutoModel.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "\n",
        "def encode_texts(texts):\n",
        "    return tokenizer(texts, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "X_parsbert = encode_texts(df['tweet'].tolist())  # Use 'tweet' instead of 'text'\n",
        "\n",
        "print(\"BOW shape:\", X_bow.shape)\n",
        "print(\"ParsBERT input shape:\", X_parsbert['input_ids'].shape)\n"
      ],
      "metadata": {
        "id": "lz6bhvgzH-Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Obtain Embedding Vectors\n"
      ],
      "metadata": {
        "id": "jTlmW2aNQFqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "\n",
        "# Initialize ParsBERT tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "parsbert = TFAutoModel.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "\n",
        "# Encode texts using ParsBERT tokenizer\n",
        "def encode_texts(texts):\n",
        "    return tokenizer(texts, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "# Example: Encode texts from the 'tweet' column in your DataFrame 'df'\n",
        "X_parsbert = encode_texts(df['tweet'].tolist())\n",
        "\n",
        "# Get embeddings from ParsBERT\n",
        "embeddings = parsbert(X_parsbert['input_ids'], attention_mask=X_parsbert['attention_mask']).last_hidden_state\n",
        "embeddings = tf.reduce_mean(embeddings, axis=1).numpy()  # Take mean of token embeddings\n",
        "\n",
        "print(\"Embedding shape:\", embeddings.shape)\n",
        "\n",
        "# The default dimension of ParsBERT embeddings\n",
        "default_dimension = embeddings.shape[1]\n",
        "print(\"Default embedding dimension:\", default_dimension)\n",
        "\n",
        "# Explanation of embedding vectors and relationships\n",
        "print(\"Embedding vectors represent words as vectors in a high-dimensional space where semantic relationships are captured.\")\n",
        "print(\"Words with similar meanings or usage patterns will have vectors that are close to each other in this space.\")\n"
      ],
      "metadata": {
        "id": "k82CiJBNQGmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Building the Model with Train/Test Split\n"
      ],
      "metadata": {
        "id": "_M-CCvqyQJH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X_bow, df['emotion'], test_size=0.3, random_state=42)\n",
        "X_train_parsbert, X_test_parsbert, y_train_parsbert, y_test_parsbert = train_test_split(embeddings, df['emotion'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Further split training set into training and validation sets\n",
        "X_train_bow, X_val_bow, y_train, y_val = train_test_split(X_train_bow, y_train, test_size=0.2, random_state=42)\n",
        "X_train_parsbert, X_val_parsbert, y_train_parsbert, y_val_parsbert = train_test_split(X_train_parsbert, y_train_parsbert, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training set size (BOW):\", X_train_bow.shape)\n",
        "print(\"Validation set size (BOW):\", X_val_bow.shape)\n",
        "print(\"Test set size (BOW):\", X_test_bow.shape)\n",
        "\n",
        "print(\"Training set size (ParsBERT):\", X_train_parsbert.shape)\n",
        "print(\"Validation set size (ParsBERT):\", X_val_parsbert.shape)\n",
        "print(\"Test set size (ParsBERT):\", X_test_parsbert.shape)\n"
      ],
      "metadata": {
        "id": "YIN0UG75QLK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Create and Train Models\n"
      ],
      "metadata": {
        "id": "kStetcPMQMw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense\n",
        "\n",
        "def create_cnn_lstm_model(input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    conv_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(input_layer)\n",
        "    pooling_layer = MaxPooling1D(pool_size=2)(conv_layer)\n",
        "    lstm_layer = LSTM(128)(pooling_layer)\n",
        "    output_layer = Dense(len(emotion_labels), activation='softmax')(lstm_layer)\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "cnn_lstm_model = create_cnn_lstm_model((X_train_parsbert.shape[1], 1))\n",
        "cnn_lstm_model.summary()\n",
        "\n",
        "history = cnn_lstm_model.fit(\n",
        "    X_train_parsbert, y_train_parsbert,\n",
        "    validation_data=(X_val_parsbert, y_val_parsbert),\n",
        "    epochs=10,  # Adjust the number of epochs as needed\n",
        "    batch_size=32  # Adjust the batch size as needed\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "cnn_lstm_model.evaluate(X_test_parsbert, y_test_parsbert)\n"
      ],
      "metadata": {
        "id": "KVoQv6YoQP3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple CNN and LSTM Models\n"
      ],
      "metadata": {
        "id": "bqM2mPucQRsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple CNN model\n",
        "def create_cnn_model(input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    conv_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(input_layer)\n",
        "    pooling_layer = MaxPooling1D(pool_size=2)(conv_layer)\n",
        "    flatten_layer = tf.keras.layers.Flatten()(pooling_layer)\n",
        "    output_layer = Dense(len(emotion_labels), activation='softmax')(flatten_layer)\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "cnn_model = create_cnn_model((X_train_parsbert.shape[1], 1))\n",
        "cnn_model.summary()\n",
        "\n",
        "cnn_history = cnn_model.fit(\n",
        "    X_train_parsbert, y_train_parsbert,\n",
        "    validation_data=(X_val_parsbert, y_val_parsbert),\n",
        "    epochs=10,  # Adjust the number of epochs as needed\n",
        "    batch_size=32  # Adjust the batch size as needed\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "cnn_model.evaluate(X_test_parsbert, y_test_parsbert)\n",
        "\n",
        "# Simple LSTM model\n",
        "def create_lstm_model(input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    lstm_layer = LSTM(128)(input_layer)\n",
        "    output_layer = Dense(len(emotion_labels), activation='softmax')(lstm_layer)\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "lstm_model = create_lstm_model((X_train_parsbert.shape[1], 1))\n",
        "lstm_model.summary()\n",
        "\n",
        "lstm_history = lstm_model.fit(\n",
        "    X_train_parsbert, y_train_parsbert,\n",
        "    validation_data=(X_val_parsbert, y_val_parsbert),\n",
        "    epochs=10,  # Adjust the number of epochs as needed\n",
        "    batch_size=32  # Adjust the batch size as needed\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "lstm_model.evaluate(X_test_parsbert, y_test_parsbert)\n"
      ],
      "metadata": {
        "id": "uYc3Zd6BQTnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Evaluate and Compare Models\n"
      ],
      "metadata": {
        "id": "c7oTBtw_QXKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Predict with each model\n",
        "y_pred_cnn_lstm = cnn_lstm_model.predict(X_test_parsbert).argmax(axis=1)\n",
        "y_pred_cnn = cnn_model.predict(X_test_parsbert).argmax(axis=1)\n",
        "y_pred_lstm = lstm_model.predict(X_test_parsbert).argmax(axis=1)\n",
        "\n",
        "# Evaluation reports\n",
        "print(\"CNN-LSTM Model Report:\")\n",
        "print(classification_report(y_test_parsbert, y_pred_cnn_lstm, target_names=emotion_labels))\n",
        "\n",
        "print(\"CNN Model Report:\")\n",
        "print(classification_report(y_test_parsbert, y_pred_cnn, target_names=emotion_labels))\n",
        "\n",
        "print(\"LSTM Model Report:\")\n",
        "print(classification_report(y_test_parsbert, y_pred_lstm, target_names=emotion_labels))\n",
        "\n",
        "# Weighted, micro, and macro averaging\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    report = classification_report(y_true, y_pred, target_names=emotion_labels, output_dict=True)\n",
        "    weighted_avg = report['weighted avg']\n",
        "    micro_avg = report['micro avg']\n",
        "    macro_avg = report['macro avg']\n",
        "    return weighted_avg, micro_avg, macro_avg\n",
        "\n",
        "weighted_avg_cnn_lstm, micro_avg_cnn_lstm, macro_avg_cnn_lstm = evaluate_model(y_test_parsbert, y_pred_cnn_lstm)\n",
        "weighted_avg_cnn, micro_avg_cnn, macro_avg_cnn = evaluate_model(y_test_parsbert, y_pred_cnn)\n",
        "weighted_avg_lstm, micro_avg_lstm, macro_avg_lstm = evaluate_model(y_test_parsbert, y_pred_lstm)\n",
        "\n",
        "print(\"CNN-LSTM Model Averages:\")\n",
        "print(\"Weighted Average:\", weighted_avg_cnn_lstm)\n",
        "print(\"Micro Average:\", micro_avg_cnn_lstm)\n",
        "print(\"Macro Average:\", macro_avg_cnn_lstm)\n",
        "\n",
        "print(\"CNN Model Averages:\")\n",
        "print(\"Weighted Average:\", weighted_avg_cnn)\n",
        "print(\"Micro Average:\", micro_avg_cnn)\n",
        "print(\"Macro Average:\", macro_avg_cnn)\n",
        "\n",
        "print(\"LSTM Model Averages:\")\n",
        "print(\"Weighted Average:\", weighted_avg_lstm)\n",
        "print(\"Micro Average:\", micro_avg_lstm)\n",
        "print(\"Macro Average:\", macro_avg_lstm)\n"
      ],
      "metadata": {
        "id": "pP78ZfCLQZei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I-eGMVtcQboV"
      }
    }
  ]
}